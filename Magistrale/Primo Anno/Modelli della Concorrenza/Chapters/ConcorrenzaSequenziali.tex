\chapter{Correttezza di programmi sequenziali}
\label{Capitolo 3}
Introduciamo l'argomento con un esempio.
\begin{esempio}
  Definiamo una funzione in C che riceve un vettore, un intero (la lunghezza del vettore) e
  e restituisce un ulteriore numero intero.
  \begin{listing}[ht]
    \begin{minted}{c}
      int f(int n, const int v[]) {
        int x = v[0];
        int h = 1;
        while (h < n) {
          if (x < v[h])
          x = v[h];        
          h = h + 1;
        }
        return x;
      }
    \end{minted}
    \caption{Esempio di funzione in C}
    \label{listing:1}
  \end{listing}
  Chiedendoci cosa fa la funzione scopriamo che si occupa di cercare il massimo in
  un vettore.\\
  La strategia della funzione è quella di spostarsi lungo il vettore e
  conservare in $x$ il valore massimo fino a ora trovato. Arrivati alla fine
  del vettore so che in $x$ avrò il valore massimo.\\
  Più formalmente suppongo che $n$ sia $n > 0$, per dire che ho almeno un
  elemento nel vettore. Suppongo inoltre che $v[i]\in\mathbb{Z},\,\,\forall i\in
  \{0,\ldots, n-1\}$. Abbiamo fissato le \textbf{condizioni iniziali}.\\
  All'inizio $x$ è il massimo del sotto-vettore con solo il primo elemento
  ($v[0..0]$) e dopo l'assegnamento di $h$ in $v[o..h-1]$, che, con $h=1$ mi
  conferma che x è il massimo in $v[0..0]$. \\
  Al termine di una certa iterazione $x$ contiene il massimo 
  tra i valori compresi tra $v[0]$ e $v[h-1]$ (detto altrimenti il massimo in
  $v[0..h-1]$). Inoltre al fine di una certa iterazione mi aspetto che $h\leq
  n$.\\
  Possiamo quindi dire che quando esco dal ciclo $x$ è il massimo in $v[0..h-1]$
  ma in questo momento $h=n$ e quindi $x$ è il massimo del vettore.\\
  Consideriamo ora la parte iterativa. All'inizio di ogni iterazione suppongo
  che $x$ è il massimo in $v[0..h-1]$. Dopo l'istruzione di scelta $x$ è il
  massimo in $v[0..h]$, comunque sia andata la scelta. Alla fine
  dell'iterazione, dopo l'incremento di $h$, avrò ancora che $x$ è il massimo in
  $v[0..h-1]$. Ragiono quindi per induzione. Se all'inizio dell'iterazione e
  alla fine ho la stessa asserzione, ed è vera prima d'iniziare l'iterazione,
  posso dire che ho una \textbf{proprietà invariante} e vale anche al termine
  dell'ultima iterazione e quindi vale anche alla fine dell'esecuzione del
  programma. 
\end{esempio}
Nell'esempio notiamo in primis l'assenza di formalità. Si introducono quindi
concetti:
\begin{enumerate}
  \item \textbf{pre-condizione} che nell'esempio è fatta da $n>0$ e
  $v[i]\in\mathbb{Z},\,\,\forall i\in \{0,\ldots, n-1\}$
  \item \textbf{post-condizione} che nell'esempio si ritrova con l'asserzione $x$
  è il massimo in $v[0..h-1]$ e $h=n$, che scritto in modo formale diventa:
  \[
    \begin{rcases}
      v[i]\leq x,\,\,\forall i\in \{0,\ldots, n-1\}\\
      \exists\,i\in\{0,\ldots, n-1\}\mbox{ t.c. } v[i]=x
    \end{rcases}
    x=max(v[0..n-1])
  \]
\end{enumerate}
\textit{Queste formule possono essere rese come formule proposizionali, tramite
  una congiunzione logica:}
\[
  \begin{cases}
    v[0]\leq x \land v[1]\leq x\land\ldots v[n-1]\leq x\\
    v[0]= x \lor v[1]= x\lor\ldots v[n-1]=x
  \end{cases}
\]
Abbiamo studiato lo stato della memoria del programma in un certo istante
tramite formule.
\begin{definizione}
  Definiamo \textbf{stato della memoria} come:
  \[s:V\to\mathbb{Z}\]
  ovvero una funzione che mappa le variabili del programma (poste nell'insieme
  $V$) in $\mathbb{Z}$.
\end{definizione}
Fissato uno stato della memoria e una formula posso validare una formula in
quello stato osservando le variabili e le relazioni aritmetiche della
formula.\\
Data una formula $\phi$ e uno stato $s$ posso sapere se $\phi$ è valida in
$s$.\\
Una formula che gode della \textbf{proprietà invariante} se vera all'inizio
dell'iterazione è vera anche alla fine della stessa. Per capire se è invariante
basta vedere lo stato di una formula ad inizio e fine di una iterazione.\\
L'esecuzione di una istruzione cambia lo stato della memoria. Potrebbe però
accadere che una serie di istruzioni non facciano terminare il programma, perciò
quanto detto sopra è in realtà un'approssimazione della realtà.
\begin{definizione}
  Definiamo la \textbf{specifica di correttezza di un programma} con la tripla:
  \[\alpha\,\, P\,\, \beta\]
  dove:
  \begin{itemize}
    \item $\alpha$ e $\beta$ sono formule (definite con tutte le simbologie
    aritmetiche tra variabili, sia di conto che di relazione).
    \item $P$ è un ``programma'' (anche un frammento o una singola istruzione) che
    modifica lo stato della memoria.
  \end{itemize}
  $\alpha$ è la \textbf{pre-condizione}, che supponiamo verificata nello stato
  iniziale e $\beta$ è la \textbf{post-condizione}, 
  che supponiamo valida dopo l'esecuzione del programma.
\end{definizione}
Durante il corso useremo un linguaggio imperativo non reale semplificato.
Dovremo anche definire una logica, definendo un apparato deduttivo, un insieme
di regole per costruire dimostrazioni derivando nuove formule da quelle
preesistenti. Useremo la \textbf{logica di Hoare}. Le formule della logica di
Hoare sono triple di tipo $(\alpha P \beta)$ quindi si tratta di una logica di tipo diverso
anche se si appoggia su quella proposizionale.
\section{Linguaggio semplificato}
Definiamo quindi il linguaggio di programmazione imperativo semplificato che
andremo a utilizzare. Si userà una grammatica formale.\\
L'elemento fondamentale di questo linguaggio è il \textbf{comando}, che indica o
una singola istruzione o un gruppo d'istruzioni strutturate. Il simbolo usato
nella grammatica per indicare un comando è ``C''. Un comando viene costruito
tramite le \textbf{produzioni}, introdotte da ``::=''. Il comando più semplice è
l'\textbf{assegnamento}, che usa l'operatore ``:='' per assegnare un valore ad
una variabile. Con il simbolo ``E'' indichiamo un simbolo non terminale della
grammatica che sta per \textit{espressione}. Una volta costruito semplici
espressioni possiamo combinarle, eseguendole in sequenza, inserendo un ``;'' tra
due comandi.\\ 
In merito all'istruzione di scelta abbiamo l'istruzione ``if'', seguito da
un'espressione booleana, seguito da ``then'', seguita da un comando, seguita da
``else'', seguita da un comando, e il tutto viene concluso da ``endif''. Qui
abbiamo una prima semplificazioni dicendo che l'\textit{else} è
obbligatorio. Per l'iterazione abbiamo il ``while'', seguito da un'espressione
booleana, seguito da ``do'' con poi il comando, il tutto concluso da
endwhile. Infine abbiamo una istruzione speciale, chiamata ``skip'', che non fa
nulla e avanza il \textit{program counter} (con essa posso saltare il ramo
alternativo dell'\textit{if-else}).\\
Un'espressione booleana ``B'' può essere la costante ``true'', la costante
``false'' o del tipo ``not B'', ``B and B'', ``B or B'', ``E < E'' (e le altre),
``E = E''. Non si hanno tipi di dato ma supporremo di avere a che fare solo con
\textit{interi}. Non si ha la funzione di nozione o di classe. Questo linguaggio
è comunque \textit{Turing Complete}.
\begin{listing}[H]
  \begin{lstlisting}
    x ~ a; y ~ b;
    while x != y do
      if x < y then
        y ~ y - x;
      else
        x ~ x - y;
      endif
    endwhile  
  \end{lstlisting}
  \caption{Esempio di programma $D$}
  \label{listing:D}
\end{listing}
Cerchiamo di capire se il programma $D$, sopra definito, soddisfa la tripla:
\[\{a>0\land b>0\}\,\,D\,\, \{x=MCD(a,b)\}\]
Quindi mi chiedo se eseguendo il programma con uno stato della memoria dove $a$
e $b$ sono due interi positivi (pre-condizione) allora, alla fine dell'esecuzione
di $D$, $x$ sarà il massimo comune divisore tra $a$ e $b$
(post-condizione). Dobbiamo dimostrare la tripla e qualora non fosse vera bisogna
confutarla trovando un caso in cui non è verificata (trovando uno stato che
soddisfi la pre-condizione ma che, una volta eseguito il programma, la post-condizione non 
sia verificata).
%----------------------------------------------------------------------------------------
%	SECTION 
%----------------------------------------------------------------------------------------
\section{Regole di derivazione}
\subsection{Logica di Hoare}
\begin{definizione}
  Una \textbf{dimostrazione}, in una logica data, è una sequenza di formule di
  quella logica che sono o \textit{assiomi} (formule che riteniamo vere a
  priori) o formule derivate dalle precedenti tramite una \textit{regola di
    inferenza}. 
\end{definizione}
Una regola d'inferenza mi manda da un insieme di formule
  $\alpha_1,\alpha_2,\ldots,\alpha_n$ a una nuova formula $\alpha$ e si indica
  con: 
  \[\frac{\alpha_1,\alpha_2,\ldots,\alpha_n}{\alpha}\]
  Che si legge come: "\textit{se ho già derivato $\alpha_1, \alpha_2,
    \ldots,\alpha_n$ sono autorizzato a derivare $\alpha$}".\\ 
  Nel nostro caso ogni $\alpha_i$ è una tripla della \textbf{logica di Hoare}.\\
  \textbf{Una dimostrazione si ottiene quindi applicando le regole di derivazione fino
  ad arrivare, se si riesce, a una soluzione.}
Vediamo quindi le regole di derivazione, che sono associate alle regole del
linguaggio sopra definito.
\subsection{Skip}
\begin{definizione}
  Partiamo con la regola per l'istruzione \textbf{\textit{skip}}, che non
  facendo nulla 
  non cambia lo stato della memoria e quindi la regola di derivazione non ha
  nessuna premessa:
  \[\frac{}{\{p\}\,\,skip\,\,\{p\}}\]
  con $p$ che è una formula proposizionale. Dopo lo \textit{skip} $p$ vale se
  valeva prima dello \textit{skip}
\end{definizione}
\subsection{Implicazione}
\begin{definizione}
  La seconda è una regola che non ha un rapporto diretto con il linguaggio e
  chiameremo \textbf{regola di conseguenza (o dell'implicazione)}. Ha due
  premesse: una tripla appartenente alla logica di Hoare e una implicazione della
  logica proposizionale.
  \[\frac{p\implies p'\,\, \,\,\{p'\}\,\,C\,\,\{q\}}{\{p\}\,\,C\,\,\{q\}}\]
  Ovvero se eseguo $C$ partendo da uno stato in cui vale $p'$ allora dopo varrà
  $q$. Ma sappiamo anche che $p$ implica $p'$. Quindi se nel mio stato della
  memoria vale $p$ e quindi anche $p'$ per l'implicazione. Posso quindi dire che
  se ho $p$ ed eseguo $C$ ottengo $q$.\\
  Ho anche una forma speculare:
  \[\frac{\{p\}\,\,C\,\,\{q'\}\,\, \,\,q'\implies q}{\{p\}\,\,C\,\,\{q\}}\]
\end{definizione}
\subsection{Sequenza}
\begin{definizione}
  La terza regola è legata alla struttura di \textbf{sequenza} dei programmi:
  \[\frac{\{p\}\,\,C_1\,\,\{q\}\,\,
      \,\,\{q\}\,\,C_2\,\,\{r\}}{\{p\}\,\,C_1;C_2\,\,\{r\}}\] 
\end{definizione}
\subsection{Assegnamento}
\begin{definizione}
  La quarta regola riguarda l'\textbf{assegnamento}. Questa è l'unica regola
  non banale (come lo è lo \textit{skip}) che non ha premesse. È la regola base
  per derivare le triple necessarie alle altre regole. Quindi, avendo $E$ come
  espressione, $x$ una variabile e $p$ come una post-condizione, ovvero una
  formula che contiene gli identificatori di diverse variabili:
  \[\frac{}{\{p[E/x]\}\,\,x\cceq E\,\,\{p\}}\]
  Dove con $p[E/x]$, come pre-condizione, indichiamo una \textbf{sostituzione} 
  indicante che cerchiamo in $p$ tutte le occorrenze $x$ e le sostituiamo con $E$.
  \begin{esempio}
    Se ho $x \cceq y+1$ con $E$ pari a $y+1$ e con $p$ pari a $\{x>0\}$ come
    post-condizione data e cerco la pre-condizione. Quindi la tripla completa
    sarebbe: 
    \[\{y+1>0\}\,\,x \cceq y+1\,\,\{x>0\}\]
    E la tripla sappiamo che è \emph{vera} (\textbf{se so che $y+1$ è positivo, dopo
    che a $x$ assegno $y+1$ posso essere sicuro che anche $x$ è positivo}).
  \end{esempio}
  \begin{esempio}
    Se ho $x \cceq x+2$ con $E$ pari a $x+2$ e con $p$ pari a $\{x>0\land x\leq
    y\}$ come post-condizione data e cerco la pre-condizione. Quindi la tripla
    completa sarebbe:
    \[\{x+2>0\land x+2\leq y\}\,\,x \cceq x+2\,\,\{x>0\land x\leq y\}\]
    e anche questa tripla è garantita dalla regola di sostituzione.
    Se a priori si conosce che $x>-2$ e che $x \leq y-2$ allora se eseguo l'operazione $x \cceq x+2$ concludo che $x>0$ e $x \leq y$
  \end{esempio}
\end{definizione}
\subsection{Istruzione di scelta}
\begin{definizione}
  La quinta regola è quella relativa all'\textbf{istruzione di scelta} che è
  della forma:
  \[\{p\}\mbox{ if \textit{B} then \textit{C} else \textit{D} endif }\{q\}\]
  Se la condizione $B$ è vera eseguo $C$ altrimenti $D$. In entrambi i casi
  alla fine deve valere la post-condizione $q$. Separiamo i due casi:
  \begin{itemize}
    \item se suppongo vere $p$ e $B$ eseguo $C$ arrivando in $q$:
    \[\{p\land B\}\,\,\,C\,\,\,\{q\}\]
    \item se suppongo vera $p$ ma falsa $B$ avrò:
    \[\{p\land \neg B\}\,\,\,D\,\,\,\{q\}\]
  \end{itemize}
  Ricavo quindi la formula generale:
  \[\frac{\{p\land B\}\,\,\,C\,\,\,\{q\}\,\,\,\,\,\,\,\,\,
      \{p\land \neg B\}\,\,\,D\,\,\,\{q\}}{\{p\}\mbox{ if \textit{B}
        then \textit{C} else \textit{D} endif }\{q\}}\]
\end{definizione}
\begin{shaded}
  Come notazione usiamo che:
  \[\vdash \{p\}\,\,\,C\,\,\,\{q\}\]
  dove $\vdash$
segnala che la tripla è stata \textbf{dimostrata/derivabile} con le regole di
derivazione (si parla quindi di \textit{sintassi}, viene infatti ignorato il
significato ma si cerca solo di applicare le regole, ottenendo in conclusione
come risultato di una catena di regole).\\
Come notazione usiamo anche che:
\[\vDash \{p\}\,\,\,C\,\,\,\{q\}\]
dove $\vDash$
indica che la tripla è \textbf{vera} (si parla quindi di \textit{semantica},
riferendosi al significato).\\
Dato che si ha \textbf{completezza} e \textbf{correttezza} dell'apparato
deduttivo si ha hanno due situazioni.
\begin{itemize}
  \item \textit{ogni tripla \textbf{derivabile} è anche \textbf{vera} in
    qualsiasi interpretazione}
  \item \textit{ogni tripla \textbf{vera} vorremmo fosse anche
    \textbf{derivabile} e il discorso verrà approfondito in seguito per la
    logica di Hoare}
\end{itemize}
$\vdash$ può avere a pedice una sigla per la regola rappresentata, ad esempio
$\vdash_{ass}$ per l'assegnamento.
\end{shaded}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{y\geq 0\}\,\,\,x\cceq 2\cdot y+1\,\,\,\{x>0\}\]
  uso la \emph{regola di assegnamento} (che non ha premesse) partendo dalla
  post-condizione. Sostituisco e ottengo:
  \[\vdash_{ass}\{2\cdot y+1> 0\}\,\,\,x\cceq 2\cdot y+1\,\,\,\{x>0\}\]
  Procedo usando la \emph{regola d'implicazione} (sapendo che $y\geq 0\implies
  2y+1>0$):
  \[\vdash_{impl}\{y\geq 0\}\,\,\,x\cceq 2\cdot y+1\,\,\,\{x>0\}\]
  Dimostrando quindi che la tripla è \textbf{vera}.
\end{esempio}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{z> 0\}\,\,\,x\cceq (y\cdot z)+1\,\,\,\{x>0\}\]
  e vediamo che la tripla non è valida in quanto $y$ potrebbe essere negativo e
  non portare alla positività di $x$. Formalmente cerchiamo un controesempio
  cercando di non soddisfare la post-condizione. Scelgo in memoria $z=1$ e
  $y=-2$. Abbiamo fatto quindi un ragionamento semantico. Provo anche
  sintatticamente e giungo a:
  \[\vdash\{(y\cdot z+1)> 0\}\,\,\,x\cceq (y\cdot z)+1\,\,\,\{x>0\}\]
  che non è vero, quindi non posso proseguire.
\end{esempio}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{s=x^i\}\,\,\,i\cceq i+1,\, s\cceq s\cdot x\,\,\,\{s=x^i\}\]
  \textbf{Anche se uguali pre-condizione e post-condizione fanno riferimento a due momenti
  della memoria diversi e quindi i valori saranno diversi.}\\
  Abbiamo a che fare con un \textbf{invariante} in quanto la formula non varia
  tra pre-condizione e post-condizione anche se i valori saranno diversi (in mezzo
  al processo posso violare comunque l'invarianza).\\
  Ragioniamo in modo puramente sintattico. Dobbiamo applicare due volte
  l'assegnamento (e spesso serve dopo anche la regola d'implicazione) e una
  volta la sequenza. Anche qui partiamo dalla post-condizione risalendo via via
  alle precondizioni:
  \[\vdash_{ass}\{sx=x^i\}\,\,\,s\cceq s\cdot x\,\,\,\{s=x^i\}\]
  Ho creato quindi la condizione intermedia tra i due assegnamenti. Essa rappresenta una pre-condizione per il passo successivo. Proseguendo si noti:
  \[\vdash_{ass}\{sx=x^{i+1}\}\,\,\,i\cceq i+1\,\,\,\{sx=x^i\}\]
  per transitività si può scrivere:
  \[\{sx=x^{i+1}\}\,\,\,i\cceq i+1,\, s\cceq s\cdot x\,\,\,\{s=x^i\}\]
  ma non coincide con quanto voglio dimostrare. Ragiono quindi in modo
  algebrico. In $\{sx=x^{i+1}\}\,\,\,i\cceq i+1\,\,\,\{sx=x^i\}$ divido per $x$:
  \[\{s\cancel{x}=x^{i+\cancel{1}}\}\to\{sx=x^i\},\mbox{ se }x\neq 0\]
  e quindi la formula iniziale è dimostrata.
\end{esempio}
\begin{esempio}
  Vediamo qualche esempio di dimostrazione. Dimostro che la tripla seguente sia
  vera:
  \[\{\top\}\,\,\,\mbox{if } x<0 \mbox{ then }y\cceq -2\cdot x \mbox{ else }
    y\cceq 2*x\mbox{ endif}\,\,\,\{y\geq 0\}\]
  Diciamo che $C$ rappresenta $y\cceq -2\cdot x$ e $D$ rappresenta $y\cceq 2*x$
  per praticità. Indichiamo la condizione booleana $x<0$ con $B$. Tutta la
  condizione di scelta la chiamiamo $S$
  Quindi avremo:
  \[\{\top\}\,\,\,\mbox{if } B \mbox{ then }C\mbox{ else }
    D \mbox{ endif}\,\,\,\{y\geq 0\}\]
  che in modo ancora più compatto sarebbe:
  \[\{\top\}\,\,\,S\,\,\,\{y\geq 0\}\]
  \textbf{Applico quindi la regola di derivazione al primo caso:}
  Ho quindi:
  \[\{\top \land x<0\}\,\,\,C\,\,\,{y\geq 0}\]
  che è uguale a:
  \[\{x<0\}\,\,\,C\,\,\,{y\geq 0}\]
  Procedo ora con l'assegnamento:
  \[\vdash_{ass}\{-2\cdot x\geq 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  che però equivale algebricamente a:
  \[\vdash_{ass}\{x\leq 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  ma siccome $x<0 \implies x\leq 0$ uso la regola dell'implicazione:
  \[\vdash_{impl}\{x< 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  \textbf{Passo al secondo caso:}
  \[\{\top \land x\geq 0\}\,\,\,D\,\,\,{y\geq 0}\]
  che è uguale a:
  \[\{x\geq 0\}\,\,\,D\,\,\,{y\geq 0}\]
  Procedo ora con l'assegnamento:
  \[\vdash_{ass}\{2\cdot x\geq 0\}\,\,\,y\cceq 2\cdot x\,\,\,\{y\geq 0\}\]
  che però equivale algebricamente a:
  \[\vdash_{ass}\{x\geq 0\}\,\,\,y\cceq -2\cdot x\,\,\,\{y\geq 0\}\]
  e quindi è dimostrabile che:
  \[\vdash \{\top\}\,\,\,S\,\,\,\{y\geq 0\}\]
\end{esempio}
\subsection{Iterazione}
% Per proseguire bisogna definire meglio il concetto di \textbf{invariante}.
\begin{definizione}
  Definiamo:
  \begin{itemize}
    \item \textbf{correttezza parziale}, dove la tripla viene letta supponendo a
    priori che l'esecuzione termini
    \item \textbf{correttezza totale}, dove la tripla viene letta dovendo anche
    dimostrare che l'esecuzione termini. Si procede quindi prima dimostrando la
    correttezza parziale aggiungendo poi la dimostrazione per l'esecuzione
    finita
  \end{itemize}
\end{definizione}
\begin{definizione}
  La sesta regola è quella relativa all'iterazione. Abbiamo quindi:
  \[\{p\}\mbox{ while \textit{B} do \textit{C} endwhile }\{q\}\]
\end{definizione}
  ma non posso determinare a priori quante volte si eseguirà $C$, che modifica
  lo stato della memoria. Per comodità $\mbox{ while \textit{B} do \textit{C}
    endwhile }$ la chiameremo $W$, quindi in modo compatto abbiamo:
  \[\{p\}\,\,\,W\,\,\,\{q\}\]
  Partiamo con la correttezza parziale supponendo a priori che l'esecuzione
  termini. Si ha quindi che in $q$ sicuramente $B$ è falsa, altrimenti non si
  avrebbe terminazione. Quindi in realtà abbiamo:
  \[\{p\}\,\,\,W\,\,\,\{q\land \neg B\}\]
  Ipotizziamo che all'inizio $B$ sia vera, quindi la pre-condizione sarà
  $\{i\land B\}$, con $i$ rappresentante una nuova formula. In questi stati
  eseguiremo $C$. Suppongo di poter derivare, tramite $C$ eseguito una sola
  volta, nuovamente $i$:
  \[\{i\land B\}\,\,\,C\,\,\,\{i\}\]
  Se vale questa tripla significa che $i$ è un \textbf{invariante} per $C$ e
  quindi $i$ viene chiamata \textbf{invariante di ciclo}. Nello stato raggiunto
  dopo $C$ la condizione $B$ può essere valida o meno. \\ Qualora valga dopo la
  singola esecuzione di $C$ allora avrei ancora $\{i\land B\}$ e dovrei eseguire
  nuovamente il programma. Questo comporta la continua validità $i$ e bisognerà ristudiare $B$
  per capire come procedere, in quanto $i$ resterà vera per qualsiasi numero di
  iterazioni di $C$. Si ha che $i$ resterà vera, teoricamente, anche una volta
  ``usciti'' dal ciclo. \\Qualora non valga $B$ si ha che:
  \[\{i\land \neg B\}\,\,\,W\,\,\,\{i\land \neg B\}\]
  (dove si nota che $i$ resta vera ma $B$ impedisce di tornare nel ciclo).\\
  Si ha quindi che:
  \[\frac{\{i\land B\}\,\,\,C\,\,\,\{i\}}{\{i\}\,\,\,W\,\,\,\{i\land \neg B\}}\]
  che è la \textbf{regola dell'iterazione}. Scritta in modo completo:
  \[\frac{\{i\land B\}\,\,\,C\,\,\,\{i\}}{\{i\}\mbox{ while \textit{B} do
        \textit{C} endwhile }\{i\land \neg B\}}\]
\begin{definizione}
Una nozione più forte di \textbf{invariante} può essere espressa dicendo che
  \[\{i\}\,\,\,C\,\,\,\{i\}\]
  che si differenzia da quella di \textbf{invariante di ciclo} (dove mi
  interessa sapere che un'invariante sia vero prima del ciclo anche se questa
  non è un proprietà intrinseca degli invarianti):
  \[\{i\land B\}\,\,\,C\,\,\,\{i\}\]
\end{definizione}
Ricordiamo che stiamo dando per scontata la \textbf{terminazione} tramite la
\textbf{correttezza parziale}.\\
Facciamo qualche osservazione:
  \begin{itemize}
    \item nella pre-condizione della conclusione non si ha $B$, in quanto il
    corpo dell'iterazione può anche non essere mai eseguito
    \item data un'istruzione iterativa posso avere più di un'invariante. Si ha
    inoltre che ogni formula iterativa ha l'\textbf{invariante banale}
    $i=\top$. Possiamo avere anche una formula in cui compaiono variabili che
    non sono modificate della funzione iterativa e quindi l'intera formula è un
    \textit{invariante di ciclo}. Studiamo quindi le invarianti ``più utili''
    \item nei casi pratici non consideriamo ovviamente iterazioni isolate ma
    iterazioni inserite in un programma. In questi casi quindi la scelta di un
    invariante adeguato dipende sia dall'iterazione che dall'intero contesto.
    \begin{esempio}
      Ho un programma su cui voglio dimostrare:
      \[\{p\}\mbox{ \textit{C; W; D} }\{q\}\]
      con $W$ iterazione e $C,D$ comandi.\\
      Spezziamo quindi il programma per fare la dimostrazione, tramite la regola
      della sequenza:
      \[\{p\}\mbox{ \textit{C} }\{r\}\mbox{ \textit{W} }\{z\}\mbox{ \textit{D}
        }\{q\}\]
      $r$ sarà quindi la pre-condizione dell'iterazione $W$ che porterà a $z$ che
      sarà pre-condizione di $D$.\\
      Sapendo che la regola di derivazione per $W$ è:
      \[\{i\}\mbox{ \textit{W} }\{i\land\neg B\}\]
      Confronto la tripla con la ``catena'' sopra espressa. Si nota che $r$
      dovrà implicare l'invariante per $W$.
    \end{esempio}
  \end{itemize}
\begin{esempio}
  Vediamo quindi un esempio completo.\\
  Si prenda il seguente programma:
  \begin{listing}[H]
    \begin{lstlisting}
      i ~ 0; s ~ 1;
      while i < N do
        i ~ i + 1;
        s ~ s * x;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
    \label{E:W}
  \end{listing}
  \textit{Per comodità chiamo $A$ i due assegnamenti iniziali,
      $W$ l'iterazione e $C$ il corpo dell'iterazione.}\\
  Ci proponiamo di derivare:
  \[\{N\geq 0\}\mbox{ \textit{P} }\{s\cceq x^N\}\]
  $x$ e $N$ sono variabili, nella realtà costanti non venendo mai modificate da
  $P$, e il loro valore fa parte dello stato della memoria.\\
  Concentriamoci in primis sull'iterazione cercando un'invariante. \textbf{Una strategia
  semplice è quella di simulare i primi passi di una esecuzione.} Supponendo $N$
  comunque non nullo abbiamo, seguendo le due variabili $i$ e $s$ nelle varie
  iterazioni (procedendo verso destra nella tabella), procedendo in modo
  \textit{simbolico} per $s$ (usando quindi il simbolo $x$ direttamente):
  \begin{table}[H]
    \centering
    \begin{tabular}[H]{c|ccccc}
      i & 0 & 1 & 2 & 3 & $\ldots$\\
      s & 1 & $x$ & $x^2$ & $x^3$ & $\ldots$
    \end{tabular}
  \end{table}
  Si noti che $s= x^i$ sembrerebbe la nostra invariante, dimostriamo ciò tramite ricordando che 
  \[\{i\land B\}\,\,\,C\,\,\,\{i\}\] rappresenta la premessa della regola d'interazione, che equivale in questo caso a:
  \[\{s=x^i\land i<N\}\mbox{ \textit{C} }\{s=x^i\}\]
  Procedo quindi con la dimostrazione, avendo l'assegnamento:
  \[\vdash_{ass}\{sx=x^{i+1}\}\mbox{ \textit{C} }\{s=x^i\}\]
  infatti i due assegnamenti sono \emph{indipendenti}, permettendo di applicare
  in una sola volta due regole di assegnamento.\\
  Sapendo che $s\cancel{x}=x^{i+\cancel{i}}\implies s=x^i$. Ho quindi mostrato
  che:
  \[\vdash \{s=x^i\}\mbox{ \textit{C} }\{s=x^i\}\]
  dimostrando che ho effettivamente l'invariante $s= x^i$ (poiché non varia all'inizio e alla fine, e nel dettaglio
  abbiamo trovato un \textbf{invariante forte}, che lo è a priori rispetto alla
  condizione booleana $B$).\\
  Vogliamo però ottenere come pre-condizione $\{s=x^i\land i<N\}$. Sapendo però
  che $s= x^i\land i<N$ è una regola ``più forte'' di $s= x^i$ (se è vera la
  prima sicuramente è vera anche la seconda) posso usare
  l'implicazione e dire che:
  \[\{s=x^i\land i<N\}\implies \{s= x^i\}\]
  quindi:
  \[\vdash_{impl} \{s=x^i\land i<N\}\mbox{ \textit{C} }\{s=x^i\}\]
  posso quindi applicare la regola dell'iterazione avendo la premessa corretta e
  ottenendo quindi:
  \[\vdash_{iter}\{s=x^i\}\mbox{ \textit{W} }\{s=x^i\land i\geq N\}\]
  ma la post-condizione non ci sta implicando $s=x^N$, per avere:
  \[\{N\geq 0\}\mbox{ \textit{P} }\{s\cceq x^N\}\]
  bisogna quindi \textbf{rafforzare} l'invariante, in modo che l'invariante
  implichi che alla fine dell'esecuzione $i=N$.\\
  Procediamo quindi con una nuova ipotesi, cercando di dimostrare che $i\leq N$
  è un'invariante:
  \[\{i\leq N\land i<N\}\mbox{ \textit{C} }\{i\leq N\}\]
  Questa pre-condizione contiene una formula che è più restrittiva dell'altra.
  Procedo con l'assegnamento (CAPIRE QUESTA PARTE):
  \[\vdash_{ass}\{i+1\leq N\}\mbox{ \textit{C} }\{i\leq N\}\]
  ma sapendo che $i<N\implies i+1\leq N$ ottengo:
  \[\vdash_{impl}\{i< N\}\mbox{ \textit{C} }\{i\leq N\}\]
  Dimostrando quindi che è un'invariante.\\
  Passo quindi all'iterazione:
  \[\vdash_{iter}\{i\leq N\}\mbox{ \textit{W} }\{i\leq N\land i\geq N\}\]
  e la post-condizione è uguale a dire che $i=N$.\\
  Considerando quindi i due invarianti ottengo:
  \[\vdash_{iter}\{s=x^i\land i\leq N\}\mbox{ \textit{W} }
    \{s=x^i\land i= N\}\]
  Dobbiamo però dimostrare anche la parte degli assegnamenti iniziali:
  \[\{N\geq 0\}\mbox{ \textit{A} }\{s=x^i\land i\leq N\}\]
  Bisogna infatti vedere se anche le condizioni iniziali permettono
  all'invariante doppio di restare tale.\\
  Applico quindi due volte l'assegnamento (partendo dal fondo); il primo con
  $s\cceq 1$: 
  \[\vdash_{ass}\{1=x^i\land i\leq N\}\mbox{ $s\cceq 1$ }\{s=x^i\land i\leq N\}\]
  Passo quindi a $i\cceq 0$ seguendo il nuovo ordine delle condizioni:
  \[\vdash_{ass}\{1=x^0\land 0\leq N\}\mbox{ $i\cceq 0$ }\{1=x^i\land i\leq N\}\]
  Applico quindi la sequenza con la prima parte (sapendo $1=x^0$ è sempre vero):
  \[\vdash_{seq}\{N\geq 0\}\mbox{ \textit{A} }\{s=x^i\land i\leq N\}\]
  completando la dimostrazioni.\\
  Da notare solo che $x$ deve essere non nullo.
\end{esempio}
\textbf{\textit{Altri esempi sono presenti sulla pagina del corso}}.\\
\subsection{Correttezza totale}
Analizziamo ora il caso in cui non si abbia certezza di terminazione di
un'iterazione:
\[\{p\}\mbox{ while \textit{B} do \textit{C} endwhile }\{q\}\]
Distinguiamo i due tipi di correttezza, dal punto di vista della derivabilità,
tramite: 
\[\vdash^{parz}\{p\}\mbox{ C }\{q\}\]
\[e\]
\[\vdash^{tot}\{p\}\mbox{ C }\{q\}\]
Dal punto di vista semantico non ha invece senso distinguere di due casi e
quindi si ha solo:
\[\vDash\{p\}\mbox{ C }\{q\}\]
In quanto dal punto di vista semantico o si ha terminazione o non si ha, non si
hanno casistiche differenti a seconda di correttezza totale o parziale.\\
Bisognerà quindi dimostrare la terminazione.\\
Studiamo il caso semplice dove:
\[W=\mbox{ while \textit{B} do \textit{C} endwhile }\]
Cerchiamo un'espressione aritmetica $E$, dove compaiono le variabili del
programma, costanti numeriche e operazioni aritmetiche. Cerchiamo anche un
\textbf{invariante di ciclo} $i$ (che quindi è una formula) per $W$. $E$ e $i$
devono soddisfare due condizioni:
\begin{enumerate}
  \item $i\implies E\geq 0$, quindi se vale $i$ allora l'espressione aritmetica
  ha valore $\geq 0$ (il valore di $E$ lo ottengo eseguendo le operazioni sulle
  costanti e sui valori, in quello stato della memoria, delle variabili)
  \item $\vdash^{tot}\{i\land B\land E=k\}\mbox{ C }\{i\land E<k\}$, dove
  nella pre-condizione abbiamo la congiunzione logica tra l'invariante $i$, la
  condizione di ciclo $B$ e tra $E=k$, avendo prima assegnato a $k$ il valore
  effettivo di $E$ nello stato in cui iniziamo a computare $C$. $k$ quindi non
  deve essere una variabile del programma e non deve apparire in $C$
  (rappresentante il corpo della singola iterazione). Se vale la
  condizione 1 e l'invariante sappiamo che $E\geq 0\implies k\geq 0$. La
  post-condizione è formata dall'invariante $i$ e dal fatto che il valore di $E$
  dopo l'esecuzione sia strettamente minore di $k$ (che è il valore di $E$ prima
  dell'esecuzione). In pratica $E$ decresce ad ogni singola iterazione, ovvero
  ad ogni esecuzione di $C$.\\
  La notazione $\vdash^{tot}$ serve a escludere che $C$ abbia altri cicli
  annidati (portando a dover dimostrare che anche i cicli interni
  terminano). Per praticità quindi supponiamo di non avere cicli interni (stiamo
  partendo dal ciclo più interno)
\end{enumerate}
\textit{Spesso si è visrto che $E$ si ricava da $B$. Se $B$ è della forma $x>y$ 
allora $E$ sarà $x-y$. In caso di $<$, $\leq$ e $\geq$ ricondursi a $>$. Si 
specifica che non è una regola formale ma si è visto che si fa praticamente 
sempre così}.\\
In pratica, nella seconda condizione, uso $E$ per concludere che una singola
esecuzione dell'iterazione mi porta in uno stato in cui vale l'invariante, dove
può valere o meno $B$ (che potrebbe portare all'uscita dall'iterazione) ma in
cui $E$ ha un valore diverso, minore a quello di partenza ma mai minore di 0 per
la prima condizione (in quanto l'invariante è sempre valido). Quindi, ad un
certo punto, $E$ raggiungerà il valore minimo e in quel momento o $B$ è falsa o
si ha una contraddizione ($E$ dovrebbe diventare negativo), quindi si ha la
terminazione. \\
Quindi se abbiamo dimostrato le due condizioni posso dire:
\[\vdash^{tot}\{i\}\mbox{ W }\{i\land\neg B\}\]
che è anche la conclusione della regola di derivazione dell'iterazione in caso
di correttezza parziale.\\
Possiamo anche osservare due cose:
\begin{enumerate}
  \item $E$ non è una formula logica ma un'espressione aritmetica con valore
  numerico ($E\geq 0$ è una formula logica)
  \item qualora si abbia $E=0$ non si hanno problemi, $0$ è solo un esempio,
  potremmo riscrivere la prima condizione come $E\geq n$, con $n$ qualsiasi
  numero intero, basta che sia ben definito per poter permettere la ripetizione
  finita delle iterazioni
\end{enumerate}
Chiameremo questa espressione aritmetica è \textbf{variante}.\\
\textit{L'invariante della correttezza totale non è sempre quello di quella
  parziale, magari è uguale, magari diverso o anche uguale solo in parte}
\begin{esempio}
  Vediamo un esempio chiarificatore.\\
  Dato il programma (una sorta di conto alla rovescia):
  \begin{listing}[H]
    \begin{lstlisting}
      while x > 5 do
        x ~ x - 1;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
    \label{E:t}
  \end{listing}
  studio la correttezza totale della tripla:
  \[\{x>5\}\mbox{ P }\{x=5\}\]
  Innanzitutto cerco un variante $E$, che decresce ad ogni singola iterazione è
  un'invariante $i$ come descritti sopra, quindi che, qualora ci sia
  l'invariante $i$ allora  $E\geq 0$.\\
  Un primo invariante ``banale'' è $i=x\geq 5$.\\
  In merito ad $E$ notiamo che nel corpo dell'iterazione abbiamo un solo
  comando, dove il valore della variabile $x$ viene decrementato, quindi un
  primo candidato per $E$ potrebbe essere proprio $x$.\\
  Per comodità scegliamo però come variante $x-5$ per avere una
  corrispondenza diretta con l'invariante $x\geq 5$, essendo derivato dallo
  stesso invariante (basandoci in primis sulla prima condizione).\\
  \textbf{Non sempre posso ottenere una corrispondenza tra variante e
    invariante}.\\
  Presi questo invariante e questo variante verifichiamo le due condizioni:
  \begin{enumerate}
    \item $x\geq 5\implies x-5\geq 0$ è ovviamente corretto perché si ottiene
    che $x\geq 5\implies x \geq 5$ (infatti questo è il motivo per cui si è
    scelto $x-5$ come variante)
    \item $\vdash^{tot}\{x\geq 5\land x>5\land x-5=k\}\,\,\,x\cceq
    x-1\,\,\,\{x\geq 5\land x-5<k\}$\\
    Applico quindi la regola dell'assegnamento (che si applica anche per la
    correttezza totale) e ottengo la pre-condizione per la post-condizione
    $\{x\geq 5\land x-5<k\}$:
    \[\{x-1\geq 5\land x-1-5<k\}=\{x\geq 6\land x-6 < k\}\]
    che però è diversa da $\{x\geq 5\land x>5\land x-5=k\}$.\\
    Cerco quindi di applicare la regola dell'implicazione. Riguardando la
    pre-condizione originale noto che $x>5$ è ``più forte'' di $x\geq 5$ e quindi
    quest'ultimo può essere trascurato. Ricordando che stiamo lavorando su
    valori interi si ha che $x>5\implies x\geq 6$. Inoltre, sempre per il fatto
    che lavoriamo su numeri interi, \\
    $x-5=k\implies x-6<k$ e quindi:
    \[\{x\geq 5\land x>5\land x-5=k\}\implies \{x\geq 6\land x-6 < k\}\]
  \end{enumerate}
  Abbiamo quindi dimostrato che il programma termina e vale la tripla
  iniziale.\\
  Qualora avessimo dovuto dimostrare la correttezza parziale si sarebbe dovuto
  procedere riutilizzando lo stesso invariante e procedendo studiando la
  negazione di $B$, ovvero $x\leq 5$ (che insieme danno la post-condizione).
\end{esempio}
\begin{esempio}
  vediamo un esempio non si ha alcuna variabile che decrementa nel corpo
  dell'iterazione:
  \begin{listing}[H]
    \begin{lstlisting}
      while x < 5 do
        x ~ x + 1;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
    \label{E:ti}
  \end{listing}
  studio la correttezza totale della tripla:
  \[\{x<5\}\mbox{ P }\{x=5\}\]
  Dal punto di vista della correttezza parziale ragiono come al solito mentre
  per la correttezza totale la situazione è un po' diversa.\\
  Ovviamente non posso usare $x$ come variante ma posso sicuramente usare, per
  esempio, $-x$, che sicuramente decresce visto che $x$ cresce, in entrambi i
  casi ad ogni iterazione. Riprendendo l'esempio sopra quindi posso usare $x\leq
  5$ come invariante e, al posto di $-x$ che comunque andrebbe bene, secondo una
  ragionamento simile allo scorso esempio, $5-x$ come variante (si nota che
  anche questo $E$ è ricavabile da $i$, anche se questo non è sempre attuabile).
  Il resto della dimostrazione è analoga all'esempio precedente.
\end{esempio}
\subsubsection{Correttezza e Completezza}
In generale quando si sviluppa una logica, con un apparato deduttivo e
un'interpretazione delle formule, siamo interessati a due proprietà generali
della logica e dell'apparato deduttivo:
\begin{enumerate}
  \item \textbf{correttezza}, ovvero il fatto che tutto quello che si può
  derivare con l'apparato deduttivo è effettivamente vero, ovvero
  $\vdash\implies\vDash$. Quindi se una tripla è derivabile è anche vera
  \item \textbf{completezza}, ovvero il fatto che l'apparato deduttivo sia in
  grado di derivare tutte le formule vere (ovvero tutte le triple) vere. Si ha
  quindi $\vDash\implies\vdash$
\end{enumerate}
Per la logica di Hoare vale la correttezza ma vale una proprietà di completezza
è \textit{relativa} in quanto nel corso di una dimostrazione di completezza
occorre a volte usare deduzioni della logica proposizionale ma tali formule
parlano anche di operazioni aritmetiche. Si ha che l'aritmetica può essere
formalizzata attraverso un linguaggio logico con degli assiomi e delle regole di
inferenza. Si hanno però i \textbf{teoremi di incompletezza dell'aritmetica} di
G\"{o}del che dicono che l'aritmetica come teoria matematica è incompleta in
quanto ci sono formule scritte nel linguaggio dell'aritmetica che sono vere ma
non sono dimostrabili. Questa incompletezza si riverbera sulla logica di Hoare,
che comunque, dal punto di vista deduttivo sulle triple, è completa.
\subsection{pre-condizione più debole}
Dato un comando $C$ e una formula $q$ che interpretiamo come post condizione di
$C$. Si cerca $p$ tale per cui:
\[\vdash\{p\}\mbox{ C } \{q\}\]
Sia valida e quindi vera.\\
Ovviamente il caso interessante è quello totale.
\begin{esempio}
  Suppongo di avere come comando un singolo assegnamento $y\cceq 2\cdot x-1$ e
  come post-condizione $\{y>x\}$.\\
  Cerco possibili precondizioni che, dopo l'assegnamento, portino a quella
  post-condizione. Ovviamente si avrebbero infiniti valori di $x$ che consentono
  di arrivare alla post-condizione (nonché altrettanti che non lo permettono). 
\end{esempio}
Si cerca quindi la ``migliore'' pre-condizione per arrivare ad una data
post-condizione ma per farlo ci serve un criterio di confronto. \\
Introduciamo quindi alcuni simboli e nozioni utili:
\begin{itemize}
  \item $V$ è l'insieme della variabili di $C$
  \item $\Sigma=\{\sigma|\,\sigma:V\to\mathbb{Z}\}$ è l'insieme degli stati
  della memoria (ricordando che posso avere solo valori interi nel nostro
  linguaggio)
  \item $\Pi$ è l'insieme di tutte le formule sull'insieme $V$
  \item $\sigma \vDash p$ significa che la formula $p$ è vera nello stato
  $\sigma$ e si ha che $\vDash\subseteq\Sigma\times \Pi$, con $\vDash$ che
  indica la veridicità di una formula in uno stato
  \item $t(\sigma)=\{p\in \Pi|\,\sigma\vDash p\}$ come la funzione che assegna
  ad uno stato l'insieme delle proposizioni, ovvero tutte le formule, che sono
  vere in $\sigma$
  \item $m(p)=\{\sigma\in \Sigma|\,\sigma\vDash p\}$ come la funzione che
  associa ad una formula l'insieme di tutti gli stati che soddisfano la formula
\end{itemize}
Tra le due funzioni finali si ha una sorta di ``dualità'' e agiscono in modo
speculare tra loro. Se, partendo da $\Sigma$, applico $t(\sigma)$ e scopriamo
che un certo stato $p$ appartiene a $\Pi$ e a tale $p$ associamo l'insieme dato
da $m(p)$ si avrà che $\sigma\in m(p)$.\\
Si possono fare altre osservazioni su queste due funzioni. \\
Prendiamo due formule $p$ e $q$ e cerco di capire se è possibile avere
$m(p)=m(q)$. La risposta è positiva e si parla, in caso, di \textit{equivalenza
  tra formule}. Fissati due stati $\sigma_1$ e $\sigma_2$ mi chiedo se possano
appartenere allo stesso insieme di formule. In questo caso la risposta è
negativa, in quanto se i due stati sono distinti allora ci deve essere almeno
una variabile $x$ che ha valore diverso nei due stati ma allora è facile trovare
una formula che separa i due stati e tale formula potrà essere vera solo in uno
dei due stati.\\
Dato $S\subseteq \Sigma$ e $F\subseteq\Pi$ (ragiono quindi su sottoinsiemi) si
ha che:
\begin{itemize}
  \item $t(S)=\{p\in \Pi|\,\forall\,\sigma\in S,\,\,\,\sigma\vDash
  p\}=\bigcap_{\sigma\in S}t(\sigma)$ (ragiono quindi su tutti gli stati di $S$
  e quindi sulle formule che sono vere in tutti questi stati) 
  \item $m(F)=\{\sigma\in \Sigma|\,\forall\,p\in F\,\,\,\sigma\vDash
  p\}=\bigcap_{p\in \Pi}m(p)$ (ragiono quindi su tutte le formule di $\Pi$
  e quindi su tutti gli stati in devono essere soddisfatte tutte queste formule) 
\end{itemize}
Abbiamo quindi esteso il dominio delle due funzioni a sottoinsiemi di stati e
sottoinsiemi di formule.\\
Si può anche dimostrare che $S\subseteq m(t(s))$ e che $F\subseteq
t(m(F))$. Inoltre, dati $A\subseteq B$, si può dimostrare che $m(B)\subseteq
m(A)$ (se ho un insieme più grande di formule ho meno stati che le soddisfano
tutte).\\
Si ha un rapporto tra la logica proposizionale (coi suoi connettivi logici) e
l'insieme di stati ($p$ e $q$ sono formule di $\Pi$): 
\begin{itemize}
  \item $m(\neg p)=\Sigma \,\,\backslash \,\,m(p)$, quindi il complemento
  insiemistico di $p$
  \item $m(p\lor q)=m(p)\cup m(q)$, quindi all'unione dei due insiemi  
  \item $m(p\land q)=m(p)\cap m(q)$ (da verificare) quindi all'intersezione dei
  due insiemi
\end{itemize}
L'implicazione merita un discorso a parte in quanto ha due ``nature'':
\begin{itemize}
  \item \textbf{operatore logico}, che comporta che $p\implies q= \neg p\lor q$
  e in tal caso si ha, in linea a quanto detto per gli altri connettivi:
  \[m(p\implies q)= m(\neg p)\cup m(q)\]
  \item \textbf{relazione tra formule}, dove se $p$ implica $q$ (in tutti in
  casi in cui è vera $p$ è vera $q$) allora
  $m(p)\subseteq m(q)$, intendendo che $q$ è ``più debole'' (ovvero come formula
  ci da una conoscenza inferiore essendo $q$ corrispondente ad un insieme più
  grande di stati) di $p$. Si ha a che
  fare con una relazione algebrica tra le due formule. Più ``debole'' non
  significa comunque ``peggiore''
\end{itemize}
Posso quindi capire come definire la pre-condizione migliore per ottenere una
tripla valida, insieme al comando $C$ e alla post-condizione.\\
Un modo è quello di definire la migliore pre-condizione come la pre-condizione più
debole $p$ che presa come pre-condizione forma una tripla valida:
\[\vdash \{p\}\mbox{ C }\{q\}\]
se $p$ è la pre-condizione più debole allora corrisponde al più grande insieme di
stati tale che la tripla sia valida.\\
Questa è una scelta ragionevole perché è la pre-condizione che impone meno
vincoli sullo stato iniziale, infatti determina tutti gli stati iniziali che
garantiscono il raggiungimento della post-condizione. È sempre possibile
calcolare la pre-condizione più debole.\\
Indichiamo, fissati $C$ comando e $q$ formula di post-condizione, con $wp(C, q)$
la pre-condizione più debole (\textbf{weakest precondition (\textit{wp})}).
\begin{teorema}[proprietà fondamentale della condizione più debole]
  Si ha che $\vDash\{p\}\mbox{ C }\{q\}$ (quindi la tripla è vera) sse:
  \[p\implies wp(C,q)\]
\end{teorema}
\begin{teorema}
  L'\textbf{esistenza} della pre-condizione più debole è garantita.\\
  È garantita inoltre l'\textbf{unicità} della pre-condizione più debole (a meno
  di eventuali equivalenze logiche).
\end{teorema}
Vediamo quindi la \textit{regola di calcolo}.\\
Si hanno diversi casi a seconda dei tipi di comando:
\begin{itemize}
  \item \textbf{assegnamento:} in questo caso la pre-condizione più debole è
  quella determinata dalla regola di derivazione introdotta per la correttezza
  parziale. Quindi dato un assegnamento del tipo $x\cceq E$ e una post-condizione
  $q$ ho che la pre-condizione più debole si ottiene sostituendo in $q$ ogni
  occorrenza di $x$ con l'espressione $E$, ottenendo quindi:
  \[\{q[E/x]\}\]
  \item \textbf{sequenza:} in questo caso, se ho la sequenza di due comando
  $C_1$ e $C_2$ allora la pre-condizione più debole si calcola nel seguente modo:
  calcolo la pre-condizione più debole per $C_2$, ottenendo $wp(C_2,q)$, che sarà
  quindi la formula usata come post-condizione per calcolare la pre-condizione più
  debole di $C_1$, ovvero: $wp(C_1,wp(C_2,q))$. Questo non è altro che la
  condizione più debole della sequenza:
  \[wp((C_1;C_2),q)\equiv wp(C_1,wp(C_2,q))\]
  \item \textbf{scelta:} in questo caso, avendo:
  \[S=\mbox{ if \textit{B} then \textit{C} else \textit{D} endif }\]
  fisso la post-condizione $q$ e procedo come per la regola di
  derivazione. Separo i due casi in cui sia vera $B$ o meno. Se $B$ è vera devo
  eseguire $C$ quindi calcolo, eseguendo $C$ la pre-condizione più debole
  $wp(C,q)$. Qualora $B$ non sia vera calcolo, eseguendo $D$, la pre-condizione
  più debole $wp(D,q)$. Nel complesso quindi, facendo la disgiunzione dei due
  casi, ottengo: 
  \[wp(S,q)\equiv(B\land wp(C,q))\lor (\neg B\land wp(D,q))\]
  \item \textbf{iterazione:} in questo caso, avendo:
  \[W=\mbox{while \textit{B} do \textit{C} endwhile}\]
  È il caso più complicato.\\
  Si ha che se $\neg B$ è nello stato iniziale dll'iterazione allora il corpo
  non viene eseguito (arrivando direttamente alla post-condizione), invece se
  vale $B$ si avrà che $W$ equivale a $C;W$, in 
  quanto sicuramente almeno una volta eseguo $C$. Ma a questo punto potrei
  rifare lo stesso discorso se $B$ è ancora vera (questo fino a che non ho $\neg
  B$, uscendo dal ciclo e arrivando a $q$), ragionando su $W$ di $C;W$. Si
  arriva di fatto ad una regola ricorsiva:
  \[wp(W,q)\equiv(\neg B\land q)\lor(B\land wp((C;W),q))\]
  Applico quindi la regola della sequenza per la condizione più debole,
  arrivando a:
  \[wp(W,q)\equiv(\neg B\land q)\lor(B\land wp(C, wp(W,q)))\]
  Si nota che questa definizione ricorsiva ha un problema grave: non si ha un
  \textbf{caso base} che risolva la catena di chiamate ricorsive.\\
  Purtroppo non si può usare il ``trucco'' dell'invariante come nella regola di
  definizione e questo comporta che non si ha un vero e proprio algoritmo unico
  ed effettivo per questa regola di ricerca della pre-condizione più debole
\end{itemize}
Si hanno dei casi estremi, ad esempio:
\begin{itemize}
  \item precondizioni più deboli sempre false, ad esempio:
  \[wp(x\cceq5,x<0)\equiv\bot\]
  ovvero se assegno a $x$ il valore 5 non avrò mai $x$ negativo,
  ovvero ho un insieme vuoto $\emptyset$ di stati che garantiscono quanto detto
  \item precondizioni più deboli sempre vere, ad esempio
  \[wp(x\cceq 5,x\geq0)\equiv\top\]
  ovvero se assegno a $x$ il valore 5 si avrà che $x$ è sempre
  positivo, quindi è vero per qualunque stato iniziale
\end{itemize}
\subsubsection{Esempi vari}
\begin{esempio}
  Vediamo un primo esempio semplice con solo la sequenza di due assegnamenti:
  \begin{listing}[H]
    \begin{lstlisting}
      x ~ x+1;
      y ~ y*x;
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Dove i due assegnamenti sono rispettivamente $A$ e $B$ e si vuole la
  post-condizione $q=x<y$. Cerco quindi $wp(P,q)$.\\
  Procediamo con la formula della sequenza (che ricordiamo procede ``a
  ritroso'').
  \begin{itemize}
    \item Parto dal secondo assegnamento, $B$, e calcolo $wp(B,q)$.\\
    Procedo con la regola e applico la sostituzione, ottenendo:
    \[r=wp(B,q)=x<y\cdot x\]
    quindi se $x<y\cdot x$ vale prima dell'esecuzione di $B$ allora sicuramente
    si raggiunge uno stato finale dove vale la post-condizione $q$ 
    \item lo stato in cui eseguiamo $B$, che abbiamo sopra chiamato $r$, è
    quello prodotto dall'esecuzione di $A$. Dobbiamo quindi calcolare
    $wp(A,r)$.\\
    Procedo quindi ancora con l'assegnamento, ottenendo:
    \[wp(A,r)\equiv x+1<y(x+1)\]
    che quindi, per composizione, è anche la pre-condizione più debole per $P$ e
    $q$. Bisogna però fare dei controlli.\\
    Uso quindi le regole delle disequazioni:
    \begin{itemize}
      \item caso 1: $x+1>0\implies y>1$
      \item caso 2: $x+1=0\implies 0<0$ e quindi non vale $q$
      \item caso 3: $x+1<0\implies y<0$
    \end{itemize}
    Costruisco quindi la pre-condizione più debole prendendo la disgiunzione
    logica dei due casi validi:
    \[wp(P,q)\equiv(x\geq 0\land y>1)\lor (x<-1\land y<1)\]
    (sempre ricordando che lavoriamo con interi)
  \end{itemize}
\end{esempio}
\begin{esempio}
   Vediamo un altro esempio:
  \begin{listing}[H]
    \begin{lstlisting}
      x ~ x+a;
      y ~ y-1;
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
Dove i due assegnamenti sono rispettivamente $A$ e $B$ e si vuole la
post-condizione $q=(x=(b-y)\cdot a)$. Cerco quindi $wp(P,q)$.\\
Nella post-condizione ho una variabile $b$ esterna alla sequenza di interesse
fissato a priori.\\
Come prima ottengo:
\[r=wp(B,q)\equiv(x=(b-y+1)\cdot a)=(x=(b-y)\cdot a)\]
e quindi:
\[wp(A,r)\equiv(x+a=(b-y)\cdot a+a)\]
quindi $x=(b-y)\cdot a$ è un'invariante ed è anche la post-condizione, Quindi $q$
è un'invariante, accezione più forte del termine, per $P$.
\end{esempio}
\begin{esempio}
  Vediamo un altro esempio:
  \begin{listing}[H]
    \begin{lstlisting}
      if y = 0 then
        x ~ 0;
      else
        x ~ x * y;
      endif
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Dove i due comandi sono rispettivamente $C$ e $D$ e la condizione booleana è
  $B$. Come post-condizione voglio $q=(x=y)$.\\
  Per il caso della scelta bisogna calcolare i due casi e usare poi la
  disgiunzione tra essi.\\
  Calcolo quindi:
  \begin{enumerate}
    \item il caso in cui vale $B$: $wp(C,q)=(0=y)$, usando l'assegnamento
    \item il caso in cui vale $\neg B$: $wp(D,q)=(x\cdot y)=y$, usando
    assegnamento. La formula però comporta che ho due casi possibili: $y=0\lor
    x=1$ (il secondo appunto se ho $y\neq 0$)
  \end{enumerate}
  Applico quindi la regola della scelta, ottenendo:
  \[wp(P,q)\equiv (y=0\land y=0)\lor(y\neq 0\land (y=0\lor x=1))\]
  Semplificando si ha che:
  \[wp(P,q)\equiv(y=0)\lor(x=1\land y\neq 0)\]
\end{esempio}
\begin{esempio}
   Vediamo un altro esempio:
  \begin{listing}[H]
    \begin{lstlisting}
      while x > 0 do
        x ~ x - 1
      endwhile
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Con la condizione $B$ e il comando $C$, nel corpo dell'iterazione $W$. Come
  post-condizione si vuole $q=(x=0)$.\\
  Seguendo le modalità discusse in merito alle tecniche relative al calcolo
  della pre-condizione più debole per l'iterazione, vediamo che:
  \[(\neg B\land q)\equiv (x\leq 0\land x=0)\equiv (x=0)\]
  Per comodità chiamo $wp(W,q)$ $r$.\\ 
  Dobbiamo ora studiare $B\land wp(C,wp(W,q))$ che quindi per noi è, per l'alias
  appena definito, $B\land wp(C,r)$:
  \[B\land wp(C,r)\equiv x>0\land r[x-1/x]\]
  usando quindi l'assegnamento e quindi si ha che:
  \[B\land wp(C,r)\equiv (x>0)\land (x-1=0\lor (x-1>0\land r[x-2/x]))\]
  Siamo quindi in piena ricorsione.\\
  Allo stato attuale ho quindi (usando $ldots$ per non dover riscrivere tutto):
  \[(\neg B\land q)(B\land wp(C,r))\equiv (x=0 \lor(x>0\land(\ldots)))\]
  in realtà, andando avanti, si otterrebbe uno sviluppo regolare che porterebbe
  alla formula infinita:
  \[(x=0\lor x=1\lor x=2\lor \ldots)\equiv x\geq 0\]
  e quindi al pre-condizione più debole diventa:
  \[wp(W,q)=(x\geq 0)\]
  Si nota quindi come i casi di iterazione si risolvono solo con tecniche ``non
  rigorose'' \textit{ad hoc}.
\end{esempio}
\subsubsection{Estensioni del linguaggio}
Abbiamo, in conclusione, sviluppato una tecnica di studio basata su un
linguaggio di programmazione molto semplificato. Innanzitutto potremmo estendere
il linguaggio usato, aggiungendo:
\begin{itemize}
  \item il \textbf{do-while}:
  \[\mbox{do \textit{C} while \textit{B} endwhile}\]
  che nella realtà corrisponde a:
  \[\mbox{\textit{C}; while \textit{B} do
      \textnormal{C} endwhile}\]
  \item il \textbf{repeat-until}:
  \[\mbox{repeat \textit{C} until \textit{B} endrepeat}\]
  che nella realtà corrisponde a:
  \[\mbox{\textit{C}; while not \textit{B} do
      \textit{C} endwhile}\]
  \item il \textbf{ciclo for}:
  \[\mbox{for(\textit{D; B; F}) \textit{C} endfor} \]
  sempre convertendolo in un while:
   \[\mbox{\textit{D}; while \textit{B} do
      \textit{C; F}  endwhile}\]
  \item \textbf{procedure, metodi e funzioni}
  \item \textbf{array}, anche se solo in lettura se vogliamo applicare la logica
  di Hoare come l'abbiamo vista
\end{itemize}
Un altro limite è dato dal fatto che abbiamo usato solo il tipo intero, ma
possiamo potenzialmente aggiungere anche gli altri senza cambiare le basi della
logica introdotte.
\subsection{Logica di Hoare per sviluppare programmi}
Possiamo vedere le logiche di Hoare come \textbf{contratti} tra chi scrive il
programma e l'utente. In questo contesto l'utente commissiona il programma
specificando la post-condizione e chiede al programmatore di garantire che tale
post-condizione venga garantita al termine dell'esecuzione. Per garantire la
post-condizione $q$ deve essere vera la pre-condizione $p$.\\
Vediamo un esempio.
\begin{esempio}
  Ci proponiamo di calcolare la radice quadrata intera di un certo $k\geq 0$,
  approssimando per difetto.\\
  Alla fine del programma voglio il risultato nella variabile $x$, quindi
  voglio:
  \[\{k\geq 0\}\mbox{ P }\{0\leq x^2\leq k<(x+1)^2)\}\]
  con $(x+1)^2$ per la correttezza dell'approssimazione.\\
  Ci si propone di fare il calcolo per approssimazioni successive, partendo da
  un valore più piccolo di quello corretto.\\
  Possiamo spaccare $q$ in:
  \[0\leq x^2\leq k \,\,\,\mbox{ e }\,\,\,k<(x+1)^2\]
  la prima possiamo considerarla come \textit{invariante} (anche solo $x^2\leq
  k$).\\
  
  All'inizio possiamo pensare che $x$ dipenda da $k$ per una certa funzione $E$.
  Si ha quindi un prototipo del genere (con un certo $B$, condizione booleana, e
  un certo $F$, incremento nel corpo, dipendenti da $x$ e $k$):
  \begin{listing}[H]
    \begin{lstlisting}
      x ~ E(x);
      while B(x,k) do
        x ~ F(x,k);
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Alla fine deve valere $x^2\leq k \land \neg B$ per la regola
  dell'iterazione.\\
  Come $B$ posso porre $(x+1)^2\leq k$ (ovvero la negazione della seconda parte
  della post-condizione, in modo da ottenere, con la negazione, $q$).\\ Si
  ottiene quindi: 
  \begin{listing}[H]
    \begin{lstlisting}
      x ~ 0;
      while (x+1)^2 <= k do
        x ~ x+1;
      endwhile  
    \end{lstlisting}
    \caption{Programma $P$}
  \end{listing}
  Bisognerebbe comunque dimostrare invariante e terminazione per validare il
  programma. 
\end{esempio}
Supponiamo un programma del tipo:
\[\{p\}\mbox{ A;W;C } \{q\}\]
che si risolve schematicamente con:
\[\{p\}\mbox{ A } \{inv\} \mbox{ W } \{inv\land \neg B\}\mbox{ C }\{q\}\]
Questo schema usa i cosiddetti \textbf{invarianti costruttivi}.\\
\subsection{Ultime considerazioni}
Vediamo qualche ultima considerazione sulla logica di Hoare.
\begin{itemize}
  \item alcune applicazioni pratiche della logica di Hoare:
  \begin{itemize}
    \item \textit{java modelling language}, un linguaggio di specificazione
    scritto in Java, che permette di fare \textit{design by contract} stabilendo
    delle precondizioni e delle postcondizioni 
    \item \textit{Eiffel, programming by contract}
    \item \textit{assert.h} in C
  \end{itemize}
  
  \item alcune applicazioni teoriche della logica di Hoare:
  \begin{itemize}
    \item \textit{semantica del programmi}, ovvero lo studio di cosa significa
    un programma, tramite:
    \begin{itemize}
      \item \textit{semantiche operazionali}, dove al programma
      viene associata una macchina astratta e, dato un programma viene associata
      una computazione sulla macchina astratta (come ad esempio la
      \textbf{Macchina di Turing} o le \textbf{Macchine a Registri})
      \item \textit{semantiche assiomatiche}, dove vediamo, mano a mano che
      viene eseguito il programma, le asserzioni che vengono verificate
      \item \textit{semantiche denotazionali}, in cui un programma è visto come
      un trasformatore da input e output. Come una $f:I\to O$. SI basa sul
      \textbf{lambda calcolo}
      \item \textit{semantiche operazionali strutturate}
    \end{itemize}
  \end{itemize}
\end{itemize}
Si hanno quindi i due punti chiave che devono essere garantiti:
\begin{enumerate}
  \item \textbf{terminazione} del programma
  \item \textbf{composizionalità} tra più comandi per ottenere un programma
\end{enumerate}
Le triple di Hoare vivono ancora nella \textbf{programmazione by contract}.