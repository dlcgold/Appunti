\documentclass[a4paper,12pt, oneside]{book}

% \usepackage{fullpage}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{engrec}
\usepackage{rotating}
\usepackage[safe,extra]{tipa}
\usepackage{showkeys}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{enumerate}
\usepackage{braket}
\usepackage{marginnote}
\usepackage{pgfplots}
\usepackage{cancel}
\usepackage{polynom}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{fancyhdr}
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[space]{grffile} % For spaces in paths
\usepackage{etoolbox} % For spaces in paths
\usepackage{fontspec} % Lettere accentate
\makeatletter % For spaces in paths
\patchcmd\Gread@eps{\@inputcheck#1 }{\@inputcheck"#1"\relax}{}{}
\makeatother
\pagestyle{fancy}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}
\fancyfoot[C]{\thepage}



\title{Algebra Lineare e Geometria}
\author{UniShare\\\\Davide Cozzi\\\href{https://t.me/dlcgold}{@dlcgold}\\\\Gabriele De Rosa\\\href{https://t.me/derogab}{@derogab} \\\\Federica Di Lauro\\\href{https://t.me/f_dila}{@f\textunderscore dila}}
\date{}

\pgfplotsset{compat=1.13}
\begin{document}
\maketitle

\definecolor{shadecolor}{gray}{0.80}

\newtheorem{teorema}{Teorema}
\newtheorem{definizione}{Definizione}
\newtheorem{esempio}{Esempio}
\newtheorem{corollario}{Corollario}
\newtheorem{lemma}{Lemma}
\newtheorem{osservazione}{Osservazione}
\newtheorem{nota}{Nota}
\newtheorem{esercizio}{Esercizio}
\tableofcontents
\renewcommand{\chaptermark}[1]{%
	\markboth{\chaptername
		\ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\newcommand{\norm}[1]{ \left\lVert {#1} \right\rVert}
\chapter{Introduzione}
Modalità d'esame: scritto+orale (si può conservare il voto dello scritto). Lo scritto prevede 2 parziali (10 domande a risposta multipla, giusta +3, sbagliata -1, somma almeno superiore a 10) si va all'orale con voto maggiore di 15.
Ricevimento: mercoledì 12.15/13.30 U5 terzo piano.
Testo: Anichini-Conti geometria analitica e algebra lineare, Pearson.\\
\\
\textbf{Questi appunti sono presi a lezione. Per quanto sia stata fatta una revisione è altamente probabile (praticamente certo) che possano contenere errori, sia di stampa che di vero e proprio contenuto. Per eventuali proposte di correzione effettuare una pull request. Link: } \url{https://github.com/dlcgold/Appunti}.\\
\textbf{Grazie mille e buono studio!}
\chapter{Matrici}
l'algebra lineare studia delle strutture algebriche per formalizzazioni specifiche.
La struttura algebrica è fatta da:
$$Matrici\,\,n\times m,\,\, con \,\, n,m\in\mathbb{N}$$
una matrice è una tabella con $n$ righe e $m$ colonne:
$$\left(\begin{matrix}
			c_{0,\,0} & \cdots & c_{0,\,m} \\
			\vdots    & \ddots & \vdots    \\
			c_{n,\,0} & \cdots & c_{n,\,m}
		\end{matrix}\right)$$
$a_{ij}$ è il numero all'incrocio tra la $i$-esima riga e la $j$-esima colonna.\\
Si definisce \textbf{Traccia} la somma degli elementi presenti sulla diagonale\\
Una matrice è detta \textit{Diagonale} se $a_{ij}=0$, per $i\neq j$, ovvero se gli elementi che non sono sulla diagonale sono nulli.
Una matrice i cui elementi sono nulli è detta \textit{Matrice Nulla}, e si scrive $A=(0)$\\
Una matrice le cui colonne sono ordinatamente le righe è detta \textit{Matrice Trasposta} e si indica con $A^T$\\
Una matrice è detta \textit{Simmetrica} se $A=A^T$, ovvero se la i-esima riga è uguale alla i-esima colonna. \\ 
Una matrice del tipo $1\times n$ è detta \textit{Matrice Riga}, mentre una del tipo $n\times 1$ è detta anche \textit{Matrice Colonna}
\section{Operazioni tra Matrici}
\subsection{Somma tra matrici}
La matrice somma è una matrice dove la posizione $a_{i,\,j}$ della matrice somma è data dalla somma delle $a_{i,\,j}$ delle matrici di partenza. Si possono sommare solo due matrici dello stesso ordine. si ha che:
$$C=(c_{ij})=(a_{ij}+b_{ij})$$
\begin{esempio}
	Si ha:\\
	$$\left(\begin{matrix}
			1 & -2 & 1 \\
			2 & 0  & 3
		\end{matrix}\right)+\left(\begin{matrix}
			0 & 2 & -1 \\
			2 & 1 & -3
		\end{matrix}\right)=\left(\begin{matrix}
			1 & 0 & 0 \\
			4 & 1 & 0
		\end{matrix}\right)$$
\end{esempio}
La somma gode delle seguenti proprietà:
\begin{itemize}
	\item $A+B=B+A$ \textit{proprietà commutativa}
	\item $A+(B+C)=(A+B)+C$ \textit{proprietà associativa}
	\item $A+(0)=(0)+A$ \textit{esistenza dell'elemento neutro, la matrice nulla}
	\item $\forall A,\, \exists (-A):\, A+(-A)=(0)$ \textit{esistenza dell'opposta, ovvero la matrice con gli elementi di A con segno invertito}
\end{itemize}
\newpage
\subsection{Prodotto di una matrice per uno Scalare}
Si ha poi il prodotto tra una matrice e uno scalare,$k\cdot A$, con $k$ costante e $A=(a_{i\,j})_{i=1,...,n}^{j=1,...,m}=\{a_{i\,j}\}_{i=1,...,n\, e \, j=1,...,m}$ matrice $n\times m$, $i$ indice delle righe, $j$ indice delle colonne, che è una matrice con i coefficienti di $A$ moltiplicati per $k$.
\begin{esempio}
	Si ha:\\
	$$k\cdot\left(\begin{matrix}
			1 & -2 & 1 \\
			2 & 0  & 3
		\end{matrix}\right)=
		\left(\begin{matrix}
			1\cdot k & -2\cdot k & 1\cdot k \\
			2\cdot k & 0\cdot k  & 3\cdot k
		\end{matrix}\right)$$
\end{esempio}
Il prodotto per uno scalare ha le seguenti proprietà:
\begin{itemize}
	\item $k\cdot(h\cdot A)=(k\cdot h)\cdot A,\, \forall k,h\in \mathbb{R}$
	\item $(k+h)\cdot A=k\cdot A+ h\cdot A,\, \forall k,h\in \mathbb{R}$
	\item $k\cdot(A+B)=k\cdot A+k\cdot B,\, \forall k\in \mathbb{R}$
	\item $1\cdot A=A$
\end{itemize}
\subsection{Prodotto di una matrice per un vettore}
Definisco una matrice $4\times 1$, è un vettore colonna:
$$v=\left(\begin{matrix}
			x_1 \\
			x_2 \\
			x_3 \\
			x_4
		\end{matrix}\right)$$
Si definisce, infine, il prodotto \textit{riga per colonna} tra una matrice e un vettore (che rappresenta gli elementi estratti da uno spazio vettoriale):
$$\left(\begin{matrix}
			0           & 0           & 1 & \frac{1}{2} \\
			\frac{1}{3} & 0           & 0 & 0           \\
			\frac{1}{3} & \frac{1}{2} & 0 & \frac{1}{2} \\
			\frac{1}{3} & \frac{1}{2} & 0 & 0
		\end{matrix}\right)\cdot \left(\begin{matrix}
			x_1 \\
			x_2 \\
			x_3 \\
			x_4
		\end{matrix}\right)=\left(\begin{matrix}
			x_3+\frac{x_4}{2}                         \\
			\frac{x_1}{3}                             \\
			\frac{x_1}{3}+\frac{x_2}{2}+\frac{x_4}{2} \\
			\frac{x_2}{3} +\frac{x_2}{2}
		\end{matrix}\right)=\left(\begin{matrix}
			x_1 \\
			x_2 \\
			x_3 \\
			x_4
		\end{matrix}\right)
$$
\textit{(In questo caso} $v$ \textit{è autovettore di }$A$ \textit{in quanto} $v=A\cdot v$\textit{)}
Inoltre siano $A$ matrice $n\times	m$ e $v$ un vettore colonna. Si ha che:
$$A\cdot v:(n\times m)(n\times 1)\rightarrow (n\times 1)=\left(\begin{matrix}
			b_1=\sum_{k=1}^n\,a_{1,\,k}x_k \\
			b_2=\sum_{k=1}^n\,a_{2,\,k}x_k \\
			\cdots\cdots\cdots\cdots\cdots \\
			b_n=\sum_{k=1}^n\,a_{n,\,k}x_k
		\end{matrix}\right)=\left(\begin{matrix}
			a_{1,\,1}x_1+a_{1,\,2}x_2+\cdots+a_{1,\,m}x_m    \\
			a_{2,\,1}x_1+a_{2,\,2}x_2+\cdots+a_{2,\,m}x_m    \\
			\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots \\
			a_{n,\,1}x_1+a_{n,\,2}x_2+\cdots+a_{n,\,m}x_m    \\
		\end{matrix}\right)$$
\textbf{(Il numero di colonne della prima matrice deve essere uguale al numero di righe della seconda matrice)}
\subsection{Prodotto Riga per Colonna tra Matrici}
Siano $A:(n\times m)$ e $C:(m\times t)$ matrici, si avrà $A\cdot C:(n,t)$ così espressa:
$$C=(c_{ij})=a_{i1}\cdot b_{1j}+a_{i2}\cdot b_{2j}+\cdots+a_{im}\cdot b_{mj},\, i=1,2,... ,n\,\, e\,\, j=1,2,... ,t$$
ovvero:
$$
	A\cdot C=
	\left(\begin{matrix}
			a_{1\,1} & a_{1\,2} & a_{1\,3} \\
			a_{2\,1} & a_{2\,2} & a_{2\,3}
		\end{matrix}\right)
	\cdot
	\left(\begin{matrix}
			c_{1\,1} & c_{1\,2} & c_{1\,3} \\
			c_{2\,1} & c_{2\,2} & c_{2\,3} \\
			c_{3\,1} & c_{3\,2} & c_{3\,3}
		\end{matrix}\right)=
$$

$$
	\left(\begin{matrix}
			a_{1\,1}\cdot c_{1\,1}+a_{1\,2}\cdot c_{2\,1}+a_{1\,3}\cdot c_{3\,1} & a_{1\,1}\cdot c_{1\,2}+a_{1\,2}\cdot c_{2\,2}+a_{1\,3}\cdot c_{3\,2} & a_{1\,1}\cdot c_{1\,3}+a_{1\,3}\cdot c_{2\,3}+a_{1\,3}\cdot c_{3\,3} \\
			a_{2\,1}\cdot c_{1\,1}+a_{2\,2}\cdot c_{2\,1}+a_{2\,3}\cdot c_{3\,1} & a_{2\,1}\cdot c_{1\,2}+a_{2\,2}\cdot c_{2\,2}+a_{2\,3}\cdot c_{3\,2} & a_{2\,1}\cdot c_{1\,3}+a_{2\,2}\cdot c_{2\,3}+a_{2\,3}\cdot c_{3\,3}
		\end{matrix}\right)
$$
ovvero:
$$ A\cdot C=B\rightarrow b_{k\,j}=\sum_{k=1}^{m} a_{k\,i}\cdot c_{i\,j}$$
Per permettere il prodotto riga per colonna si deve avere il numero di colonne della prima pari al numero di righe della seconda, inoltre la matrice risultante avrà lo stesso numero di righe della prima e lo stesso numero di colonne delle seconda.\\
Se moltiplico una matrice $n\times m$ e una $m\times n$posso definire entrambi i prodotti $A\cdot B$, che sarà di ordine $m$ e $B\cdot A$, che sarà di ordine $n$. In generale $A\cdot B \neq B\cdot A$, ovvero non si ha la proprietà commutativa.\\
Il prodotto di due matrici non nulle può dare come risultato una matrice nulla.\\
Si hanno le seguenti proprietà:
\begin{itemize}
	\item $(A\cdot B)\cdot C=A\cdot (B\cdot C)$ \textit{proprietà associativa}
	\item $A\cdot(B+C)=A\cdot B+A\cdot C$ \textit{proprietà distributiva  a destra}
	\item $(B+C)\cdot A=B\cdot A+C\cdot A$ \textit{proprietà distributiva a destra}
	\item $A\cdot (0)=(0)$ \textit{esistenza dell'elemento nullo, la matrice nulla}
	\item $A\cdot(k\cdot B)=k\cdot(A\cdot B),\, \forall k\in \mathbb{R}$
	\item $A\cdot I=I\cdot A=A$ {esistenze dell'elemento neutro, ovvero la matrice identità, la matrice diagonale con solamente il valore 1 come elementi della diagonale}
\end{itemize}
\section{Composizione}
Siano $A:\mathbb{R}^3\rightarrow \mathbb{R}^2$ e $C:\mathbb{R}^3\rightarrow \mathbb{R}^3$, $A:(2\times 3)$ e $C:(3\times 3)$ matrici, si ha che;
$$\underline{x}	\in\mathbb{R}^3\rightarrow C\cdot\underline{x}	\in\mathbb{R}^3\rightarrow A\cdot(C\cdot\underline{x})	\in\mathbb{R}^3\rightarrow (A\cdot C)\underline{x}$$
Ovvero essendo le matrici delle funzioni si ha che la loro composizione non è altro che il prodotto riga per colonna tra le due.
\section{Determinante}
\begin{shaded}
	\begin{definizione}[bonus]
		Si ha che una permutazione su $n$ elementi è un'applicazione biunivoca $\sigma:J_n\rightarrow J_n$ dove $J_n=\{1,2,...,n\}$:
		$$
			\sigma=\left(\begin{matrix}
					1         & 2         & 3         & \cdots & n         \\
					\sigma(1) & \sigma(2) & \sigma(3) & \cdots & \sigma(n)
				\end{matrix}\right)
		$$
		Si dice che $\sigma$ è di \textbf{classe pari} (rispettivamente di \textbf{classe dispari}) se si passa da ($\sigma(1),\sigma(2),...,\sigma(n)$) a ($1,2,...,n$) con un numero pari (rispettivamente dispari) di scambi.\\
		(Per esempio
		$
			\left(\begin{matrix}
					1 & 2 & 3 & 4 \\
					3 & 4 & 1 & 2
				\end{matrix}\right)
		$ è di classe pari mentre $
			\left(\begin{matrix}
					1 & 2 & 3 & 4 \\
					1 & 3 & 2 & 4
				\end{matrix}\right)
		$ è di classe dispari)\\
		Si dimostra che la definizione di classe pari e di classe dispari è ben posta, ovvero non dipende dal modo con cui $\sigma$ si ottiene come composizione di scambi.\\
		Data $\sigma\in S_n$ (gruppo simmetrico su $n$ elementi) definiamo:
		$$\varepsilon(\sigma)=\begin{cases}
				+1  \mbox{   se } \sigma \mbox{ è pari} \\
				-1  \mbox{   se } \sigma \mbox{ è dispari}
			\end{cases}$$
		Sia ora $A\in Mat_{n,n}(\mathbb{K})$ una matrice quadrata di ordine $n$. Si definisce il \textbf{determinante} di $A$:
		$$det(A)=\sum_{\sigma\in S_n} \varepsilon(\sigma)\alpha_{1\sigma(1)}\alpha_{2\sigma(2)}\cdots \alpha_{n\sigma(n)}$$
		Esso è composto da $n!$ addendi dove ciascun addendo contiene uno ed un solo fattore preso da ciascuna riga e ciascuna colonna ($\sigma$ è biunivoca). Si presentano i primi 2 casi:
		\begin{itemize}
			\item caso $n=1$: si hanno $A=(\alpha_{11})$, $S_1=\{\sigma=id\}$, $\varepsilon(\sigma)=+1$ e $
				      \sigma=\left(\begin{matrix}
						      1 \\
						      1
					      \end{matrix}\right)
			      $ e quindi:
			      $$det(A)=+\alpha_{1,1}$$
			\item caso n=2: si hanno $
				      A=\left(\begin{matrix}
						      \alpha_{11} & \alpha_{12} \\
						      \alpha_{21} & \alpha_{22}
					      \end{matrix}\right)
			      $, $S_2=\{\sigma_1=id,\, \sigma_2=\left(\begin{matrix}
						      1 & 2 \\
						      2 & 1
					      \end{matrix}\right)\}$, $\varepsilon(\sigma_1)=1$ e $ \varepsilon(\sigma_2)=-1$ e quindi:
			      $$det(A)=+\alpha_{1\sigma_1(1)}\cdot \alpha_{2\sigma_1(2)}-\alpha_{1\sigma_2(1)}\cdot \alpha_{2\sigma_2(2)}=\alpha_{11}\alpha_{22}-\alpha_{12}-\alpha_{21}$$
		\end{itemize}
	\end{definizione}
\end{shaded}
considero i tre sistemi:
$$
	\underbrace{\begin{cases}
			x+y=1 \\
			x-y=0
		\end{cases}}_{A},\,\,\,\,
	\underbrace{\begin{cases}
			x+y=1 \\
			2x+2y=2
		\end{cases}}_{B},\,\,\,\,
	\underbrace{\begin{cases}
			x+y=1 \\
			x+y=0
		\end{cases}}_{C}
$$
Si ha che:
$$
	A=\begin{cases}
		x=\frac{1}{2} \\
		y=\frac{1}{2}
	\end{cases}
$$
In \textit{B} invece ho 2 equazioni con lo stesso vincolo quindi:
$$
	B=\begin{cases}
		Infinite\,\, soluzioni
	\end{cases}
$$
In \textit{C} si ha una contraddizione quindi:
$$
	C=\begin{cases}
		Impossibile
	\end{cases}
$$
Si cerca quindi un modo per capire velocemente cosa fa un sistema. Questo ente algebrico è il \textbf{determinante}.
Si ha che
$$
	Det=\left(
	\begin{matrix}
			a & b \\
			c & d
		\end{matrix}
	\right)=(a\cdot d)-(b\cdot c)
$$
Torno agli esempi di prima:
$$det(A)=-2,\,\, det(B)=0,\,\,det(C)=0$$
Quindi un sistema è risolvibile sse $det(N)\neq 0$, se $det(n)=0$ ho non si hanno soluzioni o se ne hanno infinite.\\
\textbf{Il determinante si ha solo per matrici quadrate}\\
Si sa che data una matrice $2\times 2$ il suo determinante è 0 sse le due righe o le due colonne sono proporzionali. A livello geometrico si avrebbe l'intersezione tra le due rette nulla, ovvero con rette parallele, che o non si intersecano mai (soluzione impossibile) o sono sovrapposte (infinite soluzioni):
$$\begin{cases}
		y=m\cdot x+q \\
		y^{'}=m^{'}\cdot x^{'}+q^{'}
	\end{cases}=
	\begin{cases}
		y-m\cdot x=q \\
		y^{'}-m^{'}\cdot x^{'}=q^{'}
	\end{cases}=
	\left(
	\begin{matrix}
			-m     & 1 \\
			-m^{'} & 1
		\end{matrix}
	\right)\rightarrow -m+m^{'}=0
$$
Usando la notazione esplicita:
$$
	\begin{cases}
		a\cdot x+b\cdot y=-c \\
		a^{'}\cdot x+b^{'}\cdot y=-c^{'}
	\end{cases}=
	\left(
	\begin{matrix}
			a     & b     \\
			a^{'} & b^{'}
		\end{matrix}
	\right)\rightarrow (a\cdot b^{'})-(a^{'}\cdot b)=0\rightarrow \frac{a}{b}=\frac{a^{'}}{b^{'}}
$$
Passo alle matrici $3\times 3$, dove si ha la \textit{Regola di Sarrus}:
$$
	C=\left(\begin{matrix}
			c_{1\,1} & c_{1\,2} & c_{1\,3} \\
			c_{2\,1} & c_{2\,2} & c_{2\,3} \\
			c_{3\,1} & c_{3\,2} & c_{3\,3}
		\end{matrix}\right)
$$
prendo le prime due colonne e le accosto sulla destra:
$$
	C=\left(\begin{matrix}
			c_{1\,1} & c_{1\,2} & c_{1\,3} \\
			c_{2\,1} & c_{2\,2} & c_{2\,3} \\
			c_{3\,1} & c_{3\,2} & c_{3\,3}
		\end{matrix}\right)
	\begin{matrix}
		c_{1\,1} & c_{1\,2} \\
		c_{2\,1} & c_{2\,2} \\
		c_{3\,1} & c_{3\,2}
	\end{matrix}
$$
Procedo come con le $2\times 2$ usando le tre diagonali da tre elementi che mi si formano, si avranno le seguenti diagonali:\\
Per la parte positiva:
$$
	\begin{matrix}
		c_{1\,1} & c_{1\,2} & c_{1\,3} \\
		         & c_{2\,2} & c_{2\,3} \\
		         &          & c_{3\,3}
	\end{matrix}
	\begin{matrix}
		 &          &          \\
		 & c_{2\,1} &          \\
		 & c_{3\,1} & c_{3\,2}
	\end{matrix}
$$
e per la parte negativa:
$$
	\begin{matrix}
		         &          & c_{1\,3} \\
		         & c_{2\,2} & c_{2\,3} \\
		c_{3\,1} & c_{3\,2} & c_{3\,3}
	\end{matrix}
	\begin{matrix}
		 & c_{1\,1} & c_{1\,2} \\
		 & c_{2\,1} &          \\
		 &          &
	\end{matrix}
$$
e quindi:
$$det(C)=((c_{1\,1}\cdot c_{2\,2}\cdot c_{3\,3})+(c_{1\,2}\cdot c_{2\,3}\cdot c_{3\,1})+(c_{1\,3}\cdot c_{2\,1}\cdot c_{3\,2}))-((c_{3\,1}\cdot c_{2\,2}\cdot c_{1\,3})+(c_{3\,2}\cdot c_{2\,3}\cdot c_{1\,1})+(c_{3\,3}\cdot c_{2\,1}\cdot c_{1\,2}))$$
\textit{Con le matrici quadrate ciò che vale per le righe vale anche per le colonne}.\\
\textbf{Non si definisce il determinante per matrici non quadrate}.
\newpage
Proprietà del determinante di una matrice quadrata $n\times n$ (\textit{vari esempi sul libro da pagina 18)}:
\begin{itemize}
	\item $det(A)=det(A^T)$, quindi posso tranquillamente sviluppare il determinante secondo le righe o secondo le colonne indistintamente
	\item se c'è una riga o una colonna di tutti zeri il determinante è 0
	\item scambiando due righe tra loro il determinante non cambia in valore assoluto ma cambia segno (vale anche se scambio due colonne)
	\item se ho due righe uguali il determinante è zero
	\item se ad una colonna $A^i$ di una matrice $A$ di ordine $n$ si somma un'altra colonna $A^j$, con $i\neq j$, moltiplicata per $k\in \mathbb{R}$, il determinante non cambia. Ovvero:
	      $$det(A^1,A^2,...A^{i-1},A^i,A^{i+1},...,A^n)=det(A^1,A^2,...A^{i-1},A^i+k\cdot A^j,A^{i+1},...,A^n)$$
	\item se si hanno due colonne proporzionali (in particolare uguali) allora il determinate è nulla (per la proprietà sopra)
	\item se moltiplico una sola riga o una sola colonna per $k\in\mathbb{R}$ avrò il determinante pari a $k\cdot det(C)$ ovvero:
	      $$det(A^1,A^2,...,k\cdot A^i,...,A^n)=k\cdot det(A)$$
	\item se ho una matrice diagonale allora il determinante è apri al prodotto degli elementi sulla diagonale, in particolare $det(I)=1$
	\item sia $A=(A^1,A^2,...,A^i,...,A^n)$ una matrice e $
		      B=\left(\begin{matrix}
			      b_1    \\
			      b_2    \\
			      \vdots \\
			      b_n
		      \end{matrix}\right)
	      $ una qualunque matrice colonna. Allora, $\forall i=1,2,...,n$ si ha:
	      $$det(A^1,A^2,...,A^i+B,...,A^n)=det(A^1,A^2,..., A^i,...,A^n)+det(A^1,A^2,...,B,...,A^n)$$
	\item se moltiplico, in una $2\times 2$ entrambe le righe o entrambe le colonne per $k\in\mathbb{R}$ avrò il determinante pari a $k^2\cdot det(C)$
	\item sia così espressa la proprietà distributiva:
	      $$
		      \left(
		      \begin{matrix}
				      1 & 1 \\
				      2 & 2
			      \end{matrix}
		      \right)
	      $$
	      leggo la prima colonna come:
	      $$
		      \left(
		      \begin{matrix}
				      1 \\
				      2
			      \end{matrix}
		      \right)=
		      \left(
		      \begin{matrix}
				      1 \\
				      0
			      \end{matrix}
		      \right)+
		      \left(
		      \begin{matrix}
				      0 \\
				      2
			      \end{matrix}
		      \right)
	      $$
	      si ha che:
	      $$
		      det\left(
		      \begin{matrix}
				      1 & 1 \\
				      2 & 2
			      \end{matrix}
		      \right)=
		      det\left(
		      \begin{matrix}
				      1 & 1 \\
				      0 & 2
			      \end{matrix}
		      \right)+
		      det\left(
		      \begin{matrix}
				      0 & 1 \\
				      2 & 2
			      \end{matrix}
		      \right)=-2+2=0
	      $$
	      \textit{sistemare la versione formale}
	      $$
		      det\left(
		      \begin{matrix}
				      a_{1\,1} & a_{1\,2} \\
				      a_{2\,1} & a_{2\,2}
			      \end{matrix}
		      \right)$$
	      con:
	      $$\begin{cases}
			      a_{1\,1}=c_{1\,1}+a_{1\,1} \\
			      a_{1\,1}=a_{1\,1}+a_{1\,1}
		      \end{cases}$$
\end{itemize}
\begin{teorema}
	Sia $A$ una matrice $3\times 3$ ottenuta accostando 3 vettori colonna: $A=\underline{v}_1\, \underline{v}_2\, \underline{v}_3$. Suppongo:
	$\underline{v}_2=\underline{w}_2+{\underline{w}_2}^{'}$. Si ha che:
	$$det(A)=det(\underline{v}_1\, \underline{w}_2\, \underline{v}_3)+det(\underline{v}_1\, {\underline{w}_2}^{'}\, \underline{v}_3)$$
	\textbf{Vale per qualsiasi riga o colonna. Posso anche spezzare più volte (più delle due nell'esempio sopra) la stessa riga/colonna}
\end{teorema}
Inoltre se ho:
$$det(A)=det(\underline{v}_1,\, \underline{v}_2,\, \underline{v}_3) \,\, e\,\, A=[\underline{v}_1,\, \underline{v}_2,\, \underline{v}_3]$$
posso scrivere:
$$A^{'}=[\underline{v}_1-k\cdot\underline{v}_2,\, \underline{v}_2,\, \underline{v}_3]$$
avrò:
$$det(A^{'})=det(\underline{v}_1,\, \underline{v}_2,\, \underline{v}_3)+det(-k\cdot\underline{v}_2,\, \underline{v}_2,\, \underline{v}_3)-k\cdot det(\underline{v}_2,\, \underline{v}_2,\, \underline{v}_3)=det(A)$$
\subsection{Determinante di matrici n x n}
Vediamo come calcolare il determinante di una matrice $n\times n$.
$$
	C=\left(\begin{matrix}
			c_{1\,1} & c_{1\,2} & c_{1\,3} & \cdots & c_{1\,n} \\
			c_{2\,1} & c_{2\,2} & c_{2\,3} & \cdots & c_{2\,n} \\
			\vdots   & \ddots   & \ddots   & \ddots & \vdots   \\
			c_{3\,1} & c_{3\,2} & c_{3\,3} & \cdots & c_{n\,n}
		\end{matrix}\right)
$$

definisco il complemento algebrico di un elemento come il determinante della matrice risultante dall'eliminazione della riga e della colonna di quell'elemento moltiplicato per $(-1)^{nriga+ ncolonna}$.
\begin{teorema}[di Laplace]
	il determinante di una matrice si ottiene così:\\
	scelgo una riga e una colonna. Scelta una riga $c_{1\,1},...,c_{1\,n}$ il determinante è:
	$$det(C)=\sum_{k=1}^n c_{1\,k}\cdot \mathbb{C}_{1\,k}=\sum_{k=1}^n c_{1\,k}\cdot (-1)^{1+k} det(C_{1\,k})$$
	con $\mathbb{C}_{1\,k}$ complemento algebrico di $c_{1\,k}$
\end{teorema}
Per comodità scelgo la riga con più 0. In pratica prendo una matrice, tolgo una riga e una colonna, calcolo il determinante della matrice rimanente (avrà una riga e una colonna in meno), moltiplico per (-1) elevato alla somma tra il numero di riga e il numero di colonna e per il numero presente all'incrocio della riga e della colonna scelte.
\begin{esempio}
	$$\left(\begin{matrix}
			1 & 1 & -1 & 1 \\
			0 & 2 & 2  & 0 \\
			1 & 3 & -1 & 3 \\
			1 & 1 & 0  & 1
		\end{matrix}\right)$$
	la seconda colonna diventa la seconda meno la terza, ottenendo così altri zeri senza perdere le proprietà:
	$$\left(\begin{matrix}
			1 & 2 & -1 & 1 \\
			0 & 0 & 2  & 0 \\
			1 & 4 & -1 & 3 \\
			1 & 1 & 0  & 1
		\end{matrix}\right) \rightarrow
		\left(\begin{matrix}
			1 & 2 & | & 1 \\
			- & - & | & - \\
			1 & 4 & | & 3 \\
			1 & 1 & | & 1
		\end{matrix}\right)$$
	scelgo la seconda riga e la terza colonna da elidere:
	$$2\cdot (-1)^5\cdot det\left(\begin{matrix}
			1 & 2 & 1 \\
			1 & 4 & 3 \\
			1 & 1 & 1
		\end{matrix}\right)
	$$
	procedo ancora e per la nuova matrice elimino la prima riga  e la seconda colonna (avrei potuto usare anche Sarrus). La matrice sara:
	$$
		\left(\begin{matrix}
			- & | & - \\
			1 & | & 3 \\
			1 & | & 1
		\end{matrix}\right)
	$$
	Si  avrà quindi:
	$$2\cdot (-1)^5\cdot 1\cdot (-1)^3 det\left(\begin{matrix}
			1 & 3 \\
			1 & 1
		\end{matrix}\right)=2\cdot (-1)\cdot 1\cdot (-1)\cdot (1-3)=-4
	$$
\end{esempio}
\subsection{Teorema di Binet}
\begin{teorema}[di Binet]
	Se ho due matrici $n\times m$ dice che il determinante del prodotto è il prodotto dei determinanti:
	$$det(A\cdot B)=det(A)\cdot det(B)$$
\end{teorema}
\section{Dipendenza Lineare}
Sia data una matrice $A$ di ordine $m\times n$ si ha che:
\begin{definizione}
	Le righe $A_1,A_2,...,A_M$ di $A$ si dicono \textbf{linearmente dipendenti} se esistono $k_1,k_2,k_m \in \mathbb{R}$, non tutti nulli tali che:
	$$k_1\cdot A_1+k_2\cdot A_2+\cdots+k_m\cdot A_m=(0)$$
	con $(0)$ è la matrice nulla $1\times n$
	In caso contrario le righe sono dette \textbf{linearmente indipendenti} e si ha:
	$$k_1\cdot A_1+k_2\cdot A_2+\cdots+k_m\cdot A_m=(0)\,\, sse\,\, \forall k_i \,\, si \,\, ha \,\, k_i=0$$
	\textit{Tutto ciò vale anche per le colonne e per un qualunque numero di righe e colonne}
\end{definizione}
Ovvero $k$ vettori, ambientati nello stesso spazio, sono linearmente dipendenti, esempio $\underline{v}_1\, \underline{v}_2\, \underline{v}_3\in \mathbb{R}^{n}$, se uno di essi è esprimibile come combinazione lineare degli altri, cioè esistono $k-1$ numeri ($\alpha_1,...,\alpha_k$) tali che, per esempio, $\underline{v}_1=\alpha_2\cdot \underline{v}_2+\alpha_3\cdot \underline{v}_3$
\begin{esempio}
	Siano:
	$$
		v_1=\left(
		\begin{matrix}
				1 \\
				1
			\end{matrix}
		\right)\,\, e \,\,
		v_2=\left(
		\begin{matrix}
				2 \\
				3
			\end{matrix}
		\right)
	$$
	Si ha che sono dipendenti se:
	$\underline{v}_1=\alpha_2\cdot \underline{v}_2$, ma si avrebbe:
	$\begin{cases}1=2\cdot\alpha_2\\1=3\cdot\alpha_2\end{cases}$, che è impossibile. I vettori sono quindi indipendenti.
\end{esempio}
Se una matrice ha determinate 0 allora i vettori che la compongono sono linearmente dipendenti (e quindi proporzionali tra di loro).
3 vettori in $\mathbb{R}^3$ sono dipendenti sse $det[\underline{v}_1\, \underline{v}_2\, \underline{v}_3]=0$. Si ha $\underline{v}_1= k\cdot\underline{v}_2+h\cdot \underline{v}_3$ e:
$$det[k\cdot\underline{v}_2+h\cdot \underline{v}_3,\, \underline{v}_2,\, \underline{v}_3]=k\cdot det[\underline{v}_1\, \underline{v}_2\, \underline{v}_3]+h\cdot det[\underline{v}_1\, \underline{v}_2\, \underline{v}_3]$$
\begin{teorema}
	Sia $A$ una matrice quadrata di ordine $n$. Si ha che $det(A)=0$ sse le righe (o le colonne) di $A$ sono linearmente dipendenti
\end{teorema}
\begin{definizione}
	Si dice che una riga $A_i$ della matrice $A$  è \textbf{combinazione lineare} delle altre righe se esistono $a_1,a_2,...,a_{i-1},a_{i+1},...,a_n\in\mathbb{R}$ (che sono detti coefficienti della combinazione lineare) tali che:
	$$A_i=a_1\cdot A_1,a_2\cdot A_2,...,a_{i-1}\cdot A_{i-1},a_{i+1}\cdot A_{i+1},...,a_n\cdot A_n$$
	Un'analoga definizione vale anche per le colonne
\end{definizione}
\begin{teorema}
	Si $A$ una matrice $m\times n$. Allora le righe $A_1,A_2,...,A_m$ di $A$ sono linearmente dipendenti sse almeno una riga di $A$ è combinazione lineare delle altre righe.\\
	Viceversa se una riga, per esempio $A_1$, è combinazione lineare delle altre righe allora $\exists a_2,...,a_m\in\mathbb{R}$ tali che:
	$$-A_1+a_2\cdot A_2+\cdots+a_m\cdot A_m=(0)$$
	e dato che il coefficiente di $A_1$ è $-1$, che è diverso da $0$, si ha che le righe $A_1,A_2,...,A_m$ di $A$ sono linearmente dipendenti.
	Questo risultato si ottiene anche ragionando sulle colonne.
\end{teorema}
\newpage
\section{Riduzione in scala di Gauss}
Posso scrivere le varie righe di una matrice come prodotto di una delle altre righe con un certo $k\in\mathbb{R}$. Esempio:
$$
	\left(
	\begin{matrix}
			1 & 1 & 1  \\
			2 & 2 & -1 \\
			5 & 2 & -1
		\end{matrix}
	\right)
$$
riscrivo la seconda riga come la seconda meno due volte la prima, ottengo:
$$
	\left(
	\begin{matrix}
			1 & 1 & 1  \\
			0 & 0 & -3 \\
			5 & 2 & -1
		\end{matrix}
	\right)
$$
\textit{Queste operazioni non cambiano il determinante}.\\
SI hanno le seguenti \textbf{operazioni elementari sulle righe (o colonne)} che possono essere applicate:
\begin{itemize}
	\item scambiare le righe (o le colonne)
	\item moltiplicare una riga (o una colonna) per un $k\in \mathbb{R},\, k\neq 0$
	\item sommare ad una riga (o colonna) un'altra riga (o colonna) moltiplicata per un $k\in \mathbb{R}$
\end{itemize}
Si definisce \textit{Pivot} il primo elemento della riga $A_i$ che non è nullo. Una matrice è ridotta a scala se il pivot $P_i$ si trova a sinistra del pivot $P_{i+1}$. Esempio, dove i pivot sono 1, 2 e 2:
$$
	\left(\begin{matrix}
			1 & 7 & 5 \\
			0 & 2 & 1 \\
			0 & 0 & 2
		\end{matrix}\right)
$$
Per ridurre a scala una matrice si effettuano delle operazione sulla n-esima riga sottraendo un multiplo di una riga superiore
esempio:
$$
	\left(\begin{matrix}
			1 & -1 & 1  \\
			2 & 0  & 1  \\
			1 & 1  & -1
		\end{matrix}\right)\overbrace{\longrightarrow}^{riga2=riga2-2\cdot riga1}\overbrace{\longrightarrow}^{riga3=riga3- riga1}
	\left(\begin{matrix}
			1 & -1 & 1  \\
			0 & 2  & -1 \\
			0 & 2  & -2
		\end{matrix}\right)\overbrace{\longrightarrow}^{riga3=riga3- riga2}
	\left(\begin{matrix}
			1 & -1 & 1  \\
			0 & 2  & -1 \\
			0 & 0  & -1
		\end{matrix}\right)
$$
Si chiama \textit{metodo di riduzione di Gauss} che può essere effettuato anche invertendo delle righe.
Se ottengo una riga di 0 significa che uno dei vettori è lineari e che quindi una delle equazioni in realtà non esiste autonomamente.
\newpage
\section{Calcolo della Matrice Inversa}
$A$ è invertibile se $\exists B$ quadrata tale che $A\cdot B= b\cdot A=$ matrice identità (che ha determinante 1). Condizione necessaria per cui $A$ sia invertibile è che $Det(aA)\neq 0$ (conseguenza del teorema di Binet, infatti $det(B\cdot A)=det(b)\cdot det(A)=1$). Inoltre si ha che $det(A^{-1})=\frac{1}{det(A)}$. Quindi $\forall b\, \exists !x:\, A\cdot x=b\longleftrightarrow det(A)\neq 0$ e $x=f^{-1}(b)$.
\begin{esempio}
	Calcolo la matrice inversa:
	Prendo:
	$$
		A=\left(\begin{matrix}
				5 & 4 \\
				6 & 5
			\end{matrix}\right)
	$$
	faccio la trasposta:
	$$
		A^T=\left(\begin{matrix}
				5 & 6 \\
				4 & 5
			\end{matrix}\right)
	$$
	calcolo la matrice aggiunta: $(-1)^{i+j}\cdot det(A^T)_{i\,j}$, easy nelle 2x2:
	$$
		A_{ij}=\left(\begin{matrix}
				5  & -4 \\
				-6 & 5
			\end{matrix}\right)
	$$
	faccio:
	$$
		A^T\cdot A_{ij}= \left(\begin{matrix}
				1 & 0 \\
				0 & 1
			\end{matrix}\right)
	$$
\end{esempio}
Ecco un altro metodo per calcolare l'inversa:\\
prendo la matrice:\\
$
	\left(\begin{matrix}
			5 & 4 \\
			6 & 5
		\end{matrix}\right)
$
gli accosto l'identità:
$
	\left(\begin{matrix}
			5 & 4 & 1 & 0 \\
			6 & 5 & 0 & 1
		\end{matrix}\right)
$\\
operando sulle righe faccio comparire l'identità a sinistra:\\
riscrivo la prima riga e al posto della seconda faccio la seconda meno $\frac{6}{5}$ la prima:
$$
	\left(\begin{matrix}
			5 & 4           & 1            & 0 \\
			0 & \frac{1}{5} & -\frac{6}{5} & 1
		\end{matrix}\right)
$$
ho ottenuto così  il primo 0 della seconda riga. ora faccio la prima riga meno venti volte la seconda, la seconda non la tocco
$$
	\left(\begin{matrix}
			5 & 0           & 25           & -20 \\
			0 & \frac{1}{5} & -\frac{6}{5} & 1
		\end{matrix}\right)
$$
e ottengo lo 0 in seconda posizione della prima riga
\newpage
ora divido per 5 la prima riga e moltiplico per 5 la seconda:
$$
	\left(\begin{matrix}
			1 & 0 & 5  & -4 \\
			0 & 1 & -6 & 5
		\end{matrix}\right)
$$
e ho che le ultime due colonne sono la mia inversa, avendo ottenuto con prime due l'identità:
$$
	\left(\begin{matrix}
			5  & -4 \\
			-6 & 5
		\end{matrix}\right)
$$
se moltiplico
$$
	\left(\begin{matrix}
			0 & 1 \\
			1 & 0
		\end{matrix}\right)\cdot \left(\begin{matrix}
			a & b \\
			c & d
		\end{matrix}\right)= \left(\begin{matrix}
			c & d \\
			a & b
		\end{matrix}\right)
$$
ottengo la seconda matrice con le righe invertite.\\ se invece moltiplico una matrice per una del tipo:
$$
	\left(\begin{matrix}
			\lambda_1 & 0         \\
			0         & \lambda_2
		\end{matrix}\right)
$$ ottengo la prima riga per $\lambda_1$ e la seconda per $\lambda_2$.\\Invece se moltiplico una matrice per:
$$
	\left(\begin{matrix}
			1 & \alpha \\
			0 & 1
		\end{matrix}\right)
$$
ottengo una matrice con $\alpha$ volte la seconda riga.
\\Invece se moltiplico una matrice per:
$$
	\left(\begin{matrix}
			1      & 0 \\
			\alpha & 1
		\end{matrix}\right)
$$
ottengo una matrice con $\alpha$ volte la prima riga.\\
Un'altra tecnica per il calcolo della matrice inversa prevede i seguenti passaggi:
\begin{itemize}
	\item si calcola il determinante della matrice $A$
	\item si calcola la matrice dei cofattori (si ricorda che si parte fissando la prima colonna e scalando sulle righe e così via)
	\item si calcola la matrice trasposta della matrice dei cofattori appena ottenuta
	\item si moltiplica questa matrice trasposta per $\frac{1}{det(A)}$
\end{itemize}
\newpage
\begin{esercizio}
	Sia $V$ l'insieme delle matrici $4\times 4$ e $W$ l'insieme della matrici con determinante nullo. $W$ è sottospazio?
	\begin{itemize}
		\item dato che hanno determinante nullo si ha che lo 0 c'è
		\item se $A\in W$ allora $\lambda\cdot A\in W$, con $\lambda\in \mathbb{R}$. Ora $det(\lambda\cdot A)=\lambda^4 \cdot det(A)=0$ e quindi la condizione è verificata.
		\item se $A\in W$ e $B\in W$ allora $A+B\in W$. Sappiamo che è complesso perché il determinante non è lineare. quindi $A+B\in W$ non è verificato. per esempio si ha:
		      $$
			      A+B=
			      \left(\begin{matrix}
				      1 & 0 & 0 & 0 \\
				      0 & 1 & 0 & 0 \\
				      0 & 0 & 1 & 0 \\
				      0 & 0 & 0 & 0
			      \end{matrix}\right)+\left(\begin{matrix}
				      0 & 0 & 0 & 0 \\
				      0 & 0 & 0 & 0 \\
				      0 & 0 & 0 & 0 \\
				      0 & 0 & 0 & 1
			      \end{matrix}\right)=\left(\begin{matrix}
				      1 & 0 & 0 & 0 \\
				      0 & 1 & 0 & 0 \\
				      0 & 0 & 1 & 0 \\
				      0 & 0 & 0 & 1
			      \end{matrix}\right)=I
		      $$
	\end{itemize}
	dove $A$ e $B$ hanno determinante nullo, mentre $det(I)=!$
	Quindi $W$ non è un sottospazio.
\end{esercizio}
\begin{esercizio}
	$$
		A=\left(\begin{matrix}
				1 & 3 & 4  \\
				0 & 2 & 0  \\
				1 & 5 & -1
			\end{matrix}\right)
	$$
	è invertibile perché ha $det(A)=-10$. Applicando Binet calcolo il determinante dell'inversa, so infatti che $A\cdot A^{-1}=I$ quindi $det(\cdot A^{-1})=det(I)=1$ quindi $det(A)\cdot det(A^{-1})=1$ e $det(b^{-1})=-\frac{1}{10}$
\end{esercizio}
\newpage
\section{Rango}
\begin{definizione}
	Sia data una matrice $A$ di ordine $m\times n$. Si chiama \textbf{minore di ordine} $r$ della matrice $A$ una qualunque matrice di ordine $r$ i cui elementi sono dati dall'intersezione di $r$ righe e $r$ colonne della matrice $A$. Dalla definizione si nota che $$r\leq min\{m,n\}$$
\end{definizione}
Si ha quindi la seguente definizione:
\begin{definizione}
	Si dice che una matrice $A$ di ordine $m\times n$ ha \textbf{rango} $p=rg(A)$ se esiste un minore p con determinante diverso da zero e tutti i minori di $A$ di ordine maggiore di p ha determinante uguale a $0$. Inoltre
	$$rg(A)\leq min\{m,n\}$$
\end{definizione}
Il rango quindi è il massimo ordine di sottomatrice quadrata con determinante diverso da 0 che posso ottenere usando le righe e le colonne della matrice in esame. Inoltre se ho le $k\times k$ con determinante non nullo allora tutte le $(k+1)\times(k+1)$ avranno determinante 0.\\
Prendo una matrice $n\times m$ e scelgo tutte le sottomatrici quadrate possibili, l'ordine di quelle con determinante diverso da 0 è il rango.\\
due vettori sono dipendenti se: $$v_1=\alpha_2\cdot v_2+...+ \alpha_k\cdot v_k$$. Tutto ciò si può riscrivere come: $$0=-v_1+\alpha_2\cdot v_2+...+ \alpha_k\cdot v_k$$ quindi si hanno coefficienti non tutti zero che danno zero. Se esiste una combinazione lineare dei $v_1$ con coefficienti non tutti nulli. Se c'è un $\alpha_n$ è nullo si ha: $$\alpha_k\cdot v_k=-\alpha_1\cdot v_1+\alpha_2\cdot v_2+...+ \alpha_{k-1}\cdot v_{k-1}$$ e posso spostare $\alpha_ k$: $$v_k=\frac{-\alpha_1\cdot v_1+\alpha_2\cdot v_2+...+ \alpha_{k-1}\cdot v_{k-1}}{\alpha_k}$$
\begin{esempio}
	ho:
	$$\left(\begin{matrix}
			1 \\
			0
		\end{matrix}\right) \,\, e \,\, \left(\begin{matrix}
			0 \\
			2
		\end{matrix}\right)$$
	quindi:
	$$\alpha_1\cdot \left(\begin{matrix}
			1 \\
			0
		\end{matrix}\right) + \alpha_2\cdot \left(\begin{matrix}
			0 \\
			2
		\end{matrix}\right)=
		\left(\begin{matrix}
			0 \\
			0
		\end{matrix}\right)$$
	Quindi $\alpha_1=\alpha_2=0$.\\
	Se l'unico modo per avere corrispondenza lineare è mettere i coefficienti a zero si hanno i vettori indipendenti
\end{esempio}
Sia $A$ una matrice e $V$ la matrice $A$ ridotta a scala. Il rango di $A$ è il numero di righe di $V$ diverse da 0. infatti il rango di una matrice è uguale al rango della sua matrice ridotta.
\begin{definizione}
	Si dice che una matrice ha \textbf{rango per righe} $r$ se $A$ ha al più $r$ righe linearmente dipendenti. Analogamente ha \textbf{rango per colonne} $s$ se $A$ ha al più $s$ colonne linearmente indipendenti. Inoltre:
	$$r=s=rg(A)$$
\end{definizione}
\begin{teorema}
	data una matrice $A=n\times m$ il rango di $A$ è il massimo numero di righe (o colonne) indipendenti di $A$
\end{teorema}
\section{Sistemi Lineari}
\begin{definizione}
	Si chiama \textbf{equazione lineare} su $\mathbb{R}$ ogni equazione del tipo:
	$$a_1\cdot x_1+a_2\cdot x_2+\cdots +a_n\cdot x_n=b$$
	con $a_i\in \mathbb{R}$ che vengono chiamati \textbf{coefficienti} delle \textbf{incognite} $x_i$, mentre $b$ è il \textbf{termine noto} dell'equazione. Se $a_1\cdot k_1+a_2\cdot k_2+\cdots +a_n\cdot k_n=b$ si ha che la n-pla ordinata di numeri reali ($k_1,k_2,...,k_n$) è detta soluzione dell'equazione.
\end{definizione}
\begin{definizione}
	Si definisce nella seguente maniera un \textbf{sistema lineare} di $m$ equazioni lineari in $n$ incognite ($x_1,x_2,...,x_n$):
	$$
		\begin{cases}
			a_{11}\cdot x_1+a_{12}\cdot x_2+\cdots+a_{1n}\cdot x_n=b_1                                                                                                                         \\
			a_{21}\cdot x_1+a_{22}\cdot x_2+\cdots+a_{2n}\cdot x_n=b_2                                                                                                                         \\
			\,\,\,\,\,\,\,\,\,\vdots \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \vdots \,\,\,\,\,\,\,\,\,\,\,\,\, \ddots \,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \vdots \,\,\,\,\,\,\,  = \, \vdots \\
			a_{m1}\cdot x_1+a_{m2}\cdot x_2+\cdots+a_{mn}\cdot x_n=b_m
		\end{cases}$$
	con $a_{ij}\in\mathbb{R}$ (con $i=1,2,...,m$ e $ j=1,2,...,m$).\\
	Se tutti i termini noti delle equazioni $b_i$ (con $i=1,2,...,m$) sono nulli allora si ha un \textbf{sistema omogeneo}.
\end{definizione}
Una n-pla ordinata di reali $k_i$ ($i=1,...,n$) è detta soluzione del sistema lineare sse soddisfa tutte le $m$ equazioni. Ciascuna soluzione è detta \textbf{soluzione particolare} e l'insieme di tutte le soluzioni particolari è detta \textbf{soluzione generale} del sistema, che può essere un solo elemento se il sistema ha una sola soluzione. Risolvere il sistema significa trovare la soluzione generale del suddetto. Se un sistema ammette soluzione allora è \textbf{risolubile}. La n-pla $0,0,...,0$ è detta \textbf{soluzione nulla o banale}. Da un sistema con i termini noti non nulli posso ricavare un \textbf{sistema omogeneo associato} imponendo i termini noti nulli.
\begin{teorema}
	Sia $u=(k_1,...,k_n)$ una soluzione particolare del sistema lineare generale e $V$ quella del sistema omogeneo associato. Allpra la soluzione generale del sistema generale è data dall'insieme:
	$$u+V=\{u+v:v\in V\}$$
	con $v=h_1,...,h_n$ e $u+v=(k_1+h_1,...,k_n+h_n)$.
\end{teorema}
Due sistemi si dicono \textbf{equivalenti} se hanno la stessa soluzione generale. Per trasformare un sistema in uno equivalente posso:
\begin{itemize}
	\item scambiare l'ordine delle equazioni
	\item moltiplicare un'equazione del sistema per un $k\in \mathbb{R},\, k\neq 0$
	\item sommare ad un equazione un'altra equazione moltiplicata per un $k\in \mathbb{R}$
\end{itemize}
Si può notare un'analogia con le operazione tra le righe che si possono fare nelle matrici. \\
Posso usare le matrici anche per rappresentare un sistema lineare utilizzando la cosiddetta \textbf{matrice incompleta} che rappresenta i coefficienti: $$
	A=\left(\begin{matrix}
			a_{11} & \cdots & a_{1n} \\
			\cdots & \ddots & \cdots \\
			a_{m1} & \cdots & a_{mm}
		\end{matrix}\right)
$$ a cui si aggiungono la \textbf{colonna delle incognite} $
	X=\left(\begin{matrix}
			x_1    \\
			\cdots \\
			x_n
		\end{matrix}\right)
$ e la matrice dei termini noti $
	B=\left(\begin{matrix}
			b_1    \\
			\cdots \\
			b_n
		\end{matrix}\right)
$.
Il sistema lineare si può quindi scrivere come $$A\cdot X=B$$
dove tra $A$ e $X$ abbiamo il prodotto riga per colonna.\\
Per esempio:
$$
	\begin{cases}
		2x_1-3x_2+x_4=1 \\
		x_2-x_3=0       \\
		x_1+x_2+x_3+x_4=-1
	\end{cases}\rightarrow
	\underbrace{\left(\begin{matrix}
			2 & -3 & 0  & 1 \\
			0 & 1  & -1 & 0 \\
			1 & 1  & 1  & 1
		\end{matrix}\right)}_{A}\cdot
	\underbrace{\left(\begin{matrix}
			x_1 \\
			x_2 \\
			x_3 \\
			x_4
		\end{matrix}\right)}_{X}=
	\underbrace{\left(\begin{matrix}
			1 \\
			0 \\
			-1
		\end{matrix}\right)}_{B}
$$
Si nota che il sistema lineare può essere riscritto come:
$$x_1\cdot A^1+x_2\cdot A^2+\cdots + x_n\cdot A^n=B$$
\subsection{Teorema di Cramer}
\begin{teorema}[ di Cramer]
	Un sistema lineare ha una ed una sola soluzione sse il determinante della sua matrice incompleta ha determinante diverso da 0.
\end{teorema}
\begin{teorema}[regola di Cramer]
	Sia dato un sistema di $n$ equazioni in $n$ incognite, $A=n\times n$ il sistema ammette una e una sola soluzione sse $det(A)\neq 0$ (si nota che il teorema non è efficacie in termini computazionali)
	si ha che:
	$$\begin{cases}
			a_{1\,1}\cdot x_1+\cdots+a_{1\,n}\cdot x_n \\
			\vdots                                     \\
			a_{n\,1}\cdot x_1+\cdots+a_{n\,n}\cdot x_n
		\end{cases}$$

	$$A\cdot \underline{x}=\underline{b}$$

	$$x_{i} = {{det(A')}\over{detA}}$$

	dove la matrice A' è ottenuta sostituendo alla colonna i-esima della matrice A il vettore \underline{b}

	Se det(A)=0 il sistema non ha soluzioni oppure esistono più soluzioni
\end{teorema}
\newpage
\subsection{Teorema di Rouché-Capelli}
\begin{teorema}[di Rouché-Capelli]
	Sia dato un sistema di n equazioni in m incognite
	$$\begin{cases}
			a_{1\,1}\cdot x_1+\cdots+a_{1\,n}\cdot x_n \\
			\vdots                                     \\
			a_{n\,1}\cdot x_1+\cdots+a_{n\,n}\cdot x_n
		\end{cases}$$
	il sistema $A\cdot \underline{x}=\underline{b}$ ammette soluzione sse $rango(A)=rango(A|b)$. Inoltre detto r il rango di A, nel caso in cui si abbia $m>r$, abbiamo soluzioni che dipendono da $m-r$ parametri, ovvero $\infty^{m-r}$ soluzioni (con $m$ numero di incognite).
\end{teorema}
\subsection{Metodo di Gauss}
Per risolvere un sistema lineare posso prendere la sua matrice completa (ovvero $A|B$), 	ridurla a scala e risolvere il sistema ottenuto (drasticamente più semplice da risolvere di quello iniziale) con la riduzione a scala. Questo sistema è detto \textbf{Metodo di Gauss}
\subsection{Sistemi Lineari Omogenei}
I sistemi lineari omogenei hanno alcune particolarità:
\begin{itemize}
	\item ammettono sempre la soluzione nulla
	\item la matrice completa e quella incompleta hanno lo steso rango
	\item se la caratteristica della matrice completa (o incompleta) è pari al numero di incognite il sistema, per il teorema di Cramer, ha una e una sola soluzione, quindi la \textbf{soluzione nulla}
	\item un sistema lineare omogeneo ha soluzione sse la caratteristica della matrice completa (o incompleta) è minore del numero delle incognite
	\item un sistema lineare omogeneo di $n$ equazioni in $n$ incognite ha soluzioni non nulle sse il determinante della matrice incompleta è nullo
	\item se il rango $r$ è minore delle $n$ incognite il sistema ammette infinite soluzioni $\infty^{n-r}$
\end{itemize}
\chapter{Spazi Vettoriali}
Uno spazio  vettoriale è una struttura algebrica composta da un \textit{campo}, i cui elementi sono detti \textit{scalari}, da un insieme, i cui elementi sono dei vettori, e da due operazioni binarie ( caratterizzate da particolari proprietà), la \textit{somma} e la \textit{moltiplicazione per uno scalare}.
\begin{definizione}
	Sia $V$ un insieme di elementi con due operazioni:
	\begin{itemize}
		\item somma fra vettori
		\item prodotto di un vettore per uno scalare
	\end{itemize}
	operazioni che a loro volta hanno determinate proprietà. Se si ha questa situazione si ha che $V$ è uno \textbf{spazio vettoriale} su un certo campo (per esempio $\mathbb{R}$) e i suoi elementi sono detti \textbf{vettori}. Si ha infine che il vettore nullo è unico e che l'opposto di un certo vettore è unico.
\end{definizione}
\subsection{Somma Vettoriale}
Siano: $\underline{x}=(x_1,...,x_n)$ e $\underline{y}=(y_1,...,y_n)$ allora si ha:
$$\underline{x}+\underline{y}=(x_1+y_1,...,x_n+y_n)$$
\textbf{Posso sommare due vettori sse appartengono allo stesso spazio vettoriale}\\
Proprietà:
\begin{itemize}
	\item $\underline{x}+\underline{y}=\underline{y}+\underline{x}$ (commutativa)
	\item $(\underline{x}+\underline{y})+\underline{z}=\underline{x}+(\underline{y}+\underline{z})$ (associativa)
	\item $\underline{0}+\underline{x}=\underline{x}$ (elemento neutro)
	\item $\underline{x}+(\underline{-x})=\underline{0}$ (elemento opposto)
\end{itemize}
\subsection{Moltiplicazione di un Vettore per uno Scalare}
Siano: $\underline{x}=(x_1,...,x_n)$ e $\lambda\in\mathbb{R}$, si ha che:
$$\lambda\cdot\underline{x}=(\lambda\cdot x_1,...,\lambda\cdot x_n)$$
\subsection{Altro sui Sistemi Lineari}
Per ogni vettore va sempre specificato lo spazio vettoriale. Se ho $A\cdot \underline{x}=\underline{b}$ posso risolvere il sistema e dire che $\underline{b}\in Im(A)$.\\
Si ha che i vettori usuali dello spazio, del piano o della retta formano uno spazio vettoriale. Infatti:
\begin{itemize}
	\item essendoci corrispondenza biunivoca tra spazio e $\mathbb{R}^3$ ogni vettore dello spazio si può indicare con un vettore di $\mathbb{R}^3$ che è formato da una terna di numeri
	\item essendoci corrispondenza biunivoca tra il piano e $\mathbb{R}^2$ ogni vettore del piano si può indicare con un vettore di $\mathbb{R}^2$ che è formato da una coppia di numeri
	\item essendoci corrispondenza biunivoca tra la retta e $\mathbb{R}$ ogni vettore della retta si può indicare con un vettore di $\mathbb{R}$ che è formato da un numero
\end{itemize}
\section{Proprietà degli spazi vettoriali}
Su uno spazio vettoriale $V$ su $\mathbb{R}$ si definiscono le seguenti proprietà della somma:
\begin{itemize}
	\item $\underline{v_1}+\underline{v_2} = \underline{v_2}+\underline{v_3}\longrightarrow \underline{v_2}=\underline{v_3}$
	\item $\underline{v_1}+\underline{v_2} = \underline{v_2}+\underline{v_1}$
	\item $(\underline{v_1}+\underline{v_2})+\underline{v_3}=\underline{v_1}+(v_2+\underline{v_3})$
	\item $\underline{v}+\underline{0}=\underline{v}$
	\item $\underline{v}+(-\underline{v})=\underline{0}$
	\item $\lambda\cdot(\underline{v_1}+\underline{v_2} )=\lambda\cdot \underline{v_2}+\lambda\cdot\underline{v_1}$
	\item $(\lambda_1+\lambda_2)\cdot \underline{v}=\lambda_1\cdot \underline{v}+\lambda_2\cdot \underline{v}$
	\item esiste un solo $\underline{x}$ tale che :
	      $$\underline{u}+\underline{v}=\underline{x}\,\, ovvero \,\, \underline{x}=\underline{v}+(-\underline{u})$$
\end{itemize}
per il prodotto si hanno invece le seguenti proprietà, $\forall v,u\in V$
\begin{itemize}
	\item $(\alpha\cdot \beta)\cdot \underline{v}=\alpha\cdot (\beta\cdot \underline{v}),\,\, \forall\alpha,\beta \in \mathbb{R}$
	\item $\alpha\cdot (\underline{u}+\underline{v})=\alpha\cdot \underline{u}+\alpha\cdot \underline{v},\,\,\forall\alpha\in\mathbb{R}$
	\item $(\alpha+\beta)\cdot\underline{v}=\alpha\cdot \underline{v}+\beta\cdot \underline{v},\,\,\forall \alpha,\beta\in\mathbb{R}$
	\item $1\cdot \underline{v}=\underline{v}$
	\item $0\cdot \underline{v}=\underline{0}$
	\item $\alpha\cdot \underline{0}=\underline{0},\,\,\forall \alpha\in\mathbb{R},\, \underline{0}\in V$
	\item $(-1)\cdot \underline{v}=-\underline{v}$
\end{itemize}
\section{Sottospazi Vettoriali}
\begin{definizione}
	Un sottospazio vettoriale di $V$ è un sottoinsieme $W\subseteq V$ con le seguenti proprietà:
	\begin{itemize}
		\item $\underline{0}\in W$
		\item $\underline{w_1},\,\underline{w_2}\in W\longrightarrow \underline{w_1}+\underline{w_2}\in W$
		\item $\lambda\in\mathbb{R},\,\underline{w}\in W \longrightarrow \lambda\cdot W \in W	$
	\end{itemize}
\end{definizione}
\begin{teorema}
	Un sottoinsieme non vuoto $S$ di $V$ è un sottospazio sse comunque presi $\underline{v}$ e $\underline{u}$ $\in S$ e $\alpha,\beta \in \mathbb{R}$ si ha:
	$$\alpha\cdot \underline{v}+\beta\cdot \underline{u}\in S$$
\end{teorema}
\begin{teorema}
	Se $S$ è un sottospazio di $V$ allora $S$ è un sottospazio vettoriale rispetto alle stesse operazioni definite in $V$
\end{teorema}
\begin{definizione}
	Si hanno 2 sottospazi particolari in ogni spazio vettoriale $V$:
	\begin{itemize}
		\item $V$ stesso, che è detto \textbf{sottospazio improprio}
		\item $\{\underline{0}\}$, che è detto \textbf{sottospazio banale}
	\end{itemize}
\end{definizione}
\begin{definizione}
	Sia $V$ uno spazio vettoriale qualsiasi e siano $\underline{v}_1,...,\underline{v}_n$ vettori di $V$. Si dice \textbf{combinazione lineare} di $\underline{v}_1,...,\underline{v}_n$ ogni somma del tipo:
	$$\alpha_1\cdot \underline{v}_1+\cdots+\alpha_n\cdot \underline{v}_n=\sum_{i=1}^n \alpha_i\cdot \underline{v}_i$$
	con $\alpha_1,...,\alpha_n \in \mathbb{R}$
\end{definizione}
\begin{definizione}
	Si definisce \textbf{sottospazio di V generato da} $\underline{v}_1,...,\underline{v}_n$ l'insieme di tutte le possibili combinazioni di $\underline{v}_1,...,\underline{v}_n$ . Esso viene indicato con:
	$$[\underline{v}_1,...,\underline{v}_n]\,\, o\,\, span\{\underline{v}_1,...,\underline{v}_n\}$$
	Questo insieme di combinazioni lineari è sottospazio di $V$
	\begin{proof}
		Siano $\alpha_1\cdot \underline{v}_1+\cdots+\alpha_n\cdot \underline{v}_n$ e $\beta_1\cdot \underline{v}_1+\cdots+\beta_n\cdot \underline{v}_n$ due combinazioni lineari di $\underline{v}_1,...,\underline{v}_n$. La loro somma sarà:
		$$(\alpha_1+\beta_1)\cdot\underline{v}_1+\cdots+(\alpha_n+\beta_n)\cdot \underline{v}_n$$
		che è combinazione lineare di $\underline{v}_1,...,\underline{v}_n$. Inoltre sia $\gamma\in\mathbb{R}$ si che anche:
		$$\gamma\cdot(\alpha_1\cdot \underline{v}_1+\cdots+\alpha_n\cdot \underline{v}_n)=(\gamma\cdot\alpha_1)\cdot \underline{v}_1+\cdots+(\gamma\cdot\alpha_n)\cdot \underline{v}_n$$
		è combinazione lineare di $\underline{v}_1,...,\underline{v}_n$.
	\end{proof}
\end{definizione}
\section{Basi e Dimensione}
\begin{definizione}
	Si dice che uno spazio vettoriale $V$ ha \textbf{dimensione finita} se esistono dei vettori $\underline{v}_1,...,\underline{v}_n$ che generano tutto $V$, ovvero tali che ogni elemento di $V$ sia da loro generato.\\
	In caso contrario si parla di \textbf{dimensione infinita}
\end{definizione}
\begin{nota}
	Si ha che lo spazio dei polinomi $\mathbb{P}$ è di dimensione infinita mentre gli spazi $\mathbb{P}_n$ sono dimensione finita. Un polinomio di grado $\leq n$ si scrive come combinazione lineare dei polinomi $1,x,x^2,...,x^n$
\end{nota}
Anche per i vettori si ha la nozione di \textbf{dipendenza lineare}:
\begin{definizione}
	Dati i vettori $\underline{v}_1,...,\underline{v}_n$ si dice che essi sono \textbf{linearmente dipendenti} se esistono $n$ numeri reali $\alpha_1,...,\alpha_n$ non tutti nulli tali che si abbia la combinazione lineare:
	$$\alpha_1\cdot \underline{v}_1+\cdots+\alpha_n\cdot \underline{v}_n=\underline{0}$$
	Di conseguenze si ha che i vettori sono \textbf{linearmente dipendenti} sse la combinazione lineare sia nulla solo $\forall \alpha_i=0$.\newpage
	Si hanno quindi le seguenti proprietà:
	\begin{itemize}
		\item se il vettore $\underline{v}$ è combinazione lineare dei vettori $\underline{v}_1,...,\underline{v}_n$  allora tali vettori sono linearmente indipendenti
		\item se i vettori $\underline{v}_1,...,\underline{v}_n$  sono linearmente dipendenti significa che almeno uno di essi si ottiene da una combinazione lineare dei rimanenti
		\item se uno dei vettori $\underline{v}_1,...,\underline{v}_n$  è il vettore nullo si ha che i vettori sono linearmente dipendenti
		\item se $\underline{v}_1,...,\underline{v}_n$  sono linearmente indipendenti allora nessuno dei vettori è quello nullo
		\item se due dei vettori $\underline{v}_1,...,\underline{v}_n$  sono uguali essi sono linearmente dipendenti
		\item se ai vettori $\underline{v}_1,...,\underline{v}_n$ linearmente dipendenti si aggiungono i vettori $\underline{v}_{n+1},...,\underline{v}_m$ i vettori $\underline{v}_1,...,\underline{v}_n,\underline{v}_{n+1},...,\underline{v}_m$  saranno linearmente dipendenti
		\item se da i vettori $\underline{v}_1,...,\underline{v}_n$  linearmente indipendenti si tolgono dei vettori i restanti saranno comunque indipendenti
		\item se i vettori $\underline{w},\underline{v}_1,...,\underline{v}_n$ sono linearmente dipendenti e $\underline{v}_1,...,\underline{v}_n$ sono linearmente indipendenti si h che il vettore $\underline{w}$ è combinazione lineare dei $\underline{v}_1,...,\underline{v}_n$
	\end{itemize}
\end{definizione}
\subsection{Basi}
\begin{definizione}
	Sia $V$ uno spazio vettoriale di dimensione finita. L'insieme di vettori $\underline{v}_1,...,\underline{v}_n$ è \textbf{base} se sono un sistema di generatori per $V$ e sono linearmente indipendenti
\end{definizione}
\begin{nota}
	Si definisce la \textbf{base canonica} in $\mathbb{R}^n$ quella formata dai seguenti vettori:
	$$e_1=(1,0,...,0)$$
	$$e_2=(0,1,...,0)$$
	$$\vdots$$
	$$e_n=(0,0,...,1)$$
	che sono vettori indipendenti e generano tutto $\mathbb{R}^n$
\end{nota}
\newpage
\begin{teorema}
	Sia $V$ uno spazio vettoriale non nullo di dimensione finita, allora esiste almeno una base di $V$ e qualora esistano più basi diverse di $V$ esse hanno lo stesso numero di elementi
\end{teorema}
\subsection{Dimensione di uno spazio vettoriale}
Dall'ultimo teorema segue la seguente definizione:
\begin{definizione}
	Sia $V$ uno spazio vettoriale non nullo di dimensione finita. Si chiama \textbf{dimensione} di $V$, che si indica con $dim(V)$, il numero di vettori che compongono una base di $V$
\end{definizione}
Si hanno le seguenti dimensioni principali:
\begin{itemize}
	\item lo spazio vettoriale nullo ha dimensione 0, $dim(\{\underline{0}\})=0$
	\item nel campo $\mathbb{R}$ si ha che $dim(\mathbb{R}^n)=n$
	\item per le matrici si ha che $dim(M_{n,m})=n\cdot m$
	\item per lo spazio dei polinomi si ha che $dim (\mathbb{P}_n)=n+1$
\end{itemize}
\begin{shaded}
	Si ha il seguente teorema \textit{\underline{(utile ma non affrontato a lezione)}}:
	\begin{teorema}
		Sia $V$ uno spazio vettoriale non nullo di dimensione finita $n$. Se $\underline{v}_1,...,\underline{v}_s$ generano $V$, con $s>n$, allora togliendo opportuni vettori da $\underline{v}_1,...,\underline{v}_s$ si ottiene una base di $V$.\\
		Se invece ho vettori $\underline{v}_1,...,\underline{v}_m$ linearmente indipendenti si ha che questi formano una base di $V$ oppure esistono opportuni vettori di $V$, $\underline{v}_{m+1},...,\underline{v}_n$ tali che $\{\underline{v}_1,...,\underline{v}_m,\underline{v}_{m+1},...,\underline{v}_n\}$ è una base di $V$
	\end{teorema}
\end{shaded}
\begin{teorema}
	Sia $V$ uno spazio vettoriale non nullo di dimensione finita $n$ e sia $B=\{\underline{v}_1,...,\underline{v}_n\}$ un'insieme di vettori linearmente indipendenti. $B$ è una base di $V$ sse$n$ è il massimo numero di vettori linearmente indipendenti in $V$.\\
	Quindi $n$ vettori linearmente indipendenti in uno spazio vettoriale $V$ di dimensione $n$ sono una base. Inoltre sia $C=\{\underline{v}_1,...,\underline{v}_s\}$ un sistema di generatori per $V$, con $s>n$. Da $C$ si può ottenere una base rimuovendo opportunamente dei vettori fino ad ottenere un insieme massimale di vettori linearmente indipendenti
\end{teorema}
Si ha infine il seguente teorema:
\begin{teorema}
	Sia $V$ uno spazio vettoriale non nullo di dimensione finita $n$ e sia $B=\{\underline{v}_1,...,\underline{v}_n\}$ una base fissata di $V$. Allora ogni vettore $\underline{v}\in V$ si può esprimere in uno e un solo modo come combinazione lineare di $\underline{v}_1,...,\underline{v}_n$
\end{teorema}
\begin{definizione}
	Data una base $B=\{\underline{v}_1,...,\underline{v}_n\}$ dello spazio vettoriale $V$ si ha che $\forall \underline{v}\in V$ esistono univocamente $n$ numeri $\alpha_1,...,\alpha_n$ della combinazione lineare di $\underline{v}_1,...,\underline{v}_n$ che produce $\underline{v}$. Tali numeri sono detti \textbf{coordinate (o componenti)} di $\underline{v}$ rispetto alla base $B$.\\
	Per esempio notiamo che le componenti di $\underline{v}=(x_1,...,x_n)$ rispetto alla base canonica sono proprio $x_1,...,x_n$
\end{definizione}
\begin{shaded}
	I concetti di base e dimensione possono essere estesi ad un qualsiasi sottospazio vettoriale $S$ di $V$, essendo lui a sua volta uno spazio vettoriale.
\end{shaded}
\begin{nota}
	Per trovare le componenti di un certo vettore $\underline{v}$ rispetto alla base $B$, per esempio in $\mathbb{R}^2$ $B=\{\underline{w_1},\underline{w_2}\}$ si procede così:
	\begin{itemize}
		\item si scrive $\underline{v}=\alpha\cdot \underline{w_1}+\beta\cdot \underline{w_2}$
		\item ricordando che $\underline{v},\underline{w_1}$ e $\underline{w_2}$ sono vettori colonna risolvo il sistema lineare associato alla matrice che si viene a formare, con il vettore $\underline{v}$ che sarà la colonna dei termini noti.
		\item trovati $\alpha$  e $\beta$ (nel caso a 2 dimensioni, per $n$ dimensioni avrò $n$ coefficienti) riscrivo $\underline{v}=\alpha\cdot \underline{w_1}+\beta\cdot \underline{w_2}$ con i valori di $\alpha$ e $\beta$ trovati
	\end{itemize}
\end{nota}
\begin{nota}
	Sia $S$, generato da $\underline{v}_1,...,\underline{v}_s$,  un sottospazio di $\mathbb{R}^n$, considero la matrice le cui righe sono le componenti di questi vettori di $S$ rispetto alla base canonica. Dato che il rango ella matrice è dato dal numeri di righe linearmente indipendenti si ha che il rango rappresenta la dimensione del sottospazio generato dai vettori riga e i vettori indipendenti sono base di $S$. Inoltre si ha che $n$ vettori di $\mathbb{R}^n$formano una base sse il determinante della matrice quadrata, le cui righe sono le componenti di questi vettori, ha determinante non nullo.
\end{nota}
\begin{teorema}
	Sia $V$ uno spazio vettoriale non nullo di dimensione finita $n$ e $S$ un suo sottospazio vettoriale.\begin{center} Se $dim(S)=dim(V)$ allora si ha che $S$ coincide con $V$\end{center}
\end{teorema}

\chapter{Applicazioni Lineari}
\begin{definizione}
	Suppongo $f:\mathbb{R}\rightarrow \mathbb{R}$. $f$ è lineare se:
	\begin{itemize}
		\item $f(x+y)=f(x)+f(y)$
		\item $f(k\cdot x)=k\cdot f(x), \,k\in \mathbb{R}$
		\item $f(x-x)=f(x)+f(-x)= f(x)-f(x)=0\longrightarrow f(0)=0$
	\end{itemize}
	$f$ se è lineare manda 0 in 0.
	Per esempio $\log(x+y)$ non è lineare
\end{definizione}
il grafico di una funzione lineare è una retta che passa per l'origine.
Quindi $$f(x)=m\cdot x$$ in quanto: $$x=x\cdot 1\rightarrow f(x)=f(x\cdot 1)=x\cdot f(1),\, f(1)=m$$
Si ha che $f:\mathbb{R}^2\rightarrow \mathbb{R}$ è lineare, infatti penso a $\underline{x}=(\underline{x_1},\underline{x_2})$  e $\underline{y}=(\underline{y_1},\underline{y_2})$ si ha:
\begin{itemize}
	\item  $f(\underline{x}+\underline{y})=f(\underline{x})+f(\underline{y})$
	\item $f(k\cdot \underline{x})=k\cdot f(\underline{x}), \,k\in \mathbb{R}$
\end{itemize}

\newpage
\begin{definizione}
	SIa $f:V\rightarrow W$ un'applicazione lineare, l'insieme $f^{-1}(\underline{0})=\{\underline{v}\in V:\, f(\underline{v})=\underline{0}\}$ si dice \textbf{nucleo (kernel) dell'applicazione} $f$ e si indica con:
	$$ker(f)$$
	L'insieme $f(V)=\{f(\underline{v}),\, \underline{v}\in V\}$ si dice \textbf{immagine dell'applicazione} $f$ e si indica con:
	$$Im(f)$$
	Si definisce \textbf{rango dell'applicazione} $f$ la dimensione di $Im(f)$
\end{definizione}
Se $X$ è lo spazio di tutte le funzioni da $\mathbb{R}$ in $\mathbb{R}$  il sottoinsieme delle funzioni lineari è un sottospazio. Devo vedere se:
\begin{itemize}
	\item $\underline{0}\in W$ (lo dimostro mettendo 0 al posto di y nelle proprietà delle funzioni lineari)
	\item siano $f_1(x)=m_1\cdot x$, $f_2(x)=m_2\cdot x$ funzioni lineari, la loro somma è lineare $f_3(x)=m_3\cdot x,\, m_3=m_1+m_2$
	\item $k\cdot f(x)\in W$
\end{itemize}
Quindi la somma di funzioni lineari è lineare.\\
\textit{(nell'esempio si legga $x=\underline{x}$ e $y=\underline{y}$)}
$$(f+g)(x+y)=f(x+y)+g(x+y)=f(x)+f(y)+g(x)+g(y)$$$$=f(x)+g(x)+f(y)+g(y)=(f+g)(x)+(f+g)(y)$$
inoltre:
$$(f+g)(\lambda\cdot x)=f(\lambda\cdot x)+f(\lambda\cdot y)=\lambda\cdot f(x)+\lambda\cdot g(x)= \lambda\cdot(f+g)(x)$$
$f:\mathbb{R}^2\rightarrow \mathbb{R}$ quindi $$f(\underline{x}+\underline{y})=x_1+y_1=f(\underline{x})+f(\underline{y})$$
e $$\lambda\cdot\underline{x}=(\lambda\cdot x_1,\,\lambda\cdot x_2)$$
con: $$f(\lambda\cdot\underline{x})=(\lambda\cdot x_1)=\lambda\cdot f(\underline{x})$$
una funzione: $h:\mathbb{R}^2\rightarrow \mathbb{R}$ con $h(x)=\alpha\cdot x_1+\beta\cdot x_2$ è lineare ed è l'unico caso di funzioni da $\mathbb{R}^2$ in $\mathbb{R}$
\begin{proof}
	prendo $F$ lineare $F\mathbb{R}^2\rightarrow \mathbb{R}$. $F(1,0)=k,\, k\in\mathbb{R}$ allora siccome $F$ è lineare allora $$F(x_1,0)=F(x_1\cdot 1, 0)=F(x_1\cdot (1,0))=x_1\cdot F(1,0)=x_1\cdot k$$
	sia $F(0,1)=d$ quindi $$F(0,x_2)=F(x_2\cdot(0,1))=x_2\cdot F(0,1)=x_2\cdot d$$
	infine:
	$$F(x_1,x_2)=F(x_1,0)+F(0,x_2)=x_1\cdot k+x_2\cdot d$$
\end{proof}

Passo al caso $\mathbb{R}^n$ con $\underline{x}=x_1,...,x_n$ e $\underline{y}=y_1,...,y_n$, n-uple. le proprietà sono come sopra solo con $n$ elementi e non 2. Si avranno $n$ proiezioni $P_k$, $P_k(x_1,...,x_n)=x_k$ e ogni $P_k$ è lineare. In generale $(x_1,...x_n)\rightarrow\alpha_l\cdot x_k$. Posso sommare le proiezioni e ottenere una funzione lineare:$$F(x_1,...x_n)=\alpha_1\cdot x_1+...+\alpha_n\cdot x_n$$
e si può scrivere come:
$$
	\left(\begin{matrix}
			\alpha_1 & \cdots & \alpha_n
		\end{matrix}\right)\cdot
	\left(\begin{matrix}
			x_1    \\
			\vdots \\
			x_n
		\end{matrix}\right)=\alpha_1\cdot x_1+...+\alpha_n\cdot x_n=f(x,x_n)
$$
definisco la base fondamentale a 3 dimensioni:
$$(1,0,0)=e_1$$
$$(0,1,0)=e_2$$
$$(0,0,1)=e_3$$
Si ha: $$F(x_1\cdot e_1+...+x_n\cdot e_n)=x_1\cdot F(e_1)+...+x_n\cdot F(e_n)$$
\newpage
\begin{teorema}
	se $f:V\rightarrow W$ è lineare allora $Ker(f)$ è un sottospazio vettoriale di $V$
\end{teorema}
\begin{proof}
	mostro che $W=Ker(F)$ è un sottospazio.
	\begin{itemize}
		\item $\underline{0}\in W$ perché è lineare ($F$ manda 0 in 0)
		\item $\underline{x},\underline{y}\in W $ allora $\underline{x}+\underline{y}\in W$ perché $F(\underline{x})+F(\underline{y})=0+0=0$
		\item $F(\lambda\cdot \underline{x})=\lambda\cdot F(\underline{x})=0\cdot 0=0$
	\end{itemize}
\end{proof}
\begin{teorema}
	L'immagine di un'applicazione $f:V\rightarrow W$ è un sottospazio vettoriale di $W$
\end{teorema}
\textit{I seguenti 2 teoremi sono comodi ma non sono stati affrontati a lezione dalla professoressa kuhn}
\begin{teorema}
	Sia $f:V\rightarrow W$ un'applicazione lineare. $f$ è iniettiva sse $ker(f)=\{\underline{0}\}$ mentre è suriettiva sse il rango di $f$ è uguale alla dimensione di $W$.\\
	Inoltre se $dim(V)=dim(W)$ $f$ è iniettiva sse $f$ è suriettiva
\end{teorema}
\begin{teorema}
	Sia $f:V\rightarrow W$ un'applicazione lineare, con $V$ di dimensione finita. Vale la seguente uguaglianza:
	$$dim(Ker(f))+dim(Im(f))=dim(V)$$
\end{teorema}
\begin{teorema}
	Sia $f:V\rightarrow W$ un'applicazione lineare. Se $\{\underline{v}_1,...,\underline{v}_n\}$ è una base di $V$ allora $f(\underline{v}_1),..., f(\underline{v}_n)$ generano $Im(f)$
\end{teorema}
\begin{teorema}
	Sia $f:V\rightarrow W$ un'applicazione lineare. Se $B=\{\underline{v}_1,...,\underline{v}_n\}$ è una base di $V$ allora $\{f(\underline{v}_1),..., f(\underline{v}_n)\}$ è una base di $Im(f)$
\end{teorema}
\begin{definizione}
	Sia $f:V\rightarrow W$ un'applicazione lineare. Si ha che:
	\begin{itemize}
		\item $f$ è un \textbf{endomorfismo} (di $V$) se $V=W$
		\item $f$ è un \textbf{isomorfismo} se è biunivoca e $V$ e $W$ sono detti \textbf{isomorfi}. $f$ quindi è suriettiva, quindi $Im(f)=W$, e iniettiva, quindi $dim(Im(f))=dim(V)$. Due spazi isomorfi hanno quindi la stessa dimensione. Si nota che uno spazio vettoriale di dimensione $n$ su $\mathbb{R}$ è isomorfo a $\mathbb{R}^n$
	\end{itemize}
\end{definizione}
\subsection{Applicazioni e matrice rappresentativa}
Presa una applicazione lineare (tipo $f(x,y,z)=(z-x,y+z)$) posso associare una matrice con colonne pari al numero di incognite e righe pari al numero di operazioni (nell'esempio sarà una $2\times 3$) mettendo nella colonna i-esima le coordinare di $f(e_i)$, con $e_i$ i-esima base canonica.\\ Seguendo l'esempio sopra si avrà:
$$f(1,0,0)=(-1,0) \mbox{ ovvero i coefficienti della x}$$
$$f(0,1,0)=(0,1) \mbox{ ovvero i coefficienti della y}$$
$$f(0,0,1)=(1,1) \mbox{ ovvero i coefficienti della z}$$
formando così la seguente matrice rappresentativa:
$$
	M_f\left(\begin{matrix}
			-1 & 0 & 1 \\
			0  & 1 & 1
		\end{matrix}\right)
$$
che moltiplicata per il vettore colonna $
	\left(\begin{matrix}
			x \\
			y \\
			z
		\end{matrix}\right)
$, formato dalle componenti del vettore $(x,y,z)$ secondo la base canonica di $\mathbb{R}^3$, da esattamente $\left(\begin{matrix}
			z-x \\
			y+z
		\end{matrix}\right)
$.\\ \textbf{Si può quindi dire che applicare f o moltiplicare per la matrice rappresentativa da lo stesso risultato}
\begin{definizione}
	Formalizziamo quanto detto fin'ora.\\
	Siano $V$ e $W$ due spazi vettoriali di dimensione finita, $dim (V)=n$ e $dim(W)=m$. Siano $B=\{\underline{v}_1,...,\underline{v}_n\}$ una base di $V$ e $B^{'}=\{\underline{v}_1,...,\underline{v}_m\}$ una base di $W$. Considero l'applicazione $f:V\to W$. In generale si ha che, $\forall i,\, 1\leq i\leq n$, poiché $f(\underline{v}_i)\in W$, esistono dei numeri $a_{1i},...,a_{mi}\in \mathbb{R}$ tali che:
	$$f(\underline{v}_i)=a_{1i}\cdot \underline{w}_1+\cdots+a_{mi}\cdot \underline{w}_m$$
	Si consideri ora la seguente matrice:
	$$
		A_f=\left(\begin{matrix}
				a_{11} & a_{12} & \cdots & a_{1n} \\
				\vdots & \ddots & \ddots & \vdots \\
				\vdots & \ddots & \ddots & \vdots \\
				a_{m1} & a_{m2} & \cdots & a_{mn}
			\end{matrix}\right)
	$$
	ottenuta scrivendo nell'ordine i coefficienti $a_{ij}$ di tutte queste combinazioni lineari. Nella prima colonna si avranno le componenti di $f(\underline{v}_i$ e cosi via rispetto alle $B^{'}$.\\
	Si prenda un vettore $\underline{v}\in $, che si scriverà come combinazione lineare di $\underline{v}_1,...,\underline{v}_n $, ovvero:
	$$\underline{v}=c_1\cdot \underline{v}_1+\cdots +c_n\cdot\underline{v}_n$$
	analogamente il vettore $f(\underline{v})$ si scriverà come combinazione lineare di $\underline{w}_1,...,\underline{w}_n $ ovvero:
	$$f(\underline{v})=k_1\cdot \underline{w}_1+\cdots+k_m\cdot \underline{w}_m$$
	Quindi pero ogni vettore $\underline{v}\in V$ la matrice $A$ ha questa proprietà:
	$$
		\left(\begin{matrix}
				a_{11} & a_{12} & \cdots & a_{1n} \\
				\vdots & \ddots & \ddots & \vdots \\
				\vdots & \ddots & \ddots & \vdots \\
				a_{m1} & a_{m2} & \cdots & a_{mn}
			\end{matrix}\right)\cdot\left(\begin{matrix}
				c_1    \\
				c_2    \\
				\vdots \\
				c_n
			\end{matrix}\right)=\left(\begin{matrix}
				k_1    \\
				k_2    \\
				\vdots \\
				k_n
			\end{matrix}\right)
	$$
	ovvero moltiplicando $A_f$ per la colonna delle coordinate di $\underline{v}$ si ottiene la colonna delle coordinate (o componenti) di $f(\underline{v})$ rispetto alla base $B^{'}$. \\
	La matrice $A_f$ è detta \textbf{Matrice Associata all'applicazione lineare} $f$ rispetto alle basi $B$ e $B^{'}$.\\
	Se $V=W $ e $B=B^{'}$ allora la matrice associata a $f$ lo sarà solo rispetto alla bas e $B$
\end{definizione}
\newpage
\subsection{Funzioni lineari}
Sono $f:\mathbb{R}^n\rightarrow \mathbb{R}$ \\
Trattiamo le $f:\mathbb{R}^2\rightarrow \mathbb{R}$, trattiamo quindi vettori e non numeri (salvo gli scalari). $f:\mathbb{R}^2\rightarrow \mathbb{R}$ (significa assegnare due funzioni da $\mathbb{R}$ in $\mathbb{R}$ quindi $f=(f_1,\,f_2)$).$f$ è lineare sse $f_1$ e $f_2$ sono lineari, inoltre $f(v_1+v_2)=f(v_1)+f(v_2)$ e $ f(\lambda\cdot v)=\lambda\cdot f(v)$, dove sono tutti vettori (anche $f$) tranne $\lambda$. So che $f_1(x,y)=\alpha\cdot x +\beta\cdot y$ e $f_2(x,y)=\gamma\cdot x +\delta\cdot y$ ottengo la matrice:
$$
	A=\left(\begin{matrix}
			\alpha & \beta  \\
			\gamma & \delta
		\end{matrix}\right)\cdot
	\left[\begin{matrix}
			x \\
			y
		\end{matrix}\right]=
	\left(\begin{matrix}
			\alpha\cdot x +\beta\cdot y \\
			\gamma\cdot x +\delta\cdot y
		\end{matrix}\right)
$$
$f:\mathbb{R}^2\rightarrow \mathbb{R}$ è lineare sse $\exists A\rightarrow f(x,y)=A\cdot
	\left[\begin{matrix}
			x \\
			y
		\end{matrix}\right]$ e $A$ è detta matrice rappresentativa di $f$.\\

Si ha che:$$v=x\cdot e_1+y\cdot e_2=(x,y)\rightarrow f(v)=f(x\cdot e_1)+ f(y\cdot e_2) = x\cdot f(e_1)+y\cdot f(e_2)$$
quindi:
$$f(e_1)=(f_1(e_1),f_2(e_2))=(\alpha,\gamma)=\mbox{ prima colonna di A}$$
$$e$$
$$f(e_2)=(f_1(e_2),f_2(e_2))=(\beta,\delta)=\mbox{ seconda colonna di A}$$
$$A\cdot e_1=A\cdot
	\left(\begin{matrix}
			1      \\
			0      \\
			\vdots \\
			0
		\end{matrix}\right)=\mbox{ prima colonna}
$$
e cosi via per le altre colonne.

$$A=
	\left(\begin{matrix}
			\alpha\cdot x \\
			\gamma \cdot x
		\end{matrix}\right)+\left(\begin{matrix}
			\beta\cdot y \\
			\delta \cdot y
		\end{matrix}\right)=
	x\cdot\left(\begin{matrix}
			\alpha \\
			\gamma
		\end{matrix}\right)+
	y\cdot\left(\begin{matrix}
			\beta \\
			\delta
		\end{matrix}\right)
$$
$f(v)$ è combinazione lineare delle colonne di $A$
$f:\mathbb{R}^n\to \mathbb{R}^n$ lineare si rappresenta con una matrice $n\times n$:
$$
	A=\left[\begin{matrix}
			f(e_1) & f(e_2) & \cdots & f(e_n)
		\end{matrix}\right]
$$
\textit{con i vari $e_k$ vettori di $n$ righe}\\
Se ho invece, per esempio $f:\mathbb{R}^3\to \mathbb{R}^2$ lineare si rappresenta con una matrice $2\times 3$:
$$
	A=\left[\begin{matrix}
			f(e_1) & f(e_2) & f(e_3)
		\end{matrix}\right]
$$
\textit{con i vari $e_k$ vettori di $2$ righe}\\
\newpage
Se $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$, da $X$ a $Y$ è lineare allora $ker(f)$ è un sottospazio vettoriale di $X$ e $Im(f)$ è sottospazio vettoriale di $Y$
Vediamo perché $Im(f)$ è sottospazio vettoriale di $Y$:\\
\begin{proof}
	$W=Im(f)$, quindi:
	\begin{itemize}
		\item $0\in W$
		\item $w_1,\,w_2\in Im(f)\rightarrow w_1+w_2\in W$ allora $\exists v_1,\, v_2\in X: f(v_1)=w_1 \,\, e \,\, f(v_2)=w_2$, quindi $f(v_1+v_2)=f(v_1)+f(v_2)=w_1+w_2$
		\item se $w\in Im(f)\rightarrow \lambda\cdot w\in Im(f)$ infatti: $\exists v\in X:\, f(v)=w$ ergo: $f(\lambda\cdot v)=\lambda\cdot f(v))=\lambda\cdot w$
	\end{itemize}
\end{proof}
Se $W$ è sottospazio di $Y$ allora $f^{-1}(W)$ è un sottospazio di $X$ e se $A$ è sottospazio di $X$ allora $Im(A)$ è sottospazio di $Y$.\\
I sottospazi di $\mathbb{R}^2$ è di dimensione $1$. La dimensione di $\underline{0}$ è $0$.\\
Sia $$v=(a,b)\rightarrow t\cdot v\rightarrow (t\cdot a , t\cdot b)$$ sostituisco $x=a\cdot t$ e $y=b\cdot t$ quindi $y=\frac{b}{a}\cdot t$ ... quindi $\mathbb{r}$ è un luogo di punti con sottospazi di dimensione 1 (escludendo i casi banali $\underline{0}$ e $\mathbb{R}^2$), ovvero sono tutte le rette passanti per l'origine. \\
Vado in $\mathbb{R}^3$, qui abbiamo sottospazi di dimensione 1 (generato da un vettore) e 2 (generato da 2 vettori), oltre ai banali.
per la dimensione 1 si ha:
$$v=(a,b,c)\rightarrow t\cdot v\rightarrow (t\cdot a , t\cdot b, t\cdot c)$$sostituisco $x=a\cdot t$, $y=b\cdot t$ e $z=c\cdot t$ e rappresentano una retta passante per l'origine. Si ha quindi:
$$
	\begin{cases}
		t=\frac{x}{a} \\
		t=\frac{y}{b} \\
		t=\frac{z}{c}
	\end{cases}=
	\begin{cases}
		\frac{x}{a}=\frac{y}{b} \\
		\frac{y}{b}=\frac{z}{c}
	\end{cases}$$
$$a\cdot x+b\cdot y+c\cdot z=0$$\\
Per la dimensione 2 si ha il sottospazio vettoriale di tutti i piani passanti per l'origine degli assi. \\
\textit{Si ricorda che retta e piano sono luoghi di punti}\\
Il sottospazio generato da $v_1,...,v_k$ è l'insieme dei $W=\alpha_1\cdot _1,...,\alpha_k\cdot v_k$\\$=<\alpha_1\cdot v_1,...,\alpha_k\cdot v_k>$
	\newpage
	Sia $f$ lineare, $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ può essere rappresentato da una matrice $A$, detta \textit{matrice rappresentativa}. $ker(f)=\{x|\,f(x)=0\}=\{x|\,A\cdot x=0\}$ è un sottospazio.\\
	L'insieme delle soluzioni di $A\cdot x=b$ è sottospazio solo se $b=0$ (per la prima proprietà dei sottospazi).\\
	\begin{definizione}
		%guardare qualche esempio pratico
		Si ha la definizione del traslato di un sottospazio $V$:\\
		scelgo un $b\in \mathbb{R}^n$ e $b\neq 0$ allora il traslato è l'insieme $W=\{b=b+v,\,v\in V\}$. Geometricamente, per esempio in $\mathbb{R}^2$, sarà una retta parallela a $v$ traslata di $b$.\\
		Sia $A\cdot x=b$ vero per un certo $x_0$, per Rouché-Capelli. Cerco le soluzioni traslate.
		Ogni altra soluzione di $A\cdot x=b$  si può scrivere come:
		$$x_0+v\,\ dove\,\, A\cdot v =0$$
	\end{definizione}
	\begin{proof}
		Sia $x^{'}$ una qualunque soluzione di $A\cdot x=b$. ho quindi: $A\cdot x^{'}=b$  e v=$x-x_0\rightarrow A\cdot v= A\cdot(x-x_0)=A\cdot x^{'}-A\cdot x_0=b-b=0$.\\
		Quindi $v\in ker(f)$ inoltre $x^{'}=x_0+v$ e il numero di soluzioni dipende dalla dimensione del $ker(f)$.
	\end{proof}

	\begin{esempio}
		$$\begin{cases}
				x-y-z=2 \\
				2\cdot x-2\cdot y +z =4
			\end{cases}$$

		$$
			A=\left(\begin{matrix}
					1 & -1 & -1 \\
					2 & -2 & 1
				\end{matrix}\right)
		$$
		ovvero $A:\mathbb{R}^3\rightarrow \mathbb{R}^2$

		cerco:
		$$\begin{cases}
				x-y-z=0 \\
				2\cdot x-2\cdot y +z =0
			\end{cases}$$
		Ovvero:
		$$
			x\cdot\left(\begin{matrix}
					1 \\
					2
				\end{matrix}\right)+y\cdot\left(\begin{matrix}
					-1 \\
					-2
				\end{matrix}\right)+z\cdot\left(\begin{matrix}
					-1 \\
					1
				\end{matrix}\right)=0
		$$
		Avrò di sicuro due vettori dipendenti (i primi 2) e infatti $rg(A)=2$
		Avrò:
		$$\begin{cases}
				x-z=y \\
				2\cdot x+z=2\cdot y
			\end{cases}
		$$
		\newpage
		Si ha:
		$$
			\left(\begin{matrix}
					1 & -1 \\
					2 & 1
				\end{matrix}\right)
		$$
		con $det=3$. Ho quindi una sola variabile libera (y) e $dim(ker)=n-rg$, $n=$ numero di incognite e $rg=$ rango.\\
		Uso Cramer:
		$$
			x=\frac{\left(\begin{matrix}
					y        & -1 \\
					2\cdot y & 1
				\end{matrix}\right)}{3}=y
		$$
		e
		$$
			z=\frac{\left(\begin{matrix}
					1 & y        \\
					2 & 2\cdot y
				\end{matrix}\right)}{3}=0
		$$
		Comporta:
		$$\begin{cases}
				x=y \\
				y=y \\
				z=0
			\end{cases}=\begin{cases}
				x=t \\
				y=t \\
				z=0
			\end{cases}
		$$
		Quindi si ha il sottospazio: $t\cdot(1,\,1,\,0)$
		Tornando al sistema di partenza si hanno le seguenti soluzioni:\\
		Scelgo $x_0$, una delle infinite soluzioni. Per esempio:
		$$x_0=\begin{cases}
				x=2 \\
				y=0 \\
				z=0
			\end{cases}$$
		Ovvero valori che fanno valere la prima equazione.\\
		Quindi le soluzioni saranno solo del tipo:
		$$
			\left(\begin{matrix}
					2 \\
					0 \\
					0
				\end{matrix}\right)+t\cdot
			\left(\begin{matrix}
					1 \\
					1 \\
					0
				\end{matrix}\right)
		$$
	\end{esempio}
	\newpage
	\section{Endomorfismi e Omomorfismi}
	Siano $V$ e $W$ spazi vettoriali. $f:V\rightarrow W $ è lineare se:
	\begin{itemize}
		\item $f(x+y)=f(x)+f(y)$
		\item $f(\alpha\cdot x)=\alpha\cdot f(x)$
	\end{itemize}
	Sia $V=R[x]$ polinomi di $x$ 3 $f:p(x)\rightarrow p^{'}(x)$, derivate di $p(x)$, f è lineare. Si ha che la dimensione di $V$ è il massimo numero di vettori linearmente indipendenti che posso trovare in $V$. $R[x]$ ha dimensione infinita, \\infatti $\forall n>0, \, n\in \mathbb{N}$ ci sono $n+1$ vettori linearmente indipendenti. $\alpha_0+\cdots+\alpha_n\cdot x^n=0$ infatti mi assicura, per il teorema fondamentale dell'algebra, che ha solo vettori linearmente indipendenti.
	Siano $V$ e $W$ spazi vettoriali. $f:V\rightarrow W $ lineare
	\begin{itemize}
		\item Se $V=W$ $f$ è un endomorfismo
		\item $V\neq W$ $f$ è ha un omomorfismo
	\end{itemize}
	Sia $V$ di dimensione finita pari a $n$. Scelgo una base, che chiamo $e_1,\, e_2,\, ... \,,\, e_n$ e definisco i vari $f(e_i)$. Si ha che:
	\begin{itemize}
		\item $x=\sum_{i=0}^n \alpha_i \cdot e_i$
		\item $f(x)=\sum_{i=0}^n \alpha_i \cdot f(e_i)=y$
	\end{itemize}
	e si ha che i vettori $f(e_i)$ sono un sistema di generatori per l'immagine di $f$, $Im(f)$. Ovvero:
$$y\in Im(f) \longleftrightarrow \exists x\in V ,\,\, f(x)=y$$
e si ha che $$dim(Im(f))\leq dim(f)$$
\begin{esercizio}
	Costruisco una $f:\mathbb{R}^2\rightarrow \mathbb{R}^3$\\ tale che $Im(f)=\{(x,y),\, y=2x+3\}$
	ma non posso perché non passa per l'origine, $\nexists f$.\\
	Cambio retta e scelgo una parallela che passa per l'origine,\\ $Im(f)=\{(x,y),\, y=2x\}$
	Questo è un sottospazio vettoriale.
	Scelgo le basi canoniche di $\mathbb{R}^3$:
	$$e_1=(1,0,0)$$
	$$e_2=(0,1,0)$$
	$$e_3=(0,0,1)$$
	e
	$$f(e_1)=(0,0)$$
	$$f(e_2)=(1,2)$$
	$$f(e_3)=(1,2)$$
	che va bene, posso scegliere punti uguali, e da dimensione 2. Si ha quindi la seguente matrice associata ad $f$ secondo la base canonica:
	$$
		M_e(f)=\left(\begin{matrix}
				0 & 1 & 1 \\
				0 & 2 & 2
			\end{matrix}\right)
	$$
	e si ha che:
	$$\left(\begin{matrix}
				0 & 1 & 1 \\
				0 & 2 & 2
			\end{matrix}\right)\left(\begin{matrix}
				x \\
				y \\
				z
			\end{matrix}\right)\rightarrow \left(\begin{matrix}
				0 \cdot x & 1 \cdot y & 1\cdot z \\
				0 \cdot x & 2 \cdot y & 2\cdot z
			\end{matrix}\right)$$
\end{esercizio}
\section{Cambiamento di Base}
Sia $V$ uno spazio vettoriale su $\mathbb{R}$. Siano $B=\{\underline{v}_1,...,\underline{v}_n\}$ e $B^{'}=\{\underline{w}_1,...,\underline{w}_n\}$ delle basi di $V$. Si quindi che, con $\alpha_i,\,\beta_i\in\mathbb{R}$
$$\underline{v}=\alpha_1\cdot \underline{v}_1+\cdots+\alpha_n\cdot \underline{v}_n$$
$$\underline{w}=\beta_1\cdot \underline{w}_1+\cdots+\beta_n\cdot \underline{w}_n$$
Si ha che, dati $
	X=\left(\begin{matrix}
			\alpha_1 \\
			\vdots   \\
			\alpha_n
		\end{matrix}\right)
$ e $X^{'}=\left(\begin{matrix}
			\beta_1 \\
			\vdots  \\
			\beta_n
		\end{matrix}\right)
$ si ha che esiste una matrice invertibile $C$ tale che:
$$X^{'}=C\cdot X$$
Questa matrice è detta \textbf{matrice associata al cambiamento di base}, da $B$ a $B^{'}$. I vettori colonna di $X$ e $X^{'}$ sono le componenti di $\underline{v}$ rispetto a queste basi.\\
Se $D$ è la matrice associata all'applicazione identica per mezzo di $B$ e $B^{'}$ si ha che $C\cdot D=I$, quindi $C$ è invertibile e $X=C^{-1}\cdot X^{'}$ è la matrice associata al cambiamento di base da $B^{'}$ a $B$.\\
Sia $f$ un endomorfismo di $V$ e $A$ sia la matrice associata a $f$ rispetto a $B$ e $D$ quella rispetto a $B^{'}$. Si cerca una relazione tra $A$ e $D$:\\
sia $\underline{v}$ un vettore di $V$ e sia $\underline{w}=f(\underline{v})$. Se $X$ e $Y$ sono vettori colonna formati dalle componenti di $\underline{v}$ e $\underline{w}$ rispettivamente per mezzo di $B$ si ha che $Y=A\cdot X$.
\newpage
Se invece $X^{'}$ e $Y^{'}$ sono vettori colonna formati dalle componenti di $\underline{v}$ e $\underline{w}$ rispettivamente per mezzo di $B^{'}$ si ha $Y^{'}=D\cdot X^{'}$. Poiché $X^{'}=C\cdot X$ e $Y^{'}=C\cdot Y$, ovviamente con $C$ invertibile si ha:
$$C\cdot Y=D\cdot C\cdot X\longrightarrow Y=(C^{-1}\cdot D\cdot C)\cdot X$$
Si ha quindi che $C^{-1}\cdot D\cdot C$ è la matrice associata a $f$ per mezzo di $B$, perciò, essendo la matrice unica, si ha $A=C^{-1}\cdot D\cdot C$
\begin{definizione}
	Due matrici $A$ e $D$ si dicono \textbf{simili} se esiste una matrice invertibile tale che:
	$$A=C^{-1}\cdot D\cdot C$$
	Due matrici sono simili sse rappresentano la stessa applicazione lineare, inoltre due matrici simili hanno lo stesso determinante, infatti;
	$$det(A)=det(C^{-1}\cdot D\cdot C)=det(C^{-1})\cdot det(D)\cdot det(C)$$
	$$=det(C)^{-1}\cdot det(D)\cdot det(C)=det(D)$$
	\begin{shaded}
		\begin{nota}[determinante di endomorfismi]
			Si chiama \textbf{determinante di un endomorfismo} $f$ il determinante di una matrice associata a $f$ per mezzo di una qualsiasi base, infatti cambiando base si ottiene una matrice simile a quella data, quindi con lo stesso determinante.\\ Se $f$ è un'applicazione con determinate diverso da 0 si ha che $f$ è invertibile.
		\end{nota}
	\end{shaded}
\end{definizione}
\newpage
\section{Autovalori e Autovettori}

\begin{definizione}
	Sia $V$ uno spazio vettoriale su $\mathbb{R}$ e sia $f:V\to V$ un endomorfismo.\\Un vettore non nullo $\underline{v}\in V$ si dice \textbf{autovettore} di $f$ se esiste $\lambda\in\mathbb{R}$ tale che:
	$$f(\underline{v})=\lambda\cdot \underline{v}$$
	Tale $\lambda\in\mathbb{R}$ si dice \textbf{autovalore} di $f$ riferito all'autovettore $\underline{v}$. Egualmente si dice che $\underline{v}$ è l'autovettore relativo all'autovalore $\lambda$.
	\begin{center}

		\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(0,-3.475)(11.481327,3.475)
				\rput(2.3413274,-1.475){\psaxes[linecolor=black, linewidth=0.04, tickstyle=full, axesstyle=axes, labels=all, ticks=none, dx=1.0cm, dy=1.0cm]{->}(0,0)(0,0)(5,5)}
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.15]{->}(2.3413274,-1.475)(3.9413276,-0.675)(5.5413275,0.125)(7.1413274,0.925)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.15]{->}(2.3413274,-1.475)(4.7413273,-0.275)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.15]{->}(2.3413274,-1.475)(-0.058672484,-3.475)
				\rput[bl](2.7413275,3.325){z}
				\rput[bl](0.7413275,-3.475){x}
				\rput[bl](5.1413274,-0.675){X}
				\rput[bl](7.5413275,0.525){$f(X)=\lambda\cdot X$}
				\rput[bl](7.5413275,-1.875){y}
			\end{pspicture}
		}

	\end{center}
\end{definizione}
L'applicazione identica ha $1$ come solo autovalore e ogni suo vettore è autovettore, infatti $I_V(\underline{v})=\underline{v}$
\begin{definizione}
	Sia $f:V\to V$ un endomorfismo di $V$ e sia $\lambda$ un autovalore di $f$. L'insieme:
	$$E_\lambda=\{\underline{v}\in V:\, f(\underline{v})=\lambda\cdot V\}$$
	è detto \textbf{autospazio} di $V$. SI ha che $E_\lambda$ è sottospazio di $V$
\end{definizione}

\newpage
\subsection{Polinomio Caratteristico}
\begin{definizione}
	Sia $B=\{\underline{v}_1,...,\underline{v}n\}$ una base di $V$ e sia $A$ la matrice associata a $f$ per mezzo di $B$. Allora $\lambda$ è un autovalore di $f$ se esiste $\underline{v}\in V$ tale che $f(\underline{v})=\lambda\cdot \underline{v}$. Ergo si ha che:
	$$f(\underline{v})-\lambda\cdot \underline{v}=0,\, \underline{v}\neq 0$$ Perciò si ha che $f-I_V$ non deve essere invertibile, poiché non è iniettiva essendo il kernel non nullo, essendo $\underline{v}$ un suo elemento. Ne segue che la matrice associata a $f-I_V$ è:
	$$A-\lambda\cdot I$$
	e quindi $\lambda$ è un autovalore di $f$ sse:
	$$det(A-\lambda\cdot I)=0$$
	dove $det(A-\lambda\cdot I)=0$ è detto \textbf{polinomio caratteristico} della matrice A. Gli autovalori di $A$ sono la soluzione del polinomio caratteristico. Se si prende un'altra base $B^{'}$ si ha che la matrice associata a $f$ per mezzo di $B^{'}$ è una matrice $D$ simile ad $A$, ne segue che:
	$$\exists C \,\, invertibile :\, D=C^{-1}\cdot A\cdot C$$

	proseguendo per sostituzione si ha:
	$$det(D-\lambda\cdot I)=det(C^{-1}\cdot A\cdot C-\lambda\cdot I)=det(C^{-1}\cdot A\cdot C-\lambda\cdot C^{-1}\cdot C)$$
	$$=det(c^{-1}\cdot (A-\lambda\cdot I)\cdot C)=det(C^{-1})\cdot det(A-\lambda\cdot I)\cdot det(C)=det(A-\lambda\cdot I)$$
	Si ha quindi che il polinomio caratteristico dipende unicamente da $f$ e non dalla base scelta per determinare la matrice associata.
\end{definizione}
Supponiamo $V=\mathbb{R}^n$ e $B$ la base canonica. Sia $\underline{v}=(x_1,...,x_n)$ e sia $
	X=\left(\begin{matrix}
			x_1    \\
			\vdots \\
			x_n
		\end{matrix}\right)
$ allora si ha che $f(\underline{v})=\lambda\cdot \underline{v}$ può essere riscritto come:
$$A\cdot X=\lambda\cdot X$$
dove i vettori colonna della formula sono gli autovettori e $\lambda$ è l'autovalore di $A$.\\
Si nota che il polinomio caratteristico di una matrice di ordine $n$ ha grado $n$. Non tutte le applicazioni su uno spazio vettoriale hanno autovalori e autovettori (a meno di stare sul campo $\mathbb{C}$ dei numeri complessi).
\begin{teorema}
	Se $\underline{v}$ è tale che $A\cdot \underline{v}=\lambda\cdot \underline{v}$ e $\underline{w}$ è tale che $A\cdot\underline{w}=\gamma\cdot\underline{w}$, con $\lambda\neq \gamma$. Allora $\underline{v}$ e $\underline{w}$ sono indipendenti.
\end{teorema}
\begin{proof}
	Suppongo per assurdo che $$A\cdot \underline{v}=A\cdot(k\cdot \underline{w})=k\cdot A\cdot\underline{w}=k\cdot \gamma\cdot \underline{w}$$
	ovvero suppongo una dipendenza lineare. Ne segue che $\lambda\cdot \underline{v}=\lambda\cdot k\cdot \underline{w}$ che implica:
	$$(\lambda-\gamma)\cdot k\cdot\underline{w}$$
	ma questo implica che $\lambda=\gamma$, che è assurdo per ipotesi, quindi i vettori sono indipendenti
\end{proof}
\begin{teorema}
	Sia $f:V\to V$ un'applicazione lineare. Allora se $f$ ha $\lambda_1,...,\lambda_n$ come autovalori distinti si ha che i corrispondenti autovettori $\underline{v}_1,...,\underline{v}_n$ sono linearmente indipendenti.
\end{teorema}
\begin{proof}
	Bisogna provare che $\underline{v}_1,...,\underline{v}_n$ sono vettori linearmente indipendenti, ovvero che $\alpha_1\cdot\underline{v}_1+\cdots+\alpha_n\cdot \underline{v}_n=\underline{0}$ sse $\alpha_1=\cdots=\alpha_n=0$.\\
	Si procede per induzione:
	\begin{itemize}
		\item \textbf{caso base:} per $n=1$ è vero perché un vettore non nullo è linearmente indipendente
		\item \textbf{caso passo:} suppongo vero il teorema per $n-1$. Moltiplico l'applicazione lineare per $\lambda_1$, ottenendo:
		      $$\lambda_1\cdot\alpha_1\cdot\underline{v}_1+\cdots+\lambda_1\cdot\alpha_n\cdot \underline{v}_n=\underline{0}$$
		      essendo $f$ un'applicazione lineare si ha:
		      $$\alpha_1\cdot f(\underline{v}_1)+\cdots+\alpha_n\cdot f(\underline{v}_n)=\underline{0}$$
		      ovvero:
		      $$\lambda_1\cdot\alpha_1\cdot\underline{v}_1+\cdots+\lambda_n\cdot\alpha_n\cdot \underline{v}_n=\underline{0}$$
		      dove posso effettuare la sottrazione membro a membro ottenendo:
		      $$\alpha_2\cdot (\lambda_2-\lambda_1)\cdot\underline{v}_2+\cdots+\alpha_n\cdot(\lambda_n-\lambda_1)\cdot \underline{v}_n=\underline{0}$$
		      essendo per ipotesi $\underline{v}_2,...,\underline{v}_n$ linearmente indipendenti si ha che $\alpha_2\cdot (\lambda_2-\lambda_1)=\alpha_n\cdot(\lambda_n-\lambda_1)=0$. Essendo i vari $\lambda_i$ diversi tra loro ne segue che $\alpha_2,...,\alpha_n=0$ e quindi anche $\alpha_1=0$, quindi il risultato è stato dimostrato induttivamente
	\end{itemize}
\end{proof}
\textbf{Non vale l'inverso, si possono avere autovettori linearmente indipendenti  corrispondenti tutti allo stesso autovalore}
\newpage
\section{Diagonalizzazione}
\begin{definizione}
	Data una matrice quadrata $n\times n$ ad elementi reali si ha che è simile ad una matrice diagonale (una matrice con valori, anche nulli, unicamente sulla diagonale principale), detto altrimenti che è una\textbf{ matrice diagonalizzabile} se esiste una matrice $C$, a valori reali, invertibile tale che:
	$$A=C^{-1}\cdot D\cdot C$$
	con $$
		D=\left(\begin{matrix}
				\lambda_1 & 0         & \cdots & 0         \\
				0         & \lambda_2 & \cdots & 0         \\
				\vdots    & \ddots    & \ddots & \vdots    \\
				0         & 0         & \cdots & \lambda_n
			\end{matrix}\right)
	$$
	con $\lambda_1\,...,\lambda_n\in\mathbb{R}$ che ono gli autovalori di $A$
\end{definizione}
Dato che da una matrice quadrata $A$ di ordine $n$, fissata una base $B$ di $V$, si può costruire un'applicazione lineare $f:V\to V$ definita da $A$ per mezzo di $B$ si ha:
\begin{teorema}
	Una matrice $A$ è diagonalizzabile sse la corrispondente applicazione lineare $f$ ha $n$ autovettori linearmente indipendenti.
\end{teorema}
\begin{proof}
	Suppongo che $f$ abbia $n$ autovettori indipendenti $\underline{v}_1,...,\underline{v}_n$ relativi agli autovalori $\lambda_1,...,\lambda_n$ non necessariamente distinti. Si ha che $B^{'}=\{\underline{v}_1,...,\underline{v}_n\}$ è una base di $V$ e si ha:
	$$\begin{cases}
			f(\underline{v}_1)=\lambda_1\cdot \underline{v}_1 \\
			f(\underline{v}_2)=\lambda_2\cdot \underline{v}_2 \\
			\,\,\,\,\cdots\,\,\,\,\,\,\,\,\,\,\,\,\,\,\cdots  \\
			f(\underline{v}_n)=\lambda_n\cdot \underline{v}_n\
		\end{cases}$$
	Quindi la matrice associata a $f$ per mezzo di $B^{'}$ è una matrice diagonale in cui gli elementi della diagonale sono gli autovalori di $f$.\\
	Sia ora $C$ la matrice di passaggio da $B$ a $B^{'}$. Si ha che $A=C^{-1}\cdot D\cdot C$ e la prima parte del teorema è dimostrata.\\
	Viceversa, sia $A$ simile ad una matrice diagonale, ovvero $\exists C$, quadrata di ordine $n$, tale che $A=C^{-1}\cdot D\cdot C$ con $D$ diagonale. Sia $B^{'}=\{\underline{v}_1,...,\underline{v}_n\}$ la base ottenuta col cambiamento di base determinato da $C$ dalla base $B=\{\underline{u}_1,...,\underline{u}_n\}$. $D$ è la matrice associata a $f$ per mezzo di $B^{'}$ quindi:
	$$\begin{cases}
			f(\underline{v}_1)=\lambda_1\cdot \underline{v}_1 \\
			f(\underline{v}_2)=\lambda_2\cdot \underline{v}_2 \\
			\,\,\,\,\cdots\,\,\,\,\,\,\,\,\,\,\,\,\,\,\cdots  \\
			f(\underline{v}_n)=\lambda_n\cdot \underline{v}_n\
		\end{cases}$$
	quindi esistono $n$ autovettori linearmente indipendenti e il teorema è dimostrato.
\end{proof}
\textbf{Una matrice con \textit{n} autovalori reali distinti è sicuramente diagonalizzabile. Se invece si hanno autovalori coincidenti (ovvero con molteplicità maggiore di 1) occorre vedere se gli autovettori sono linearmente indipendenti. }
In aiuto si ha il seguente teorema \textit{(comodo ma non esposto a lezione}
\begin{teorema}[bonus]
	Sia $A$ una matrice quadrata di ordine $n$. Allora $A$ è diagonalizzabile sse la dimensione degli autospazi coincide con la molteplicità algebrica dei corrispondenti autovalori
\end{teorema}
\begin{teorema}
	Sia $A$ una matrice quadrata di ordine $n$ e si suppone $A$ diagonalizzabile. Allora, se $X_1,...,X_N$ sono autovettori linearmente dipendenti di $A$ e $C$ è la matrice con i vettori $X_1,...,X_N$ come colonne si ha:
	$$C^{-1}\cdot A \cdot C=D$$
	con $D$ matrice diagonale degli autovalori di $A$
\end{teorema}
Sia:
$$
	A=\left(\begin{matrix}
			0 & 2  \\
			1 & -1
		\end{matrix}\right)
$$
con autovalori $1 $ e $-2$ e sia $$
	M_d=\left(\begin{matrix}
			1 & 0  \\
			0 & -2
		\end{matrix}\right)
$$ la matrice diagonale con gli autovalori.
ho che: $M_d\cdot e_1=e_1$ e che $B\cdot M_d\cdot e_1=B\cdot e_1= prima \,\,\, colonna=v_1$. Inoltre $A\cdot v_1=1\cdot v_1$ per l'autovalore $1$ quindi $v_1$ è l'autovettore dell'autovalore $1$. Per io secondo elemento si ha  $M_d\cdot e_2=-2\cdot e_2$ e si ha $B\cdot(M_d\cdot e_2)=B\cdot (-2\cdot e_2)=-2\cdot B\cdot e_2=-2\cdot seconda\,\,\ colonna$
inoltre $B\cdot e_2=seconda\,\,\, colonna=v_2$ e quindi: $A\cdot(B\cdot e_2)=A\cdot v_2=-2\cdot v_2$. Ovviamente $e_1$ e $e_2$ sono base di $\mathbb{R}^2$, la base canonica.\\
Nel caso $n\times n$ suppongo $\lambda_1,...,\lambda_n$ autovalori e $v_1,...,v_n$ autovettori linearmente indipendenti. se chiamo $B$ kla matrice con i $v_i$ sulle colonne questa è una matrice invertibile e posso continuare come per le $2\times 2$: $A\cdot B= B\cdot M_d$ infatti $A$ e $M_d$ coincidono sui valori della base canonica. $M_d\cdot e_i=\lambda_i\cdot e_i$ poi applico $B$: $$B\cdot (M_d\cdot e_i)=B\cdot(\lambda_i\cdot e_i)=\lambda_i\cdot B\cdot e_i=\lambda_i\cdot e_i$$
inoltre $B\cdot e_i=v_i$ e $A\cdot B\cdot e_i=A\cdot v_i=\lambda:i\cdot v_1$ e infine:
$$B^{-1}\cdot A\cdot B=M_d$$
geometricamente. si ha che $B:e_1\to v_1$ e $B^{-1}:v_1\to e_1$
\begin{definizione}
	due matrici $A$ e $C$ si dicono \textbf{simili} se hanno la stessa dimensione $n\times n$ e deve esistere una matrice $B$ invertibile tale che:
	$$A=B^{-1}\cdot C \cdot B$$
	una matrice è diagonalizzabile se è simile ad una matrice diagonale.
	$A \,\,sim\,\, C$ è una relazione di equivalenza, ovvero:
	\begin{itemize}
		\item $A\,\, sim \,\, A$ ovvero $\exists B$ tale che $A=B^{-1}\cdot A\cdot B$ ed è l'identità
		\item $A\,\, sim \,\, C$ implica $C\,\, sim \,\, A$ ovvero $A=B^{-1}\cdot C\cdot B$ e $C=D^{-1}\cdot A\cdot B$ quindi $B\cdot A=C\cdot B$ e $B\cdot A\cdot B^{-1}=C$ quindi $D=B^{-1}$
		\item vale la transitività: se $A\,\, sim \,\, C$ e $C\,\, sim \,\, E$ allora $A\,\, sim \,\, E$. si ha: $A=B\cdot C\cdot B^{-1}$ e $C=D\cdot E\cdot D^{-1}$, per sostituzione si ha: $A=B^{-1}\cdot d^{-1}\cdot E\cdot D\cdot B$
	\end{itemize}
	inoltre matrici simili hanno lo stesso determinante. Inoltre matrici simili hanno gli stessi autovalori. Infatti sapendo che : $det(A-\lambda\cdot I)=0$, con $\lambda$ autovalore sse è soluzione di quell'equazione. Sia $A\,\, sim \,\, C$. Quindi $A=B\cdot C\cdot B^{-1}$ e $C=B\cdot A\cdot B^{-1}$. l'equazione caratteristica di $C$ è: $det(C-\lambda\cdot I)=0$, sostituisco: $det(B\cdot A\cdot B^{-1}-\lambda\cdot I)=0$. essendo $I=B\cdot B^{-1}$ si ha $det(B\cdot A\cdot B^{-1}-\lambda\cdot B\cdot B^{-1})=0$ e $B\cdot A\cdot B^{-1}-\lambda\cdot B\cdot B^{-1}=B\cdot (A-\lambda\cdot I)\cdot B^{-1}$ quindi: $det(B\cdot (A-\lambda\cdot I)\cdot B^{-1})=det(B)\cdot det(A-\lambda\cdot B)\cdot det(B^{-1}$ ma $det(B)\cdot det(B^{-1})=1$ ergo:
	$$det(A-\lambda\cdot I)=det(C-\lambda\cdot I)$$
\end{definizione}
\begin{teorema}
	Sia $A=A^T$, ovvero sia simmetrica, allora $A$ è diagonalizzabile
\end{teorema}





\chapter{Sistemi Sovradeterminati e Regressione Lineare}
Si ha che le incognite possono essere pensate come variabili indipendenti o come gradi di libertà del sistema.\\
Si impongono dei vincoli (per esempio per una retta sarà $y=m\cdot x+q$).\\
Per esempio può servire per trovare una retta che approssima il passaggio per, si parla della \textit{retta di regressione lineare}. \\
Si definisce: $Y=X\cdot \beta$ con $\beta=
	\left(\begin{matrix}
			q \\
			m
		\end{matrix}\right)
$ e $\beta=
	\left(\begin{matrix}
			\beta_0 \\
			\beta_1
		\end{matrix}\right)
$
quindi:
$$
	\left(\begin{matrix}
			y_1    \\
			\vdots \\
			y_n
		\end{matrix}\right)=Y=\left(\begin{matrix}
			1      & x_1    \\
			\vdots & \vdots \\
			1      & x_n
		\end{matrix}\right)\cdot\left(\begin{matrix}
			\beta_0 \\
			\beta_1
		\end{matrix}\right)
$$
Il sistema può avere o non avere soluzioni;  nel caso abbia soluzioni, il Teorema di Rouché-Capelli dà importanti informazioni sulla loro natura.  Nel caso non abbia soluzioni, fino ad ora
abbiamo evitato ogni ulteriore considerazione.  Esistono tuttavia contesti in cui la situazione
più frequente è proprio quella della non esistenza di soluzioni.
Tale sistema
sicuramente
non
avrà soluzioni se misuriamo con sufficiente precisione le distanze $y_i$.  Nonostante ciò, esso contiene molte informazioni di interesse per le scienze applicate.  In questo
testo analizzeremo superficialmente solo una di queste.
\newpage
Ho quindi la retta nel piano definita come $y=m\cdot x+q$ quindi: $$y=\beta_0+\beta_1\cdot x$$.\\
Definisco il residuo i-esimo di $(x_i, y_i)$ rispetto alla retta $l$, retta che rappresenta la regressione, (ovvero la distanza tra la retta e il punto):
$$r(l)_i=y_i-\beta_0-\beta_1\cdot x_i$$
Ora tra tutte le rette così fatte cerco quella che minimizza i vari scarti.Si ha il metodo dei minimi quadrati con la seguente sommatoria:
$$\sum_{i=1}^n [r(l)_i]^2$$
quindi:
$$f(\beta_0,\beta_1)=\sum_{i=1}^n(y_i-\beta_0-\beta_1\cdot x_i)^2$$
e trovo il minimo (non si dimostra, servono le derivate parziali) di quest'ultima quantità. Si ottengono quindi:
definisco:
$$\overline{x}=\frac{\sum_{i=1}^n x_i}{n}$$
$$\overline{y}=\frac{\sum_{i=1}^n y_i}{n}$$
e si ha:
$$\beta_1^{'}=\frac{\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}{\sum_{i=1}^n (x_i-\overline{x})^2}$$
$$\beta_0^{'}=\overline{y}-\beta_1^{'}\cdot \overline{x}$$
\newpage
\begin{esempio}
	prendo un po' di punti:
	$$(-1,0)\,(1,2)\,(2,1)$$
	%aggiungere disegno
	che ovviamente non sono allineati.\\
	calcolo il baricentro $(\overline{x},\overline{y})$
	e si ha:
	$$(\overline{x},\overline{y})=\left(\frac{2}{3},1\right)$$
	ora calcolo il coefficiente angolare $\beta_1$
	$$\beta_1^{'}=\frac{(-1-\frac{2}{3})\cdot (-1)+(1-\frac{2}{3})\cdot(2-1)+(2-\frac{2}{3})\cdot(0)}{\left(\frac{5}{3}\right)^2+\left(\frac{1}{3}\right)^2+\left(\frac{4}{3}\right)^2}=\frac{\frac{5}{3}+\frac{1}{3}}{\frac{25}{9}+\frac{1}{9}+\frac{16}{9}}=2\cdot\frac{9}{42}=\frac{3}{7}$$
	$$e$$
	$$\beta_0^{'}=\frac{5}{7}+\frac{3}{7}\cdot x$$
	Ecco il grafico dell'esercizio:
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[width= 8cm,height= 8cm,xmin=-3, xmax=3,ymin=-3, ymax=3,axis lines= middle,xtick={-3,-2,-1,0,1,2,3}, ytick={-3,-2,-1,0,1,2,3}]
				\addplot [only marks,mark=*] coordinates { (-1,0) };
				\addplot [only marks,mark=*] coordinates { (1,2) };
				\addplot [only marks,mark=*] coordinates { (2,1) };
				\addplot[black,line width=1pt, samples =100]{5/7+3/7*x};
			\end{axis}
		\end{tikzpicture}
	\end{center}
\end{esempio}
\newpage
\subsection{Altre proprietà della regressione Lineare}
Si ha che $x\cdot x^T$ genera una matrice invertibile sotto determinate ipotesi:
\begin{itemize}
	\item $x$ ha rango massimo
	\item $n\geq k+1$
\end{itemize}
Sia $y=\beta_0+\beta_1\cdot x_1+\beta_2\cdot x_2+\cdots+\beta_k\cdot x_k$.\\ Si hanno quindi $k+1$ incognite e $
	\left(\begin{matrix}
			\beta_1 \\
			\beta_2 \\
			\vdots  \\
			\beta_k
		\end{matrix}\right)=\underline{\beta}
$
Da vari esperimenti si ottengono vari $y,\cdots, y^k$ composte a loro volta da vari $x^k_n$ che ne danno le caratteristiche.\\
chiedo che: $y^1=\beta_0+\beta_1\cdot x^1_1+\cdots+\beta_k\cdot x^1_k$
fino a $y^n=\beta_0+\beta_1\cdot x^n_1+\cdots+\beta_k\cdot x^n_k$
gli indici sono il numero dell'esperimento e i pedici il numero dell'incognita. Tutto ciò si può scrivere in forma matriciale con:
$$
	y=\left(\begin{matrix}
			y^1    \\
			\vdots \\
			y^k
		\end{matrix}\right)
$$

e la matrice, $(n\,\times\, k+1)$, è:
$$
	\left(\begin{matrix}
			1      & x_1^1  & \cdots & x_k^1  \\
			1      & x_1^2  & \cdots & x_k^2  \\
			\vdots & \ddots & \ddots & \vdots \\
			1      & x_1^n  & \cdots & x_k^n
		\end{matrix}\right)
$$
Quindi $x^T$ è del tipo $k+1\,\times\, n$ e quindi $x\cdot x^T$ è del tipo $k+1,\times\, k+1$ ed è invertibile.
Si ha: $$x^T\cdot y = x\cdot x^T\cdot \beta$$
e tutto ciò ha senso a livello dimensionale, entrambi lati hanno dimensione $k+1$. Sia $c=x\cdot x^T$, posso scrivere: $$c^{-1}\cdot x^T\cdot y=c^{-1}\cdot c\cdot \beta$$ e posso dire che: $$c^{-1}\cdot x^t\cdot y=\beta$$
\textbf{Guardare anche dispense sulla Regressione Lineare, fruibili su Moodle}
\chapter{Geometria Analitica}
\section{Coordinate}
Considerando il classico piano euclideo fissiamo 2 assi, con due rette ortogonali $x$ (\textit{ascissa}) e $y$ (\textit{ordinata}), dove prendiamo dei segmenti unitari, che si suppone uguale per i due assi, e sia $O$ il punto della loro intersezione, detta \textit{origine degli assi}. Abbiamo appena fissato un \textbf{sistema di coordinate cartesiane ortogonali (\textit{O,x,y})}.\\
Sia ora dato un punto $P$ del piano e siano $P_1$ e $P_2$ le sue proiezioni ortogonali sul piano rispettivamente sull'asse $x$ e sull'asse $y$. Sia $x$ la misura di $OP_1$ e $y$ quella di $OP_2$
\begin{center}

	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-2.4828205)(4.965641,2.4828205)
			\rput(0.09855774,-2.3842628){\psaxes[linecolor=black, linewidth=0.04, tickstyle=full, axesstyle=axes, labels=none, ticks=none, dx=1.0cm, dy=1.0cm, Dx=4, Dy=4]{->}(0,0)(0,0)(5,5)}
			\psdots[linecolor=black, dotsize=0.2](3.6985579,-2.3842628)
			\psdots[linecolor=black, dotsize=0.2](0.09855774,1.2157372)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](0.09855774,1.2157372)(3.6985579,1.2157372)(3.6985579,1.2157372)(3.6985579,-2.3842628)(3.6985579,-2.3842628)
			\psdots[linecolor=black, dotsize=0.2](3.6985579,1.2157372)
			\rput[bl](3.7,-2.9335416){$P_1$}
			\rput[bl](3.8,1.264583){$P$}
			\rput[bl](-0.4,1.3664583){$P_2$}
			\rput[bl](-0.4,-2.6335417){$O$}
			\rput[bl](-0.4,2.095){$y$}
			\rput[bl](4.6,-2.705){$x$}
		\end{pspicture}
	}
\end{center}
Si ha una corrispondenza biunivoca tra i punti $P$ del piano euclideo e le coppie ordinate di numeri reali $(x,y)$ e si indica con $P=(x,y)$ o $P\longleftrightarrow (x,y)$. Questa corrispondenza biunivoca identifica i punti del piano con l'insieme $\mathbb{R}^2$ delle coppie ordinate di reali. A ogni coppia di reali corrisponde un punto del piano.
\newpage
Questo discorso si può fare anche nello spazio tridimensionale, ottenendo un sistema di coordinate cartesiane ortogonali $(O,x,y,z)$, dove l'asse $z$ è detto \textit{quota}, ottenendo:
\begin{center}

	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-3.275)(8.56,3.275)
			\rput(3.6,-1.675){\psaxes[linecolor=black, linewidth=0.04, tickstyle=full, axesstyle=axes, labels=none, ticks=none, dx=1.0cm, dy=1.0cm, Dx=4, Dy=4]{->}(0,0)(0,0)(5,5)}
			\psdots[linecolor=black, dotsize=0.2](7.2,-1.675)
			\psdots[linecolor=black, dotsize=0.2](3.6,1.925)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](3.6,1.925)(7.2,1.925)(7.2,1.925)(7.2,-1.675)(7.2,-1.675)
			\rput[bl](7.2,-2.175){$P_2$}
			\rput[bl](3.2,-1.675){$O$}
			\rput[bl](3.2,3.125){$z$}
			\rput[bl](8.4,-2.075){$y$}
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.15]{->}(3.6,-1.675)(0.0,-3.275)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](3.6,1.925)(0.8,1.125)(0.8,1.125)(0.8,-2.875)(5.2,-2.875)(7.2,-1.675)(7.2,-1.675)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](5.2,-2.875)(5.2,1.125)(5.2,1.125)(5.2,1.125)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](5.2,1.125)(7.2,1.925)(7.2,1.925)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](0.8,1.125)(5.2,1.125)(5.2,1.125)
			\psdots[linecolor=black, dotsize=0.2](5.2,1.125)
			\rput[bl](5.4,0.725){$P$}
			\rput[bl](0.8,-3.33){$P_1$}
			\rput[bl](3.1,2.125){$P_3$}
			\rput[bl](0.0,-2.975){$x$}
			\psdots[linecolor=black, dotsize=0.2](0.8,-2.875)
		\end{pspicture}
	}

\end{center}
Si ha quindi corrispondenza biunivoca tra i punti $P$ e l'insieme delle terne di numeri reali, ovvero $\mathbb{R}^3$. I tre piani individuati  dalle coppie di rette $x$ e $y$, $y$ e $z$ e $x$ e $z$ sono detti \textit{piani coordinati}, \textit{piano xy (x,y,0), piano yz (0,y,z) e piano xz (x,0,z)}. Inoltre i punti, per esempio su $x$ avranno coordinate $(x,0,0)$ e coì via per $y$ e $z$.
\section{vettori}
Si chiama vettore applicato, e si indica con $\vec{AB}$ una coppia ordinata di punti $A$ e $B$ del piano o dello spazio. $A$ è detto \textbf{punto di applicazione}  e $B$ \textbf{punto finale}:
\begin{center}
	\psscalebox{0.8 0.8} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-3.2652442)(8.56,3.2652442)
			\rput(3.6,-1.6847558){\psaxes[linecolor=black, linewidth=0.04, tickstyle=full, axesstyle=axes, labels=none, ticks=none, dx=1.0cm, dy=1.0cm, Dx=4, Dy=4]{->}(0,0)(0,0)(5,5)}
			\rput[bl](3.2,-1.6847558){$O$}
			\rput[bl](3.2,3.1152442){$z$}
			\rput[bl](8.4,-2.084756){$y$}
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.15]{->}(3.6,-1.6847558)(0.0,-3.284756)
			\rput[bl](0.0,-2.975){$x$}
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.15]{->}(4.8,-0.48475587)(7.6,0.7152441)
			\rput[bl](4.3,-0.88475585){$A$}
			\rput[bl](7.6,0.7152441){$B$}
			\rput[bl](6.0,-0.58475586){$\vec{AB}$}
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](7.6,0.6959529)(7.6,-1.7040471)(7.6,-1.7040471)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](7.6,0.6959529)(3.6,0.6959529)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](4.8,-0.50404716)(4.8,-1.7040471)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](4.8,-0.50404716)(3.6,-0.50404716)
			\rput[bl](7.6,-2.104047){$b_1$}
			\rput[bl](4.8,-2.104047){$a_1$}
			\rput[bl](3,-0.50404716){$a_2$}
			\rput[bl](3,0.6959529){$b_2$}
		\end{pspicture}
	}
\end{center}
Quindi un vettore è un segmento orientato da $A$ a $B$.
\newpage
Il verso è detto \textbf{verso del vettore} e la direzione individuata dalla retta è detta \textbf{direzione del vettore} e la misura è detta \textbf{lunghezza o norma o modulo del vettore}, definita come $\sqrt{(b_1-a_1)^2+(b_2-a_2)^2}$. Direzione e verso si definiscono solo per $A\neq B$.\\
\begin{definizione}
	Due vettori $\vec{AB}$ e $\vec{CD}$ sono \textit{equivalenti} o \textit{equipollenti} se $A,B,C,D$ sono i vertici di un parallelogramma, ovvero se uno può essere ottenuto dall'altro mediante una traslazione degli assi, quindi i due vettori devono avere stesso verso, direzione e lunghezza:
	\begin{center}
		\psscalebox{0.8 0.8} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(0,-3.2652442)(8.56,3.2652442)
				\rput(3.6,-1.6847558){\psaxes[linecolor=black, linewidth=0.04, tickstyle=full, axesstyle=axes, labels=none, ticks=none, dx=1.0cm, dy=1.0cm, Dx=4, Dy=4]{->}(0,0)(0,0)(5,5)}
				\rput[bl](3.2,-1.6847558){$O$}
				\rput[bl](3.2,3.1152442){$z$}
				\rput[bl](8.4,-2.084756){$y$}
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.15]{->}(3.6,-1.6847558)(0.0,-3.284756)
				\rput[bl](0.0,-2.975){$x$}
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.15]{->}(4.8,-0.48475587)(7.6,0.7152441)
				\rput[bl](4.6,-0.88475585){$A$}
				\rput[bl](7.6,0.4152441){$B$}
				\rput[bl](6.0,-0.58475586){$\vec{AB}$}
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.15]{->}(4.8,0.7152441)(7.6,1.9152441)
				\rput[bl](5.6,1.6152441){$\vec{CD}$}
				\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](7.6,1.9152441)(7.6,0.7152441)(7.6,0.7152441)
				\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](4.8,0.7152441)(4.8,-0.48475587)(4.8,-0.48475587)
				\rput[bl](4.4,0.8152441){$C$}
				\rput[bl](7.6,2.0152442){$D$}
			\end{pspicture}
		}
	\end{center}

	Inoltre ogni vettori ha un vettore equivalente applicato all'origine.
\end{definizione}
\begin{shaded}
	\begin{definizione}
		Si definiscono due operazioni:
		\begin{itemize}
			\item si definisce così l'addizione tra punti in uno spazio n-dimensionale:\\
			      siano $A=(a_1,...,a_n)$ e $B=(b_1,...,b_n)$ si ha:
			      $$A+B=(a_1+b_1,...,a_n+b_n)$$
			\item si definisce così la moltiplicazione tra un punto in uno spazio n-dimensionale e uno scalare:\\
			      siano $A=(a_1,...,a_n)$ e $k\in\mathbb{R}$ si ha:
			      $$k\cdot A=(k\cdot a_1,...,k\cdot a_n)$$
		\end{itemize}
	\end{definizione}
\end{shaded}
\newpage
\begin{teorema}
	\textit{Si enuncia questo teorema nel piano ma resta valido anche in dimensione n}\\
	Siano $A=(a_1,a_2)$, $B=(b_1,b_2)$, $C=(c_1,c_2)$ e $D=(d_1,d_2)$ quattro punti del piano. Allora $\vec{AB}$ è equivalente a $\vec{CD}$ sse $$B-A=D-C$$
\end{teorema}
\begin{proof}
	Si ha che:
	$$\begin{cases}
			b_1-a_1=d_1-c_1 \\
			b_2-a_2=d_2-c_2
		\end{cases}$$
	Si portano da $A$ e $B$ le parallele ad $x$ e $y$, con $H$ il punto di intersezione. Analogamente si fa con $C$ e $D$, con $K$ intersezione delle parallele. Si ottiene:
	$$\begin{cases}
			AH=b_1-a_1 \\
			BH=b_2-a_2 \\
			CK=d_1-c_1 \\
			DK=d_2-c_2
		\end{cases}$$
	e per ipotesi si ha che:
	$$\begin{cases}
			AH=CK=b_1-a_1=d_1-c_1 \\
			BH=DL=b_2-a_2=d_2-c_2
		\end{cases}$$
	quindi i triangoli che si vengono a formare, $A\overset{\triangle}{H}B$ e $C\overset{\triangle}{K}D$ sono uguali. Quindi $\vec{AB}$ e $\vec{CD}$ hanno lo stesso verso, la stessa direzione e la stessa lunghezza
	\begin{center}

		\psscalebox{1.5 1.5} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(0,-2.705)(5.78,1.705)
				\rput(0.4,-2.305){\psaxes[linecolor=black, linewidth=0.04, tickstyle=full, axesstyle=axes, labels=none, ticks=none, dx=1.0cm, dy=1.0cm, Dx=4, Dy=4]{->}(0,0)(0,0)(5,3)}
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2,-1.505)(2.8,-0.705)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.2,-1.105)(4.8,-0.305)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](2.8,-0.705)(2.8,-2.305)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](4.8,-0.305)(4.8,-2.305)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](1.2,-1.505)(1.2,-2.305)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](1.2,-1.505)(0.4,-1.505)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](2.8,-0.705)(0.4,-0.705)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](4.8,-0.305)(0.4,-0.305)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](3.2,-1.105)(3.2,-2.305)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](3.2,-1.105)(0.4,-1.105)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](3.2,-1.105)(4.8,-1.105)(4.8,-1.105)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](1.2,-1.505)(2.8,-1.505)(2.8,-1.505)
				\psdots[linecolor=black, dotsize=0.12](4.8,-1.105)
				\psdots[linecolor=black, dotsize=0.12](2.8,-1.505)
				\rput[bl](4.9,-1.105){$K$}
				\rput[bl](2.4,-1.905){$H$}
				\rput[bl](4.8,-0.305){$D$}
				\rput[bl](3.2,-1.505){$C$}
				\rput[bl](2.8,-0.705){$B$}
				\rput[bl](0.8,-1.505){$A$}
				\rput[bl](0.0,-2.305){$O$}
				\rput[bl](1.2,-2.705){$a_1$}
				\rput[bl](2.8,-2.745){$b_1$}
				\rput[bl](3.2,-2.705){$c_1$}
				\rput[bl](4.8,-2.745){$d_1$}
				\rput[bl](-0.05,-1.505){$a_2$}
				\rput[bl](0.0,-1.105){$c_2$}
				\rput[bl](0.0,-0.705){$b_2$}
				\rput[bl](-0.05,-0.315){$d_2$}
				\rput[bl](5.3,-2.705){$x$}
				\rput[bl](0.0,0.55){$y$}
			\end{pspicture}
		}
	\end{center}
\end{proof}
\newpage
Ogni vettore $\vec{AB}$ ha un vettore equivalente $\vec{OP}$ applicato all'origine. Grazie al teorema appena enunciato possiamo dire che:
$$P=A-B$$
per comodità di notazione una terna di numeri reali può indicare sia un punto nello spazio che il vettore $\vec{OP}$ applicato nell'origine.
\begin{definizione}
	Definiamo 3 vettori particolari in $\mathbb{R}^3$
	$$\underline{i}=(1,0,0)$$
	$$\underline{j}=(0,1,0)$$
	$$\underline{k}=(0,0,1)$$
	essi vengono detto \textbf{versori degli assi}. Si ha che:
	$$\underline{v}=x\cdot (1,0,0)+y\cdot (0,1,0)+z\cdot (0,0,1)=x\cdot \underline{i}+y\cdot \underline{j}+z\cdot \underline{k}$$
	Ovvero sono vettori di norma 1:
	\begin{center}
		\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(0,-3.075)(6.108351,3.075)
				\rput(1.9483514,-1.075){\psaxes[linecolor=black, linewidth=0.04, tickstyle=full, axesstyle=axes, labels=none, ticks=none, dx=1.0cm, dy=1.0cm]{->}(0,0)(0,0)(4,4)}
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.9483514,-1.075)(-0.05164856,-3.075)
				\psline[linecolor=black, linewidth=0.06, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.9483514,-1.075)(1.9483514,0.125)
				\psline[linecolor=black, linewidth=0.06, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.9483514,-1.075)(3.1483514,-1.075)
				\psline[linecolor=black, linewidth=0.06, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.9483514,-1.075)(1.1483514,-1.875)
				\rput[bl](1.5483514,2.925){$z$}
				\rput[bl](0.34835145,-3.075){$x$}
				\rput[bl](5.9483514,-1.475){$y$}
				\rput[bl](2.7483513,-1.575){$\underline{i}$}
				\rput[bl](1.5483514,-1.975){$\underline{j}$}
				\rput[bl](2.1483515,-0.275){$\underline{k}$}
			\end{pspicture}
		}
	\end{center}
\end{definizione}
\newpage
\begin{nota}[Significato geometrico della somma tra punti]
	Sappiamo che due vettori $\vec{AB}$ e $\vec{CD}$ sono equivalenti se hanno la stessa direzione, verso e lunghezza. Inoltre i 4 punti che li compongono formano i vertici di un parallelogramma, quindi $B-A=D-C$.
	Se prendiamo il punto $A$ coincidente con l'origine, si ha $B=D-C$, quindi si ha $D=B+C$ e $\vec{OD}$ forma la diagonale del parallelogramma $OCDB$. Quindi se considero i punti $B$ e $C$ e ne sommo le coordinate ottengo il punto $D$.\\
	Se invece considero $B$ e $C$ come vettori applicati all'origine e li sommiamo con la regola del parallelogramma otteniamo il vettore $D$ applicato all'origine
	\begin{center}

		\psscalebox{0.9 0.9} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(0,-3.075)(6.108351,3.075)
				\rput(1.9483514,-1.075){\psaxes[linecolor=black, linewidth=0.04, tickstyle=full, axesstyle=axes, labels=none, ticks=none, dx=1.0cm, dy=1.0cm]{->}(0,0)(0,0)(4,4)}
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.9483514,-1.075)(-0.05164856,-3.075)
				\rput[bl](1.5483514,2.925){$z$}
				\rput[bl](0.34835145,-3.075){$x$}
				\rput[bl](5.9483514,-1.475){$y$}
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.9483514,-1.075)(5.1483517,1.725)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.9483514,-1.075)(4.7483516,-0.675)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.9483514,-1.075)(2.3483515,1.325)
				\rput[bl](5.1483517,1.795){$D$}
				\rput[bl](2.3483515,1.525){$B$}
				\rput[bl](4.9183517,-0.675){$C$}
				\rput[bl](3.5483515,0.025){$\vec{OD}$}
				\rput[bl](1.9083514,-1.475){$O$}
				\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](2.3483515,1.325)(5.1483517,1.725)(4.7483516,-0.675)(4.7483516,-0.675)(4.7483516,-0.675)(4.7483516,-0.675)
			\end{pspicture}
		}

	\end{center}
\end{nota}
\begin{nota}[Significato geometrico del prodotto di un numero reale per un vettore]
	Sia $A=(x,y,z)$ e sia $k\in \mathbb{R}$. Considero $A$ come un vettore non nullo applicato all'origine. Si ha che il vettore $k\cdot A$ ha la stessa direzione di $A$, lo stesso verso se $k>0$, il verso opposto se $k<0$, e la lunghezza è quella di $A$ moltiplicata per $|k|$
	\begin{center}

		\psscalebox{0.9 0.9} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(0,-3.0788372)(9.33,3.0788372)
				\rput(3.630237,-1.0937053){\rput{-0.32284054}(-0.011237453,0.01130095){\psaxes[linecolor=black, linewidth=0.04, tickstyle=full, axesstyle=axes, labels=none, ticks=none, dx=1.0cm, dy=1.0cm]{->}(0,0)(0,0)(4,4)}}
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.6189742,-1.0823789)(1.6032,-3.075564)
				\rput{-0.32284054}(-0.016451398,0.018306362){\rput[bl](3.2406676,2.9288394){$z$}}
				\rput{-0.32284054}(0.017374117,0.011243422){\rput[bl](2.004096,-3.077823){$x$}}
				\rput{-0.32284054}(0.008605996,0.042943727){\rput[bl](7.6256747,-1.505864){$y$}}
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.6,-1.0657605)(7.2,-0.26576048)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.6,-1.0657605)(0.0,-1.8657604)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.6,-1.0657605)(5.6,-0.66576046)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.6,-1.0657605)(1.6,-1.4657605)
				\rput[bl](5.2,-0.40576048){$A$}
				\rput[bl](1.6,-1.1857605){$-A$}
				\rput[bl](7.6,-0.26576048){$k\cdot A,\, k>0$}
				\rput[bl](3.6,-1.4657605){$O$}
				\rput[bl](3.6,-1.4657605){$O$}
				\rput[bl](0.0,-2.2957604){$k\cdot A,\,k<0$}
			\end{pspicture}
		}

	\end{center}
	Ovviamente se $k=0$ si otterrà il vettore nullo, ovvero un punto, che sarà esattamente l'origine.
\end{nota}
\begin{definizione}
	I vettori $A$ e $k\cdot A$ sono detti \textbf{vettori paralleli}, se si assume che un vettore nullo sia parallelo a tutti i vettori si ha che questa definizione vale $\forall k\in\mathbb{R}$.
	Quindi due vettori $\vec{AB}$ e $\vec{CD}$ sono paralleli sse lo sono $B-A$ e $D-C$ ovvero se $\exists k\in \mathbb{R}$ tale che:
	$$B-A=k\cdot (D-C)\,\, oppure \,\, k\cdot (B-A)=D-C$$
\end{definizione}
\begin{definizione}
	Si dice che un vettore $A$ è \textbf{complanare} ai vettori $B$ e $C$ sse $\exists \beta,\,\gamma$ tali che:
	$$A=\beta\cdot B+\gamma\cdot C$$
\end{definizione}
\subsection{Prodotto scalare}
\begin{definizione}
	Siano $A=(a_1,a_2,a_3)$ e $B=(b_1,b_2,b_3)$ due vettori nello spazio. Si definisce il loro \textbf{prodotto scalare}, indicato con $A\cdot B$ il numero reale dato da:
	$$A\cdot B=a_1\cdot b_1+a_2\cdot b_2+a_3\cdot b_3$$
\end{definizione}
Il prodotto scalare gode delle seguenti proprietà:
\begin{itemize}
	\item $A\cdot B=B\cdot A$
	\item $A\cdot (B+C)=A\cdot b+A\cdot C$
	\item $(	\alpha\cdot A)\cdot B=\alpha\cdot (A\cdot B),\, \alpha\in\mathbb{R}$
	\item $A\cdot A\geq 0,\,\forall A\in\mathbb{R}^3$
	\item $A\cdot A= 0,\,A=\underline{0}=(0,0,0)$

\end{itemize}

\begin{definizione}
	Si definisce \textbf{norma} del vettore $A$ il numero reale:
	$$\norm{A}=\sqrt{A\cdot A}$$
	Quindi, se per esempio $A=(a_1,a_2,a_3)$, si avrà:
	$$\norm{A}=\sqrt{a_1^2+a_2^2+a_3^2}$$
	Ovviamente vale per $\forall n$.
	Quindi la norma di un vettore è la sua lunghezza.\\
	Inoltre si ha che:
	$$\norm{B-A}=\sqrt{(b_1-a_1)^2+(b_2-a_2)^2+(b_3-a_3)^2}$$
	che è la distanza tra i punti $A$ e $B$ nello spazio, ovvero la lunghezza di $\vec{AB}$. Infine si ha che:
	$$\norm{k\cdot A}=|k|\cdot \norm{A},\, \forall k \in \mathbb{R}$$
\end{definizione}
\newpage
\begin{nota}
	Grazie alla definizione di norma possiamo dare una nuoca definizione di \textbf{versore} (in quanto è un vettore di norma 1), con la stessa direzione e verso di $A$:
	$$vers(A)=\frac{A}{\norm{A}}$$
\end{nota}
Si definiscono altre 2 proprietà:
\begin{itemize}
	\item $(A+B)^2=A^2+2\cdot A\cdot B+B^2$
	\item $(A-B)^2=A^2-2\cdot A\cdot B+B^2$
\end{itemize}
\textit{Con i seguenti teoremi si cercherà di comprendere il significato geometrico del prodotto scalare}
\begin{teorema}
	Due vettori non nulli $A$ e $B$ sono \textbf{perpendicolari (o ortogonali)} sse:
	$$A\cdot B=0$$
\end{teorema}
\begin{proof}
	Si parte dal seguente grafico, con due vettori $A$ e $B$ non nulli e ortogonali per la dimostrazione:
	\begin{center}

		\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(0,-2.13)(8.15,2.13)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.6,-1.73)(6.8,-1.73)(6.8,-1.73)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.6,-1.73)(0.4,-1.73)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.6,-1.73)(3.6,1.47)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.6,-1.73)(0.4,1.47)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.6,-1.73)(6.8,1.47)
				\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm](0.4,-1.73)(0.4,1.47)(6.8,1.47)(6.8,1.47)(6.8,-1.73)
				\rput[bl](3.6,-2.13){$O$}
				\rput[bl](0.4,-2.13){$-A$}
				\rput[bl](0.95,-1.13){$\norm{B-A}$}
				\rput[bl](4.7,-1.13){$\norm{A+B}$}
				\rput[bl](6.6,-2.18){$A$}
				\rput[bl](3.4,1.67){$B$}
				\rput[bl](6.2,1.67){$A+B$}
				\rput[bl](0.0,1.67){$B-A$}
			\end{pspicture}
		}
	\end{center}
	dal grafico si vede che vale vale:
	$$\norm{A+B}=\norm{B-A}$$
	quindi si ha che:
	$$\norm{A+B}^2=\norm{B-A}^2\longrightarrow (A+B)^2=(B-A)^2$$
	ovvero:
	$$A^2+2\cdot A\cdot B+B^2=A^2-2\cdot A\cdot B+B^2 \longrightarrow 4\cdot A\cdot B=0$$
	e quindi si ha:
	$$A\cdot B=0$$
	Quindi la tesi è dimostrata
\end{proof}
\newpage
\begin{definizione}
	Si definisce \textbf{angolo tra due vettori} $A$ e $B$, indicato con $\theta$ l'angolo convesso individuato tra $A$ e $B$:
	\begin{center}

		\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(2.5,-1.13)(24.15,-3.13)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(7.6,-2.3958476)(10.0,-0.7958475)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(7.6,-2.3958476)(10.8,-2.7958474)
				\psarc[linecolor=black, linewidth=0.04, dimen=outer](7.6,-2.3958476){0.04}{0.0}{50.0}
				\psarc[linecolor=black, linewidth=0.04, dimen=outer](7.6,-2.3958476){0.04}{0.0}{50.0}
				\pscustom[linecolor=black, linewidth=0.04]
				{
					\newpath
					\moveto(8.4,-1.5958475)
				}
				\pscustom[linecolor=black, linewidth=0.04]
				{
					\newpath
					\moveto(8.8,-2.3958476)
				}
				\pscustom[linecolor=black, linewidth=0.04]
				{
					\newpath
					\moveto(2.4,0.004152527)
				}

				\pscustom[linecolor=black, linewidth=0.04]
				{
					\newpath
					\moveto(8.8,-1.5958475)
				}
				\pscustom[linecolor=black, linewidth=0.04]
				{
					\newpath
					\moveto(9.2,-2.3958476)
				}
				\psarc[linecolor=black, linewidth=0.04, dimen=outer](7.6,-2.3958476){0.04}{0.0}{50.0}


				\psarc[linecolor=black, linewidth=0.04, dimen=outer](8.0,-2.5958474){1.0}{0.0}{56.0}


				\rput[bl](8.9,-1.9958475){$\theta$}
				\rput[bl](7.2,-2.3958476){$O$}
				\rput[bl](9.9,-1.2958475){$A$}
				\rput[bl](10.8,-2.7958474){$B$}
			\end{pspicture}
		}

	\end{center}
\end{definizione}
\begin{teorema}
	Siano $A$ e $B$ due vettori non nulli e sia $\theta$ l'angolo tra $A$ e $B$. Allora:
	$$\cos\theta=\frac{A\cdot B}{\norm{A}\cdot \norm{B}}$$
	Inoltre dato che $-1\leq \cos\theta\leq 1$ si ha che:
	$$|A\cdot B|\leq |\norm{A}\cdot \norm{B}|$$
	questa diseguaglianza è detta \textbf{diseguaglianza di Cauchy}
\end{teorema}
\subsection{Prodotto Vettoriale}
\begin{definizione}
	Siano dati due vettori dello spazio:
	$$\underline{v}=(a,b,c)=a\cdot \underline{i}+b\cdot \underline{j}+c\cdot \underline{k}$$
	$$\underline{w}=(a_1,b_1,c_1)=a_1\cdot \underline{i}+b_1\cdot \underline{j}+c_1\cdot \underline{k}$$
	Si definisce \textbf{prodotto vettoriale} tra $\underline{v}$ e $\underline{w}$, e si indica con $\underline{v}\land\underline{w}$ il vettore:
	$$\underline{v}\land\underline{w}=(b\cdot c_1-b_1\cdot c)\cdot \underline{i}+(a_1\cdot c-a\cdot c_1)\cdot\underline{j}+(a\cdot b_1-a_1\cdot b)\cdot \underline{k}$$
	o anche:
	$$\underline{v}\land\underline{w}=(b\cdot c_1-b_1\cdot c,\,a_1\cdot c-a\cdot c_1,\,a\cdot b_1-a_1\cdot b)$$
\end{definizione}
\begin{nota}
	Per calcolare il prodotto vettoriale si può usare il seguente procedimento:\\
	Prendo le componenti dei vettori e li metto in matrice e alla prima riga metto i versori e ne calcolo il determinante
	\begin{esempio}
		Siano $\underline{v}=2\cdot \underline{i}+3\cdot \underline{j}+ \underline{k}$ e $\underline{w}=4\cdot \underline{i}+ \underline{j}-2\cdot \underline{k}$. Si avrà:
		$$
			\underline{v}\land\underline{w}=det\left(\begin{matrix}
					\underline{i} & \underline{j} & \underline{k} \\
					2             & 3             & 1             \\
					4             & 1             & -2
				\end{matrix}\right)
		$$
		$$=\underline{i}\cdot (-6-1)-\underline{j}\cdot (-4-4)+\underline{k}\cdot(2-12)=-7\cdot \underline{i}+8\cdot \underline{j}-10\cdot \underline{k}$$
	\end{esempio}
\end{nota}
\newpage
Ecco alcune proprietà del prodotto vettoriale:
\begin{itemize}
	\item $\underline{v}\land\underline{w}=-\underline{w}\land\underline{v}$
	\item $\underline{v}\land(\underline{w}+\underline{u})=\underline{v}\land \underline{w}+\underline{v}\land \underline{u}$
	\item $(h\cdot \underline{v})\land \underline{w}=h\cdot (\underline{v}\land \underline{w}),\, h\in\mathbb{R}$
	\item $\underline{v}\cdot (\underline{v}\land \underline{w})=0\,\, e\,\,\underline{w}\cdot (\underline{w}\land \underline{v})$ ovvero si ha che $\underline{v}\land\underline{w}$ è perpendicolare ad entrambi i vettori
	\item $(\alpha\cdot \underline{v})\land \underline{v}=\underline{0},\,\alpha\in\mathbb{R}$ ovvero il prodotto di due vettori paralleli è nullo
	\item se $\underline{v}\land \underline{w}=\underline{0}$ allora $\exists\, \alpha\in\mathbb{R}$ tale che $\underline{w}=\alpha\cdot \underline{v}$ oppure $\underline{v}=\alpha\cdot \underline{w}$ ovvero se due vettori hanno il prodotto vettoriale nullo allora sono paralleli
\end{itemize}
Inoltre i tre vettori $\underline{v}$, $\underline{w}$ e $\underline{v}\land \underline{w}$ formano una terna destrorsa, ovvero rappresentabile con la \textit{regola della mano destra}, dove il pollice rappresenta un vettore, l'indice un altro e il prodotto tra i due sarà il vettore uscente dal palmo.
\begin{center}

	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-2.153479)(3.926958,2.153479)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.02,-1.7534791)(0.02,2.2465208)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.02,-1.7534791)(4.02,-1.7534791)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.02,-1.7534791)(3.62,-0.55347914)
			\rput[bl](3.62,-2.153479){$\underline{v}$}
			\rput[bl](3.62,-0.55347914){$\underline{w}$}
			\rput[bl](0.22,1.8465209){$\underline{v}\land\underline{w}$}
			\rput[bl](-0.12,-2.153479){$O$}
		\end{pspicture}
	}

\end{center}
\begin{nota}
	Si cerca ora il modo di calcolare il modulo del prodotto vettoriale.
	Sappiamo che:
	$$\norm{\underline{v}\land\underline{w}}^2=(b\cdot c_1-b_1\cdot c)^2+(a_1\cdot c-a\cdot c_1)^2+(a\cdot b_1-a_1\cdot b)^2$$
	$$=(a^2+b^2+c^2)\cdot(a_1^2+b_1^2+c_1^2)-(a\cdot a_1+b\cdot b_1+c\cdot c_1)=\norm{\underline{v}}^2\cdot\norm{\underline{w}}^2-(\underline{v}\cdot \underline{w})^2$$
	\textbf{quindi si ha anche la seguente proprietà:}
	$$\norm{\underline{v}\land\underline{w}}^2=\norm{\underline{v}}^2\cdot\norm{\underline{w}}^2-(\underline{v}\cdot \underline{w})^2$$
	Ma quindi, ricordando il prodotto scalare tra vettori si ha:
	$$\norm{\underline{v}\land\underline{w}}^2=\norm{\underline{v}}^2\cdot\norm{\underline{w}}^2-(\underline{v}\cdot \underline{w})^2=\norm{\underline{v}}^2\cdot \norm{\underline{w}}^2-\norm{\underline{v}}^2\cdot \norm{\underline{w}}^2\cdot \cos ^2\theta$$
	$$=\norm{\underline{v}}^2\cdot \norm{\underline{w}}^2\cdot(1-\cos ^2\theta)=\norm{\underline{v}}^2\cdot \norm{\underline{w}}^2\cdot \sin ^2\theta$$
	\textbf{Quindi si ha che:}
	$$\norm{\underline{v}\land \underline{w}}=\norm{\underline{v}}\cdot \norm{\underline{w}}\cdot \sin \theta$$
	\newpage
	inoltre si ha $\sin \theta\geq 0$, essendo $\theta$ un angolo convesso
	e poiché:
	$$\norm{\underline{w}}\cdot\sin\theta=BH$$
	si ha:
	$$\norm{\underline{v}\land \underline{w}}=\norm{\underline{v}}\cdot \norm{\underline{w}}\cdot \sin \theta=OA\cdot AH$$
	quindi il prodotto vettoriale non è altro che l'area del parallelogramma $OACB$ individuato da $\underline{v}$ e $\underline{w}$
	\begin{center}

		\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(0,-1.5727397)(7.05,1.5727397)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.4,-1.0672604)(4.8,-1.0672604)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.4,-1.0672604)(2.0,0.9327397)
				\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](4.8,-1.0672604)(6.4,0.9327397)(2.0,0.9327397)(2.0,-1.0672604)
				\psarc[linecolor=black, linewidth=0.04, dimen=outer](0.4,-1.0672604){0.8}{0.0}{51.0}
				\rput[bl](6.6,0.9327397){$C$}
				\rput[bl](2.0,1.1127397){$B$}
				\rput[bl](1.5,0.9327397){$\underline{w}$}
				\rput[bl](0.0,-1.0672604){$O$}
				\rput[bl](0.65,0.13273966){$\norm{\underline{w}}$}
				\rput[bl](2.7,-1.5272604){$\norm{\underline{v}}$}
				\rput[bl](4.4,-1.4672604){$\underline{v}$}
				\rput[bl](4.9,-1.4072604){$A$}
				\rput[bl](1.2,-0.66726035){$\theta$}
				\rput[bl](1.85,-1.4672604){$H$}
			\end{pspicture}
		}

	\end{center}
\end{nota}
\subsection{Dipendenza e indipendenza lineare}
\textit{In questa sezione si analizza il significato geometrico della dipendenza lineare}\\
Innanzitutto si ricorda che due vettori $\underline{v}$ e $\underline{w}$ sono dipendenti se $\exists \,k\in\mathbb{R}$ tale che $\underline{v}=k\cdot \underline{w}$ o $\underline{w}=k\cdot \underline{v}$. Inoltre se si ha $\lambda_1\cdot \underline{v}+\lambda_2\cdot \underline{w}$ con $\lambda_1,\,\lambda_2$ non entrambi nulli accade che:
\begin{itemize}
	\item se $\lambda_1\neq 0$ si ha: $\underline{v}=\frac{-\lambda_2}{\lambda_1}\cdot \underline{w}$
	\item se $\lambda_2\neq 0$ si ha: $\underline{w}=\frac{-\lambda_1}{\lambda_2}\cdot \underline{v}$
\end{itemize}
quindi 2 vettori nel piano (o nello spazio etc...) sono linearmente dipendenti sse sono paralleli.
Inoltre 2 vettori del piano $\underline{v}_1=a_1\cdot \underline{i}+b_1\cdot \underline{j}$ e $\underline{v}_2=a_2\cdot \underline{i}+b_2\cdot \underline{j}$ sono paralleli sse $\exists \,\lambda_1,\,\lambda_2$, non entrambi nulli, tali che:
$$\lambda_1\cdot (\underline{v}_1=a_1\cdot \underline{i}+b_1\cdot \underline{j})+\lambda_2\cdot (\underline{v}_1=a_2\cdot \underline{i}+b_2\cdot \underline{j})=0\cdot \underline{i}+o\cdot \underline{j}$$
cioè sse solo se il sistema omogeneo:
$$\begin{cases}
		a_1\cdot \lambda_1+a_2\cdot \lambda_2=0 \\
		b_1\cdot \lambda_1+b_2\cdot \lambda_2=0
	\end{cases}$$
ha soluzioni non nulle, quindi si ha:
$$
	det\left(\begin{matrix}
			a_1 & a_2 \\
			b_1 & b_2
		\end{matrix}\right)=0
$$
che quindi è la\textit{ condizione necessaria} affinché due vettori nel piano siano paralleli.\\ Lo stesso discorso vale nello spazio con tre vettori complanari (ovviamente si avrà il determinante di una matrice $3\times 3$ con le 3 componenti dei 3 vettori).
\begin{teorema}
	Siano dati 3 vettori dello spazio, $\underline{v}_1$, $\underline{v}_2$ e $\underline{v}_3$, non complanari. Allora ogni altro vettore$\underline{v}$ dello spazio è combinazione lineare di $\underline{v}_1$, $\underline{v}_2$ e $\underline{v}_3$ e formano una \textbf{base} per i vettori dello spazio e quindi le componenti $\lambda_1,\lambda_2\,\, e \,\, \lambda_3$ di $\underline{v}$ rispetto alla base $\{\underline{v}_1,\underline{v}_2,\underline{v}_3\}$ sono uniche. Quindi i vettori $\underline{v}_1,...,\underline{v}_s$, con $s\geq 4$, dello spazio sono sempre linearmente indipendenti (nel piano saranno con $s\geq 2$ etc...).
\end{teorema}
\section{Coordinate Polari}
Oltre le coordinate cartesiane esiste un altro sistema di coordinate: le \textbf{coordinate polari}:\\
Si fissa nel piano un punto $O$ detto \textbf{origine }o\textbf{ polo}.Da $O$ si manda una retta orientata $x$ detta \textbf{asse polare}. Si fissa un verso positivo (antiorario) per le rotazioni e il radiante come unità degli angoli. Si prende ora un punto $P\neq O$ e chiamo $r$ la distanza tra $P$ e $O$ e $\theta$ l'angolo che si viene a formare tra $x$ (il semiasse polare positivo) e la semiretta $OP$ ($\theta$ viene detto \textbf{anomalia}). Si ha che $r$ e $\theta$ sono dette le \textbf{coordinate polari} di $P$:
\begin{center}
	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-1.1838238)(4.58,1.1838238)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.4,-0.78382385)(4.4,-0.78382385)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.4,-0.78382385)(3.6,1.2161762)
			\psdots[linecolor=black, dotsize=0.08](2.4,0.41617614)
			\psarc[linecolor=black, linewidth=0.04, dimen=outer, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.4,-0.78382385){1.2}{0.0}{32.0}
			\rput[bl](1.8,-0.38382384){$\theta$}
			\rput[bl](0.0,-1.1838238){$O$}
			\rput[bl](1.2,0.016176147){$r$}
			\rput[bl](2.2,0.561762){$P$}
			\rput[bl](4.4,-1.1838238){$x$}
		\end{pspicture}
	}

\end{center}
All'origine si assegna $r=0$ e $\theta$ indefinito. Per ogni valore si ha un solo valore di $r>0$ associato e infiniti valori di $\theta$ che differiscono per multipli di $2\cdot \pi$, quindi per comodità si definisce $\theta\in[0,2\cdot\pi)$ così da avere un solo $\theta$ associato.\\
Suppongo ora di avere un sistema di coordinate cartesiane ortogonali con origine uguale a quelle di un sistema di coordinate polari, con l'asse polare coincidente con l'asse delle ascisse e l'asse delle ordinate scelto in modo che il semiasse positivo delle $x$ si sovrapponga al semiasse positivo delle $y$ con una rotazione positiva di $\frac{\pi}{2}$ radianti. Cerco la formula di passaggio tra i due sistemi di coordinate.
\newpage
Sia $P=(x,y)=(r,\theta)$. Si ha che:
$$\begin{cases}
		x=r\cdot \cos \theta \\
		y=r\cdot \sin\theta
	\end{cases}$$
viceversa, se $P\neq O$ si ha:
$$\begin{cases}
		r=\sqrt{x^2+y^2}                    \\
		\cos\theta=\frac{x}{\sqrt{x^2+y^2}} \\
		\sin\theta=\frac{y}{\sqrt{x^2+y^2}}
	\end{cases}
$$
Graficamente si ha:
\begin{center}

	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-1.905)(4.58,1.905)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.4,-1.505)(4.4,-1.505)
			\psarc[linecolor=black, linewidth=0.04, dimen=outer](0.4,-1.505){1.2}{0.0}{32.0}
			\rput[bl](1.65,-1.105){$\theta$}
			\rput[bl](0.0,-1.905){$O$}
			\rput[bl](2.0,-0.305){$r$}
			\rput[bl](3.67,0.495){$P$}
			\rput[bl](4.4,-1.905){$x$}
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.4,-1.505)(0.4,1.695)
			\psline[linecolor=black, linewidth=0.04](0.4,-1.505)(3.6,0.495)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](3.6,-1.505)(3.6,0.495)(0.4,0.495)
			\rput[bl](2.0,-1.905){$x$}
			\rput[bl](0.0,-0.305){$y$}
			\rput[bl](0.0,1.695){$y$}
		\end{pspicture}
	}

\end{center}
\subsection{Traslazioni e Rotazioni}
In geometria analitica è spesso comodo lavorare con un sistema di coordinate diverso da quello si partenza. Si supponga di fissare un punto $P$ in un sistema di coordinate $(O;x,y)$ che quindi avrà coordinate $x,y$. Nello stesso piano fisso un nuovo sistema di coordinate $(O^{'};X,Y)$, che per comodità hanno la stessa unità di misura. Si vuole esprimere $P$ in $(O^{'};X,Y)$ per mezzo delle vecchie coordinate $(O;x,y)$. Si hanno due opzioni: la \textbf{traslazione degli assi} e la \textbf{rotazione degli assi}.
\newpage
\subsubsection{traslazione degli assi}
Suppongo che l'asse $X$ sia parallelo ed equiverso all'asse $x$ e che l'asse $Y$ sia parallelo ed equiverso all'asse $y$. In questo caso per ottenere il nuovo sistema si usa la traslazione degli assi.\\
Siano $(a,b)$ le coordinate di $O^{'}$ rispetto a $(O;x,y)$. Siano $(x,y)$ le coordinate di $P$ rispetto al vecchio sistema e $(X,Y)$ quelle rispetto al nuovo. Si ha:
\begin{center}
	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-2.505)(5.38,2.505)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.0,-1.705)(5.2,-1.705)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.8,-2.505)(0.8,2.295)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.4,-0.905)(4.8,-0.905)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.6,-2.105)(1.6,1.895)
			\psdots[linecolor=black, dotsize=0.08](4.0,-0.105)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](4.0,-0.105)(4.0,-1.705)
			\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](4.0,-0.105)(0.8,-0.105)
			\rput[bl](4.2,-0.115){$P$}
			\rput[bl](4.1,-0.805){$P_1$}
			\rput[bl](4.8,-1.305){$X$}
			\rput[bl](5.2,-2.105){$x$}
			\rput[bl](3.6,-2.115){$Q_1$}
			\rput[bl](1.65,-2.125){$O_1$}
			\rput[bl](0.4,-2.125){$O$}
			\rput[bl](1.15,-2.105){$a$}
			\rput[bl](0.4,-1.505){$b$}
			\rput[bl](0.2,-0.145){$Q_2$}
			\rput[bl](0.25,-0.85){$O_2$}
			\rput[bl](1.65,-0.115){$P_2$}
			\rput[bl](0.4,2.295){$y$}
			\rput[bl](1.6,1.895){$Y$}
			\psdots[linecolor=black, dotsize=0.08](4.0,-0.905)
			\psdots[linecolor=black, dotsize=0.08](1.6,-0.105)
			\psdots[linecolor=black, dotsize=0.16](1.6,-0.105)
			\psdots[linecolor=black, dotsize=0.16](4.0,-0.905)
			\psdots[linecolor=black, dotsize=0.16](4.0,-0.105)
			\psdots[linecolor=black, dotsize=0.16](1.6,-0.905)
			\psdots[linecolor=black, dotsize=0.16](0.8,-0.105)
			\psdots[linecolor=black, dotsize=0.16](4.0,-1.705)
		\end{pspicture}
	}
\end{center}
e si hanno:
$$O^{'}P=X,\,\,\,\,\, O^{'}P_2=Y,\,\,\,\,\, OO_1=O_2O^{'}=a,$$
$$OO_2=O_1O^{'}=b,\,\,\,\,\, OQ_1=x,\,\,\,\,\, OQ_2=y$$
da cui le seguenti formule:
$$\begin{cases}
		x=X+a \\
		y=Y+b
	\end{cases}
	\,\,\, da\,\, cui \,\, \begin{cases}
		X=x-a \\
		Y=y-b
	\end{cases}
$$ che sono le formule di trasformazione di coordinate nel caso in cui il nuovo sistema di riferimento sia traslato rispetto al primo.
\newpage
\subsection{Rotazione degli assi}
Suppongo che i due sistemi di riferimento $(O,x,y)$ e $(O;X;Y)$ abbiano la stessa origine e che il secondo sistema sia ottenuto dalla rotazione del primo di un angolo $\theta$. Sia $P=(x,y)$, si ha che $\vec{OP}=x\cdot \underline{i}+y\cdot \underline{j}$; sia $P=(X;Y)$, $\vec{OP}$ nel nuovo riferimento sarà dato da $X\cdot \underline{i}^{'}+Y\cdot \underline{j}^{'}$ ed essendo i moduli di $\underline{i}$, $\underline{j}$, $\underline{i}^{'}$ e $\underline{j}^{'}$ pari a 1 si ha:
$$\underline{i}\cdot \underline{i}^{'}=\cos\theta,\,\,\, \underline{j}\cdot \underline{i}^{'}=\cos\left(\frac{\pi}{2}-\theta\right)=\sin\theta$$
$$\underline{i}\cdot \underline{i}=1,\,\,\,\underline{i}^{'}\cdot \underline{i}^{'}=1,\,\,\,\underline{j}\cdot \underline{j}=1,\,\,\,\underline{j}^{'}\cdot \underline{j}^{'}=1$$
$$\underline{i}^{'}\cdot \underline{j}=0,\,\,\, \underline{i}\cdot\underline{j}^{'}=\cos\left(\frac{\pi}{2}+\theta\right)=-\sin\theta,\,\,\, \underline{j}\cdot\underline{j}^{'}=\cos\theta$$
\begin{center}

	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-2.324138)(5.7597065,2.324138)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.5797064,-2.315862)(1.5797064,2.084138)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.7797064,-1.515862)(5.579706,-1.515862)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.9797064,-2.315862)(-0.020293579,2.084138)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.7797064,-1.9158621)(4.7797065,0.084137954)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.5797064,-1.515862)(3.5797064,1.284138)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.5797064,-1.515862)(2.7797065,-1.515862)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.5797064,-1.515862)(2.3797064,-1.115862)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.5797064,-1.515862)(2.3797064,-0.31586206)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.5797064,-1.515862)(1.5797064,-0.31586206)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.5797064,-1.515862)(1.1797065,-0.71586204)
			\rput[bl](0.1797064,2.084138){$Y$}
			\rput[bl](1.7797064,1.684138){$y$}
			\rput[bl](3.5797064,1.284138){$P$}
			\rput[bl](4.7797065,-0.31586206){$X$}
			\rput[bl](5.609706,-1.715862){$x$}
			\rput[bl](1.1797065,-1.9858621){$O$}
			\rput[bl](2.7797065,-1.9458621){$\underline{i}$}
			\rput[bl](2.3797064,-1.015862){$\underline{i}^{'}$}
			\rput[bl](1.7797064,-0.31586206){$\underline{j}$}
			\rput[bl](0.7197064,-0.71586204){$\underline{j}^{'}$}
			\psarc[linecolor=black, linewidth=0.04, dimen=outer, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.5797064,-1.515862){1.2}{0.0}{27.0}
			\psline[linecolor=black, linewidth=0.04](1.5797064,-0.71586204)(1.1797065,-0.71586204)(1.1797065,-0.71586204)
			\rput[bl](2.8997063,-1.215862){$\theta$}
			\rput[bl](1.1857065,-0.34586206){$\theta$}
		\end{pspicture}
	}

\end{center}
Inoltre avendo $x\cdot \underline{i}+y\cdot \underline{j}=\vec{OP}=X\cdot \underline{i}^{'}+Y\cdot \underline{j}^{'}$ si ha:
$$x\cdot \underline{i}+y\cdot \underline{j}=X\cdot \underline{i}^{'}+Y\cdot \underline{j}^{'}$$
e di conseguenza:
\begin{itemize}
	\item moltiplicando per $\underline{i}^{'}$ si ha:
	      $$x\cdot \underline{i}\cdot \underline{i}^{'}+y\cdot \underline{j}\cdot \underline{i}^{'}=X\cdot \underline{i}^{'}\cdot \underline{i}^{'}+Y\cdot \underline{j}^{'}\cdot \underline{i}^{'}$$
	      che comporta:
	      $$x\cos \theta+y\sin\theta=X$$
	\item moltiplicando per $\underline{j}^{'}$ si ha:
	      $$x\cdot \underline{i}\cdot \underline{j}^{'}+y\cdot \underline{j}\cdot \underline{j}^{'}=X\cdot \underline{i}^{'}\cdot \underline{j}^{'}+Y\cdot \underline{j}^{'}\cdot \underline{j}^{'}$$
	      che comporta:
	      $$x\cdot \sin\theta+y\cos\theta=Y$$
\end{itemize}
Si hanno così le formule della rotazione:
$$\begin{cases}
		X=x\cos \theta +y\sin\theta \\
		Y=-x\cdot\sin\theta+y\cdot \cos\theta
	\end{cases}$$
e
$$\begin{cases}
		x=X\cos\theta-Y\sin\theta \\
		y=X\sin\theta+Y\cos\theta
	\end{cases}
$$
\section{La retta}
Una retta è un sottospazio di dimensione 1.\\
Dato un vettore $\underline{v}=l\cdot \underline{i}+m\cdot \underline{j}=(l,m)$ non nullo e un punto $P_0=(x_0,y_0)$. A quale relazione devono soddisfare le coordinate di un certo $P=/x,y)$, appartenente alla retta $r$, passante per $P_0$ e parallela a $\underline{v}$?
\begin{center}
	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-2.705)(6.6454945,2.705)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2054944,-1.905)(1.2054944,2.495)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2054944,-1.905)(6.0054946,-1.905)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2054944,-1.905)(5.6054945,-0.705)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2054944,-1.905)(2.8054943,-1.505)
			\psline[linecolor=black, linewidth=0.04](5.6054945,0.895)(0.0054943846,-0.705)
			\psdots[linecolor=black, dotsize=0.14](4.405494,0.495)
			\psdots[linecolor=black, dotsize=0.14](1.6054944,-0.305)
			\rput[bl](1.46054944,2.495){$y$}
			\rput[bl](6.0054946,-2.305){$x$}
			\rput[bl](5.6054945,-0.705){$P-P_0$}
			\rput[bl](2.4054945,-1.405){$\underline{v}$}
			\rput[bl](4.1054946,0.695){$P$}
			\rput[bl](1.5054944,-0.0835){$P_0$}
			\rput[bl](0.8053528,-2.305){$O$}
			\rput[bl](5.405353,0.995){$r$}
			\psline[linecolor=black, linewidth=0.04](1.2054944,-1.905)(0.4054944,-1.905)
			\psline[linecolor=black, linewidth=0.04](1.2054944,-2.705)(1.2054944,-1.505)
		\end{pspicture}
	}

\end{center}
Un punto $P$ appartiene alla retta $r$ sse il vettore $P-P_0$ è parallelo a $\underline{v}$, ovvero sse $\exists\, t\in\mathbb{R}$
tale che $$P-P_0=t\cdot \underline{v}$$
Inoltre, poiché $P=(x,y)$, $P_0=(x_0,y_0)$ e $\underline{v}=(l,m)$ si ha:
$$\begin{cases}
		x=x_0+l\cdot t \\
		y=y_0+m\cdot t
	\end{cases}$$
che sono le \textbf{equazioni parametriche della retta \textit{r}} passante per $P_0$ e parallela a $\underline{v}$.
\newpage
Voglio ora studiare il comportamento di una retta passante per i due punti distinti $P_1=(x_1,y_1)$ e $P_2=(x_2,y_2)$. Un punto $P$ appartiene alla retta $r$ sse il vettore $P-P_1$ è parallelo a $P_2-P_1$, ovvero sse $\exists\, t\in\mathbb{R}$ tale che $P-P_1=t\cdot (P_2-P_1)$ ovvero:
$$P=P_1+t\cdot (P_2-p_1)$$
\begin{center}

	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-2.705)(7.455353,2.705)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2053528,-1.905)(1.2053528,2.495)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2053528,-1.905)(6.005353,-1.905)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2053528,-1.905)(5.605353,-0.705)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2053528,-1.905)(2.8053527,-1.505)
			\psdots[linecolor=black, dotsize=0.14](4.4053526,0.495)
			\psdots[linecolor=black, dotsize=0.14](1.6053528,-0.305)
			\rput[bl](6.005353,-2.305){$x$}
			\rput[bl](5.605353,-0.705){$P-P_1$}
			\rput[bl](4.005353,0.695){$P_2$}
			\rput[bl](1.6053528,-0.095){$P_1$}
			\psline[linecolor=black, linewidth=0.04](1.2053528,-1.905)(0.40535277,-1.905)
			\psline[linecolor=black, linewidth=0.04](1.2053528,-2.705)(1.2053528,-1.505)
			\psline[linecolor=black, linewidth=0.04](0.005352783,-0.705)(7.205353,1.295)
			\psdots[linecolor=black, dotsize=0.14](7.205353,1.295)
			\rput[bl](7.205353,1.495){$P$}
			\rput[bl](1.6053528,2.495){$y$}
			\rput[bl](2.4053528,-1.105){$P_2-P_1$}
			\rput[bl](0.8053528,-2.305){$O$}
			\rput[bl](7.405353,0.895){$r$}
		\end{pspicture}
	}

\end{center}

e sostituendo $P$, $P_1$ e $P_2$ nell'equazione parametrica si ottengono le equazioni parametriche di una retta passante per due punti:
$$\begin{cases}
		x=x_1+t\cdot (x_2-x_1) \\
		y=y_1+t\cdot(y_2-y_1
	\end{cases}
$$
dato che al variare di $t$ ottengo tutti i punti della retta con $t=\frac{1}{2}$ il punto medio del segmento $\overline{P_1P_2}$:
$$\begin{cases}
		x=\frac{x_1\cdot x_2}{2} \\
		y=\frac{y_1\cdot y_2}{2}
	\end{cases}
$$
al variare ti $t\in [0,1]$ si ottengono tutti i punti del segmento $\overline{P_1P_2}$, e quindi ricavando da $t$ dalle equazioni parametriche, con $x_2\neq x_1$ e $y_2\neq y_1$, si ha:
$$t=\frac{x-x_1}{x_2-x_1}=\frac{y-y_1}{y_2-y_1}$$
quindi si ha:
$$\frac{x-x_1}{x_2-x_1}=\frac{y-y_1}{y_2-y_1}$$
che è detta \textbf{equazione normale} della retta passante per $P_1$ e $P_2$. Tale formula non è definita se $x_2-x_1=0$ o se $ y_2-y_1=0$, in tal caso diventa:
$$x-x_1=0,\, se\,\, x_2=x_1$$
e
$$y-y_1=0,\, se\,\, y_2=y_1$$

\newpage
Dall'equazione normale della retta si può proseguire, trovando:
$$(x-x_1)\cdot (y_2-y_1)=(y-y_1)\cdot (x_2-x_1)$$
$$\downarrow$$
$$x\cdot \underbrace{(y_2-y_1)}_{a}-y\cdot\underbrace{(x_2-x_1)}_b+\underbrace{y_1\cdot x_2-y_2\cdot x_1}_{c}=0$$
ovvero:
$$a\cdot x+b\cdot y+c=0$$
che è l'\textbf{equazione cartesiana della retta in forma implicita}

Si può arrivare anche in un altro modo allo stesso risultato:
sia un vettore $\underline{n}=a\cdot \underline{i}+b\cdot \underline{j}=(a,b)$, non nullo, e un punto $P_0=(x_0,y_0)$. Un punto $P=(x,y)$ appartiene alla retta $r$, passante per $P_0$ e perpendicolare a $\underline{n}$ sse  il vettore $P-P_0$ è perpendicolare a $\underline{n}$ , ovvero sse:
$$P-P_0\cdot \underline{n}=0$$
sostituendo le coordinate $P$, $P_0$ e $\underline{n}$ si ottiene:
$$a\cdot (x-x_0)+b\cdot (y-y_0)=0$$
che è l'\textbf{equazione cartesiana della retta} passante per $P_0$ e perpendicolare  a $\underline{n}$. Ponendo $-a\cdot x_0-b\cdot  y_0=c$ si ottiene l'equazione cartesiana della retta:
$$a\cdot x+b\cdot y+c=0$$
\begin{center}

	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-2.5)(7.8456435,2.5)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(2.8056433,-1.7)(2.8056433,2.3)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(2.8056433,-1.7)(7.205643,-1.7)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(2.8056433,-1.7)(6.805643,-0.5)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(2.8056433,-1.7)(1.6056433,2.3)
			\psline[linecolor=black, linewidth=0.04](2.8056433,-1.7)(2.8056433,-2.5)(2.8056433,-2.1)
			\psline[linecolor=black, linewidth=0.04](2.8056433,-1.7)(2.0056434,-1.7)
			\psline[linecolor=black, linewidth=0.04](2.0056434,-1.7)(0.0056433105,-1.7)
			\psline[linecolor=black, linewidth=0.04](6.805643,1.1)(0.0056433105,-0.9)
			\psdots[linecolor=black, dotsize=0.14](4.0056434,0.3)
			\psdots[linecolor=black, dotsize=0.14](5.6056433,0.7)
			\rput[bl](3.0556434,1.9){$y$}
			\rput[bl](7.205643,-2.1){$x$}
			\rput[bl](2.4056432,-2.1){$O$}
			\rput[bl](1.7156433,2.3){$\underline{n}$}
			\rput[bl](5.6056433,1){$P$}
			\rput[bl](3.9056434,0.45){$P_0$}
			\rput[bl](6.805643,-0.5){$P-P_0$}
			\rput[bl](6.4056435,1.1){$r$}
		\end{pspicture}
	}

\end{center}
\textit{Si ha quindi che i coefficienti delle incognite $x$ e $y$ dell'equazione in forma implicita della retta sono ordinatamente le componenti di un vettore perpendicolare alla retta stessa}\\
Analizziamo ora la retta partendo dalla goniometria.
\begin{shaded}
	\begin{nota}
		Si ricorda:
		\begin{center}
			\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
			{
				\begin{pspicture}(0,-2.753479)(6.18,2.753479)
					\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(2.8,-2.7534792)(2.8,2.8465207)
					\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.0,-0.3534793)(6.0,-0.3534793)
					\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.8,-0.3534793){2.0}
					\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(2.8,-0.3534793)(6.0,2.0465207)
					\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(2.8,-0.3534793)(4.4,0.84652066)
					\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](4.4,-0.3534793)(4.4,0.84652066)(2.8,0.84652066)
					\psarc[linecolor=black, linewidth=0.04, dimen=outer](2.8,-0.3534793){0.8}{0.0}{37.0}
					\rput[bl](6.0,1.6465207){$\underline{v}$}
					\rput[bl](4.35,1.0465206){$\underline{u}$}
					\rput[bl](2.3,-0.7534793){$O$}
					\rput[bl](3.68,-0.1652069){$\theta$}
					\rput[bl](3.7,-0.7534793){$\cos\theta$}
					\rput[bl](1.9,0.84652066){$\sin\theta$}
					\rput[bl](6.0,-0.7534793){$x$}
					\rput[bl](2.4,2.4465208){$y$}
				\end{pspicture}
			}

		\end{center}
		dove:\\
		$\underline{u}$b è un versore, $\norm{\underline{u}}=1$\\
		$$\underline{v}=\norm{\underline{v}}\cdot \underline{u}$$
		$$\norm{\underline{v}} \mbox{ è la lunghezza di } \underline{v}=(x,y)$$
	\end{nota}
\end{shaded}
Si ha:
\begin{center}

	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-2.41)(4.994142,2.41)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.81414187,-2.41)(0.81414187,1.99)(0.81414187,2.39)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.0141418455,-1.61)(4.8141418,-1.61)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.81414187,-1.61)(2.414142,-0.01)
			\psline[linecolor=black, linewidth=0.04](2.414142,1.59)(0.0141418455,-0.81)
			\psline[linecolor=black, linewidth=0.04](2.414142,1.59)(3.2141418,2.39)(3.2141418,2.39)
			\psdots[linecolor=black, dotsize=0.14](2.8141418,1.99)
			\psdots[linecolor=black, dotsize=0.14](0.81414187,-0.01)
			\rput[bl](4.8141418,-2.01){$x$}
			\rput[bl](0.41414183,1.99){$y$}
			\rput[bl](0.41414183,-2.01){$O$}
			\rput[bl](2.414142,-0.41){$\overline{u}$}
			\rput[bl](0.31414183,-0.01){$P_0$}
			\rput[bl](2.8141418,1.59){$P$}
		\end{pspicture}
	}
\end{center}
nella figura si ha:
$$P=(x,y),\,\,\, P_0=(x_0,y_0)$$
$$\underline{u}=(\cos\theta,\sin\theta)=(k\cdot\cos\theta,k\cdot \sin\theta),\,\, con\,\, k\neq 0$$
$$\overline{P_0P}=\, vettore\,\, applicato$$
$$\underline{v}=\lambda\cdot \underline{u},\,\lambda\in\mathbb{R}$$
quindi:
$$\overline{P_0P}=\lambda\cdot \underline{u}=(x-x_0,y-y_0)$$
e quindi si ottiene:
$$\begin{cases}
		x=x_0+\lambda\cdot\cos\theta \\
		y=y_0+\lambda\cdot \sin\theta
	\end{cases}\rightarrow \begin{cases}
		y=\frac{x-x_0}{\cos\theta} \\
		x=\frac{y-y_0}{\sin\theta}
	\end{cases}$$
ne segue che:
$$\frac{x-x_0}{\cos\theta}=\frac{y-y_0}{\sin\theta}\longrightarrow y-y_0=\frac{\sin\theta}{\cos\theta}\cdot (x-x_0)\longrightarrow y=m\cdot x+q$$
Spostandoci in $\mathbb{R}^3$ si ha:
\begin{center}
	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-3.02)(5.3634806,3.02)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.1834804,-1.8)(1.1834804,2.6)(1.1834804,3.0)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.3834804,-1.0)(5.1834803,-1.0)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.1834804,-1.0)(2.7834806,0.6)
			\psline[linecolor=black, linewidth=0.04](2.7834806,2.2)(0.3834804,-0.2)
			\psline[linecolor=black, linewidth=0.04](2.7834806,2.2)(3.5834804,3.0)(3.5834804,3.0)
			\psdots[linecolor=black, dotsize=0.14](3.1834803,2.6)
			\psdots[linecolor=black, dotsize=0.14](1.1834804,0.6)
			\rput[bl](5.1834803,-1.4){$y$}
			\rput[bl](0.7834804,2.6){$z$}
			\rput[bl](0.5834804,-1.4){$O$}
			\rput[bl](2.7834806,0.2){$\overline{u}$}
			\rput[bl](0.5834804,0.6){$P_0$}
			\rput[bl](3.1834803,2.2){$P$}
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.1692804,-1.02)(-0.030719604,-3.02)
			\rput[bl](0.3692804,-3.02){$x$}
		\end{pspicture}
	}

\end{center}
con:
$$P_0=(x_0,y_0,z_0)$$
$$P=(x,y,z)$$
$$\underline{u}=(a,b,c)$$
\begin{center}$\overline{P_0P}$ deve essere proporzionale  a $\underline{u}$ \end{center}
e si ha.
$$\begin{cases}
		x-x_0=\lambda\cdot a \\
		y-y_0=\lambda\cdot b \\
		z-z_0=\lambda\cdot c
	\end{cases}$$
\begin{shaded}
	\begin{nota}[retta passante per l'origine]
		Se la retta passa per $(0,0,0)$ si ha che:
		$$\exists \,\lambda:\,\, P_0+\lambda\cdot \underline{u}=0$$
		Quindi $P_=$ e $\underline{u}$ sono linearmente dipendenti. Se sono indipendenti la retta non passerà mai per l'origine.
	\end{nota}
\end{shaded}
\subsubsection{posizione delle rette nello spazio}
Due rette nello spazio possono essere \textbf{parallele, incidenti o sghembe}.\\
Siano:
$$\begin{cases}
		\underline{v}=P_0+\lambda\cdot \underline{u}_1 \\
		\underline{w}=Q_0+\tau\cdot \underline{u}_2    \\
		P_0+\lambda\cdot \underline{u}_1=Q_0+\tau\cdot \underline{u}_2\longrightarrow P_0-Q_0=\tau\cdot \underline{u}_2-\lambda\cdot \underline{u}_1
	\end{cases}$$
Sia quindi $[\underline{u}_2,-\underline{u}_1]$ la matrice dei coefficienti. Se questa matrice ha rango 2 si ha che le rette non sono parallele (non si ha dipendenza lineare). Inoltre se il rango di $[P_0-Q_0,\underline{u}_2,-\underline{u}_1]=2$ si ha che le rette sono incidenti e $P_0-Q_0$ è combinazione lineare di $\underline{u}_2$ e $-\underline{u}_1$.
\subparagraph{Parallelismo e perpendicolarità}
Siano due rette $r$, \\di equazione $a\cdot x+b\cdot y+c=0$ e $r^{'}$, di equazione $a^{'}\cdot x+b^{'}\cdot y+c^{'}=0$. Le due rette sono parallele sse i vettori $\underline{n}=(a,b)$ e $\underline{n}^{'}=(a^{'},b^{'})$ sono paralleli, ovvero se $\exists\, k\in\mathbb{R},\, k\neq 0$ tale che:
$$\begin{cases}
		a=k\cdot a^{'} \\
		b=k\cdot b^{'}
	\end{cases}
$$
che è la \textbf{condizione di parallelismo}
\begin{center}

	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-2.5097885)(5.397558,2.5097885)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.217558,-2.5002115)(1.217558,2.2997885)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.41755798,-1.7002115)(5.217558,-1.7002115)
			\psline[linecolor=black, linewidth=0.04](0.017557984,1.8997885)(2.417558,-2.5002115)
			\psline[linecolor=black, linewidth=0.04](2.817558,1.8997885)(5.217558,-2.5002115)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.217558,-1.7002115)(3.217558,0.2997885)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.217558,-1.7002115)(2.417558,-0.5002115)
			\rput[bl](4.617558,-2.5002115){$r^{'}$}
			\rput[bl](1.817558,-2.5002115){$r$}
			\rput[bl](0.817558,-2.1002114){$O$}
			\rput[bl](0.817558,2.1997885){$y$}
			\rput[bl](5.317558,-1.9002115){$x$}
			\rput[bl](3.287558,-0.1997885){$\underline{n}^{'}$}
			\rput[bl](2.417558,-0.9002115){$\underline{n}$}
		\end{pspicture}
	}

\end{center}
\newpage
Questo risultato si può ottenere anche in una maniera diversa:\\
le due rette hanno punti in comune solo se il sistema formato dalle loro equazioni ha soluzioni:
$$\begin{cases}
		a\cdot x+b\cdot y+c=0 \\
		a^{'}\cdot x+b^{'}\cdot y+c^{'}=0
	\end{cases}
$$
Se $
	\left|\begin{matrix}
		a     & b     \\
		a^{'} & b^{'}
	\end{matrix}\right|\neq 0
$ si ha per Cramer che si ha una e una sola soluzione, quindi le rette sono incidenti.\\
Se $
	\left|\begin{matrix}
		a     & b     \\
		a^{'} & b^{'}
	\end{matrix}\right|= 0
$ si possono avere due casi:
\begin{itemize}
	\item il sistema non ha soluzioni e le rette quindi sono parallele e distinte
	\item il sistema ha infinite soluzioni e quindi le rette sono coincidenti
\end{itemize}
$
	\left|\begin{matrix}
		a     & b     \\
		a^{'} & b^{'}
	\end{matrix}\right|= 0
$ implica che le righe della matrice sono proporzionali e quindi linearmente dipendenti, ovvero $\exists \, k\in\mathbb{R},\,k\neq 0$ tale che:
$$\begin{cases}
		a=k\cdot a^{'} \\
		b=k\cdot b^{'}
	\end{cases}
$$
la stessa condizione trovata in precedenza.\\
Siano ora $r$ e $r^{'}$ definite come sopra. Siano $\underline{n}=(a,b)=a\cdot \underline{i}+b\cdot\underline{j}$ e $\underline{n}^{'}=(a^{'},b^{'})=a^{'}\cdot \underline{i}+b^{'}\cdot\underline{j}$ perpendicolari alle rette. Quindi $r$ è perpendicolare a $r^{'}$ sse $\underline{n}$ è perpendicolare a $\underline{n}^{'}$. Cioè sse:
$$a\cdot a^{'}+b\cdot b^{'}=0$$
tale formula è detta \textbf{condizione di perpendicolarità} tra le due rette.
\begin{center}
	\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
	{
		\begin{pspicture}(0,-2.5108123)(5.02,2.5108123)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2,-2.0991876)(1.2,2.3008125)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.4,-1.2991877)(4.8,-1.2991877)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2,-1.2991877)(0.4,-0.09918762)
			\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(1.2,-1.2991877)(2.4,-0.49918762)
			\psline[linecolor=black, linewidth=0.04](2.4,-2.0991876)(4.8,-0.49918762)
			\psline[linecolor=black, linewidth=0.04](4.4,-2.4991877)(2.4,0.30081236)
			\psline[linecolor=black, linewidth=0.04](1.2,-2.0991876)(1.2,-2.4991877)
			\rput[bl](0.8,2.0008125){$y$}
			\rput[bl](4.8,-1.6991876){$x$}
			\rput[bl](4.8,-0.8991876){$r^{'}$}
			\rput[bl](2.6,0.30081236){$r$}
			\rput[bl](0.7,-1.6991876){$O$}
			\rput[bl](2.0,-0.49918762){$\underline{n}$}
			\rput[bl](0.0,-0.49918762){$\underline{n}^{'}$}
		\end{pspicture}
	}

\end{center}
\newpage
Suppongo di avere le equazioni parametriche delle rette:
$$
	\begin{cases}
		x=x_0+l\cdot t \\
		y=y_0+m\cdot t
	\end{cases}\,\,\,e\,\,\,
	\begin{cases}
		x^{'}=x_0^{'}+l^{'}\cdot t \\
		y^{'}=y_0^{'}+m^{'}\cdot t
	\end{cases}
$$
Si ha che $\underline{n}=(a,b)=a\cdot \underline{i}+b\cdot\underline{j}$ e $\underline{n}^{'}=(a^{'},b^{'})=a^{'}\cdot \underline{i}+b^{'}\cdot\underline{j}$ sono paralleli sse lo sono anche $r$ e $r^{'}$, cioè sse $\exists \, k\in\mathbb{R},\, k\neq 0$ tale che:
$$\begin{cases}
		l=k\cdot l^{'} \\
		m=k\cdot m^{'}
	\end{cases}
$$
invece $r$ è perpendicolare a $r^{'}$ sse i vettori sono perpendicolari, ovvero se:
$$l\cdot l^{'}+m\cdot m^{'}=0$$
\subparagraph{Rette sghembe}
\begin{definizione}
	Due rette si dicono sghembe se non sono complanari, quindi ne incidenti ne parallele
\end{definizione}
Per trovarle basta verificare che il sistema delle loro equazioni non ha soluzione e che le rette non siano parallele.
\subsubsection{Basi ortonormali}
\begin{definizione}
	Sia $\underline{v}_1,...,\underline{v}_n$ una base. Essa è una \textbf{base ortonormale }se:
	\begin{itemize}
		\item $\underline{v}_i\cdot \underline{v}_j=0,\, i\neq j$
		\item $\norm{\underline{v}}=1$
	\end{itemize}
\end{definizione}
\subsubsection{Approfondimento sul piano}
Sia un piano nello spazio, si cerca:
\begin{itemize}
	\item passaggio per un punto $P_0$
	\item direzione perpendicolare al piano
\end{itemize}
Si ha che $P-P_0$ è ortogonale a $v\land w$ quindi:
$$P-P_0=\alpha	\cdot v+\beta\cdot w$$
equazione cartesiana del piano. Quindi $P=(x,y)$ è tale che $(x-x_0,y-y_o,z-z_0)=\alpha\cdot v+\beta\cdot w$
inoltre si ha:
$$(x-x_0)\cdot a+(y-y_0)\cdot b+(z-z_0)\cdot x=0$$
$$\downarrow$$
$$a\cdot x+b\cdot y+c\cdot z-(x_0\cdot a+y_0\cdot b+z_0\cdot c)=0$$
tutte le equazioni ha tre incognite omogenee sono un piano.

2 piani $\pi$ e $\pi^{'}$ sono paralleli se i rispettivi vettori normali sono tra loro paralleli, rispettivamente ortogonali.\\
\\
2 rette $r$ e $r^{'}$ sono parallele se i rispettivi versori sono proporzionali e rispettivamente perpendicolari.\\
domande:
\begin{itemize}
	\item Siano $r$ una retta  e $\pi$ un piano è possibile trovare una retta $r^{'}$ contenuta  nel piano che sia sghemba con $r$? Solo se $r$ non appartiene al piano

	\item Siano $r$ una retta  e $\pi$ un piano è possibile trovare una retta $r^{'}$ contenuta  nel piano che sia perpendicolare a $r$? Sempre
	      infatti
	      $$\begin{cases}
			      v\cdot w=0 \\
			      w=\alpha\cdot v_1+\beta\cdot v_2
		      \end{cases}
	      $$
	      ha sempre soluzioni con $w=(a,b,c)$, infatti $(\alpha\cdot v_1+\beta\cdot v_2)\cdot v=0$
\end{itemize}
distanza di un punto $P$ da un piano:\\
calcolo la norma di $\overline{PP_0}$. il piano è $a\cdot x+b\cdot y+c\cdot z+d=0$. SI ha $v\cdot w=\norm{v}\cdot \norm{w}\cdot \cos \theta$, quindi con $\theta =0,\pi$ si ha $\norm{v\cdot w}=\norm{v}$.inoltre $w=\left(\frac{a}{\sqrt{a^2+b^2+c^2}},\frac{b}{\sqrt{a^2+b^2+c^2}},\frac{c}{\sqrt{a^2+b^2+c^2}}\right)$
quindi $$
	\norm{P-P_0}=\frac{a\cdot (x-x_0)+b\cdot (y-y_o)+c\cdot (z-z_0)}{\sqrt{a^2+b^2+c^2}}$$
$$=\frac{a\cdot x+b\cdot y+c\cdot z-(x_0\cdot a+y_0\cdot b+z_0\cdot c)}{\sqrt{a^2+b^2+c^2}}$$
ma dato che il punto sta sul piano si ha che :
$$-(x_0\cdot a+y_0\cdot b+z_0\cdot c)=-d$$
quindi:
$$\norm{P-P_0}=\frac{a\cdot x+b\cdot y+c\cdot z+d}{\sqrt{a^2+b^2+c^2}}$$
\begin{shaded}
	esercizi:
	\begin{itemize}
		\item quel è il minimo del seguente insieme di numeri:
		      $$\begin{cases}
				      dist(p,l) \mbox{ dove } p=(1,0,1)\end{cases} $$$$\mbox{ e H è il piano e l una retta del piano } 2\cdot x -3\cdot y-z=0
		      $$
		      $$=\frac{2-1}{\sqrt{4+9+1}}$$
		      %\item sia $l=\{t\cdot v+p\}$ una retta e $h=\{r\cdot w+s\cdot z+q\}$un piano. Sia $H(u)$ la retta di equazioni $\{u\cdot w+2\cdot w\cdot z+q\}$. per che condizioni $l$ e $H$ sono sghembe? (non si è capita la risposta)
	\end{itemize}
\end{shaded}
\section{Coniche}
Prendendo un cono a due falde e tagliandolo con un piano si ottengono le cosiddette \textit{coniche}, dette anche \textit{sezioni coniche}, ovvero delle curve tra le quali si distinguono le \textit{coniche non degeneri}: \textit{ellissi, parabole e iperbole}
\subsection{Ellisse}
\begin{definizione}
	se prendo due punti fissi, detti fuochi $F1$ e $F2$, e un $P$, punto generico e ne prendo la distanza coi fuochi ho che la somma di queste distanze rimane costante $\overline{PF1}+\overline{PF2}=k,\, k\in \mathbb{R}$, $a$ è il semiasse maggiore e $b$ è il semiasse minore. l'ellisse è un  luogo di punti, dal punto di vista geometrico. Come sistema di riferimento prendo la retta tra i due punti e la retta perpendicolare passante per il punto medio tra essi. Si ha la seguente equazione:
	$$\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$$
	se il punto di coordinate $(x,y)$ soddisfa l'equazione lo fa anche quello di coordinate $(-x,y)$, è simmetrica sull'ordinata e anche sulle ascisse $(x,-y)$ e sull'origine $(-x,-y)$. Se $y=0$ si ha che $x=\pm a$ e se metto $x=0$ ho $y=\pm b$.\\
	$k=2\cdot\sqrt{b^2+c^2}$ con $c$ distanza tra il centro e $b$, la distanza focale.\\ La circonferenza è un ellisse con i due fuochi coincidenti.\\
	Ecco un disegno:
	\begin{center}

		\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(0,-2.753479)(5.906958,2.753479)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(2.8,-2.7534792)(2.8,2.4465208)(2.8,2.8465207)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(0.0,0.04652069)(6.0,0.04652069)
				\psellipse[linecolor=black, linewidth=0.04, dimen=outer](2.8,0.04652069)(1.6,1.2)
				\psline[linecolor=black, linewidth=0.04](4.0,0.84652066)(3.6,0.04652069)
				\psline[linecolor=black, linewidth=0.04](4.0,0.84652066)(2.0,0.04652069)
				\rput[bl](5.6,-0.3534793){$x$}
				\rput[bl](2.0,2.4465208){$y$}
				\rput[bl](4.0,0.84652066){$P$}
				\rput[bl](3.6,-0.3534793){$F1$}
				\rput[bl](2.0,-0.3534793){$F1$}
				\rput[bl](4.4,-0.3534793){$a$}
				\rput[bl](0.6,-0.3534793){$-a$}
				\rput[bl](3,1.3465207){$b$}
				\rput[bl](3.0,-1.5534793){$-b$}
				\rput[bl](2.85,-0.3534793){$O$}
			\end{pspicture}
		}
	\end{center}
\end{definizione}
\subsection{Iperbole}
\begin{definizione}
	se prendo due punti fissi, detti fuochi $F1$ e $F2$, e un $P$, punto generico e ne prendo la distanza coi fuochi ho che la differenza di queste distanze rimane costante in modulo: $|\overline{PF1}-\overline{PF2}|=k,\, k\in \mathbb{R}$. Si sceglie un sistema di riferimento, l'asse $x$ sarà la retta dei fuochi e l'asse $y$ la perpendicolare che incide nel punto medio della distanza dei fuochi. Si ha:
	$$\frac{x^2}{a^2}-\frac{y^2}{b^2}=1$$
	se il punto di coordinate $(x,y)$ soddisfa l'equazione lo fa anche quello di coordinate $(-x,y)$, è simmetrica sull'ordinata e anche sulle ascisse $(x,-y)$ e sull'origine $(-x,-y)$. Se $y=0$ si ha che $x=\pm a$ e se metto $x=0$ ho $-y^2= b^2$, quindi non ha intersezioni con l'asse delle $y$. \\ Vedo $y=f(x)$, ma è impossibile, perché ad una $x$ corrisponderebbero due $y$. Ma metà curva va bene:
	$$y=\pm\sqrt{\frac{b^2}{a^2}\cdot x^2-b^2}$$
	sono tutti valori positivi, quindi, spostandomi sul limite ho:
	$$\lim_{x\to\infty} \frac{b}{a}\cdot x\cdot \sqrt{1-\frac{a^2}{x^2}}= \frac{b}{a}\cdot x\cdot \left(1+\overbrace{\frac{1}{2}\cdot \frac{-a^2}{x^2}+o\left(\frac{1}{x^2}\right)}^{\to 0}\right)=\frac{b}{a}\cdot x$$
	si ottiene quindi un asintoto obliquo, mentre l'altro è  $-\frac{b}{a}\cdot x$-\\ se i due asintoti sono rette ortogonali tra loro si ha un caso particolare:
	$$\frac{b}{a}\cdot \left(-\frac{b}{a}\right)=-1\longrightarrow b=a$$
	quindi $x=y$ e $ -x=y$ sono i due asintoti, ovvero sono una rotazione degli assi cartesiani. Dopo la rotazione si ha l'iperbole rappresentabile con una funzione:
	$$y=f(x)=\frac{\alpha\cdot x +\beta}{\gamma\cdot x +\delta}$$
	e si ha $x\cdot y=h,\, h\in \mathbb{R}$ costante negativa, se avessi ruotato nell'altro senso gli assi avrei avuto la costante positiva

	ecco un disegno:
	\begin{center}

		\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
		{
			\begin{pspicture}(0,-2.8150525)(6.720127,2.8150525)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(3.2131689,-2.8000002)(3.2131689,2.3999999)(3.2131689,2.8)
				\psline[linecolor=black, linewidth=0.04](0.41316894,0.0)(6.413169,0.0)
				\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(5.6131687,0.0)(6.813169,0.0)
				\rput{-90.182}(0.053337947,0.05316856){\psarc[linecolor=black, linewidth=0.04, dimen=outer](0.053168945,0.0){2.0}{0.0}{180.4}}
				\rput{-269.31546}(6.4290724,-6.3527155){\psarc[linecolor=black, linewidth=0.04, dimen=outer](6.353169,0.0){2.0}{0.0}{180.0}}
				\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](3.2131689,0.0)(6.413169,2.8)
				\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](3.2131689,0.0)(0.013168945,-2.8000002)
				\psline[linecolor=black, linewidth=0.04, linestyle=dashed, dash=0.17638889cm 0.10583334cm](6.413169,-2.8000002)(0.013168945,2.8)
				\rput[bl](2.8513169,2.3999999){$y$}
				\rput[bl](6.513169,-0.40000013){$x$}
				\rput[bl](5.6131687,-0.40000013){$F2$}
				\rput[bl](0.81316894,-0.40000013){$F1$}
				\psdots[linecolor=black, dotsize=0.14](5.6131687,0.0)
				\psdots[linecolor=black, dotsize=0.14](0.81316894,0.0)
				\rput[bl](4.513169,-0.40000013){$a$}
				\rput[bl](1.373169,-0.40000013){$-a$}
				\psdots[linecolor=black, dotsize=0.14](5.213169,1.5999999)
				\rput[bl](5.213169,1.1999999){$P$}
			\end{pspicture}
		}
	\end{center}
\end{definizione}
\subsection{Parabola}
\begin{definizione}
	Si hanno una retta (\textit{direttrice}) e un punto (\textit{fuoco}). Voglio che un punto $P$ stia sulla parabola sse la distanza del punto con la direttrice è uguale a quella tra il punto e il fuoco. Il punto medio, $O$ del segmento che congiunge perpendicolarmente direttrice e fuoco sta sulla parabola. Il sistema di riferimento ha come $x$ la retta perpendicolare alla direttrice passante per il punto e come $Y$ la retta tangente alla parabola passante per $O$. In questo sistema di riferimento si ha una funzione e mediante traslazione si ha $y=a\cdot x^2+b\cdot y+c$.\\
	\begin{tikzpicture}
		\begin{axis}[width= 6.5cm,height= 6.5cm,xmin=-3, xmax=3,ymin=-3, ymax=3,axis lines= middle,title=Con $a>0$ la parabola sta sopra l'asse delle $x$]
			\addplot[black,line width=1pt,samples=100]{x^2};
		\end{axis}
	\end{tikzpicture}
	~
	\begin{tikzpicture}
		\begin{axis}[width= 6.5cm,height= 6.5cm,xmin=-3, xmax=3,ymin=-3, ymax=3,axis lines= middle,title=Con $a<0$ sotto]
			\addplot[black,line width=1pt, samples =100]{-x^(2)};
		\end{axis}
	\end{tikzpicture}
\end{definizione}
\subsection{Equazione generale delle coniche}
Ellissi, parabole e iperbole sono tutte equazioni sempre di secondo grado in $x$ e $y$. Cerco un'equazione più generica per rappresentare le curve:
$$a_{1,\,1}\cdot x^2+2\cdot a_{1,\,2}\cdot x\cdot y+a_{2,\,2}\cdot y^2+2\cdot a_{1,\,3}\cdot x+2\cdot a_{2,\,3}\cdot y+a_{3,\,3}=0$$
con la seguente matrice associata:
$$
	\left(\begin{matrix}
			a_{1,\,1} & a_{1,\,2} & a_{1,\,3} \\
			a_{2,\,1} & a_{2,\,2} & a_{2,\,3} \\
			a_{3,\,1} & a_{3,\,2} & a_{3,\,3}
		\end{matrix}\right)\cdot
	\left(\begin{matrix}
			x \\
			y \\
			1
		\end{matrix}\right)
$$
con $a_{1,\,2}=a_{2,\,1},\,\,\, a_{1,\,3}=a_{3,\,1},\,\,\,a_{3,\,2}=a_{2,\,3}$
\begin{esempio}
	prendo $x^2+2\cdot x+2\cdot y^2-1=0$. Si ha per la rappresentazione indicata sopra:
	$$
		\left(\begin{matrix}
			1  & 0 & -1 \\
			0  & 2 & 0  \\
			-1 & 0 & -1
		\end{matrix}\right)\cdot
		\left(\begin{matrix}
			x \\
			y \\
			1
		\end{matrix}\right)=\left(\begin{matrix}
			x & y & 1
		\end{matrix}\right)\cdot\left(\begin{matrix}
			x-1      \\
			2\cdot y \\
			-x-1
		\end{matrix}\right)=x^2+2\cdot x+2\cdot y^2-1
	$$
\end{esempio}

con questa equazione generale si hanno dei casi estremi in cui un'equazione di secondo grado (tipo $x^2+y^2+1=0$ (impossibile) o $(x+y+2)\cdot(x-y)=0$ (rette perpendicolari) o $(y-x)\cdot (y-x-1)$ (rette parallele) ) non vale nulla.
\newpage

L'algebra ci insegna come trovare le equazioni corrette:
\begin{teorema} [Spiegato malissimo]
	Sia $A=
		\left(\begin{matrix}
				a_{1,\,1} & a_{1,\,2} & a_{1,\,3} \\
				a_{2,\,1} & a_{2,\,2} & a_{2,\,3} \\
				a_{3,\,1} & a_{3,\,2} & a_{3,\,3}
			\end{matrix}\right)$. Se $det(A)\neq 0$ la conica non è degenere, ovvero non è una coppia di rette (o una retta contata due volte).
\end{teorema}

\begin{teorema}
	Sia poi il complemento algebrico dell'elemento di posto $(3,3)$, ovvero considero solo gli elementi di secondo grado: $A_{3,\,3}=\left(\begin{matrix}
				a_{1,\,1} & a_{1,\,2} \\
				a_{2,\,1} & a_{2,\,2} \\
			\end{matrix}\right)$, ne cerco gli autovalori e gli autovettori. Trovo quindi la matrice diagonalizzante, ne calcolo il determinate e divido tutti i valori per la radice del determinate. Si ottiene una matrice $P$ tale che $P^{-1}=P^T$, si ha quindi una \textbf{matrice ortogonale} (matrici dove $P^{-1}=P^T$, dove i vettori colonna/riga sono ortogonali tra loro e che hanno lunghezza 1). Sia $D$ la matrice diagonale con gli autovalori come valori della diagonale, si ha che:
	$$A\cdot P=P\cdot D\longrightarrow A=P\cdot D\cdot P$$
	Siano ora $\underline{v}x=
		\left(\begin{matrix}
				x \\
				y
			\end{matrix}\right)
	$ e $\underline{v}^T=(x,y)$. Si ha che:
	$$\underline{v}^T\cdot A\cdot \underline{v}=\underline{v}^T\cdot P\cdot D\cdot P^{-1}\cdot \underline{v}$$ e si ha che:
	$$W=P^{-1}\cdot \underline{v}$$
	$$w^T=\underline{v}^T\cdot (P^{-1})^T$$
	Si ha quindi che:
	$$w^T=\underline{v}^T,\,\,\ P^{-1}=P^T,\,\,\, (P^T)^T=P$$
	quindi:
	$$\underline{v}=P^{-1}\cdot \underline{v},\,\,\ w^T\cdot D\cdot w$$
	facendo quindi $\underline{v}^T\cdot D\cdot \underline{v}$ si torna all'equazione di partenza.\\
	\textbf{Si ha che è sempre possibile ruotare gli assi in modo che si prendano gli assi di simmetria della conica}
\end{teorema}


Si può ora capire come riconoscere le coniche:\\
Sia $A=
	\left(\begin{matrix}
			a_{1,\,1} & a_{1,\,2} & a_{1,\,3} \\
			a_{2,\,1} & a_{2,\,2} & a_{2,\,3} \\
			a_{3,\,1} & a_{3,\,2} & a_{3,\,3}
		\end{matrix}\right)$ che sappiamo essere però:
$A=
	\left(\begin{matrix}
			a_{1,\,1} & a_{1,\,2} & a_{1,\,3} \\
			a_{1,\,2} & a_{2,\,2} & a_{2,\,3} \\
			a_{1,\,3} & a_{2,\,3} & a_{3,\,3}
		\end{matrix}\right)$
e sia:
$
	A_{3,\,3}\left(\begin{matrix}
			a_{1,\,1} & a_{1,\,2} \\
			a_{1,\,2} & a_{2,\,2}
		\end{matrix}\right)
$
\begin{itemize}
	\item se $det(A)=0 $ la conica è degenere
	\item se $det(A)\neq 0 $ la conica non è degenere
	\item se $det(A_{3,\,3})= 0 $ si ha una parabola
	\item se $det(A_{3,\,3})< 0 $ si ha un'iperbole
	      \begin{itemize}
		      \item se $a_{1,\,1}+a_{2,\,2}=0$ è equilatera
	      \end{itemize}
	\item se $det(A_{3,\,3})>0$ si ha:
	      \begin{itemize}
		      \item se $a_{1,\,1}\cdot det(A)<0$ si ha un ellisse
		      \item se $a_{1,\,1}\cdot det(A)>0$ non si ha nulla
	      \end{itemize}
\end{itemize}
Si hanno le seguenti notazioni:
\begin{itemize}
	\item si denota come \textbf{invariante lineare} $I_1=a_{1,\,1}+a_{2,\,2}$
	\item si denota come \textbf{invariante quadratico}  $I_2=det(A_{3,\,3})$
	\item si denota come \textbf{invariante cubico} $I_3=det(A)$
\end{itemize}
\newpage
\begin{esempio}
	Sia:
	$$x^2+6x\cdot y-2\cdot x-8\cdot y=0$$
	scrivo la matrice:
	$$
		A=\left(\begin{matrix}
				1  & 3  & -1 \\
				3  & 0  & -4 \\
				-1 & -4 & 0
			\end{matrix}\right)
	$$
	Si ha quindi:
	$$
		A_{3,\,3}=\left(\begin{matrix}
				1 & 3 \\
				3 & 0
			\end{matrix}\right)
	$$
	calcolo il determinante: $det(A)=-8$ e $det(A_{3,\,3})=-9$ quindi si ha un'iperbole non equilatera
\end{esempio}
\begin{esercizio}
	si ha:
	$$x^2-4\cdot x\cdot y+y?2-2\cdot x+4\cdot y-1=0$$
	scrivo la matrice:
	$$
		A=\left(\begin{matrix}
				1  & -2 & -1 \\
				-2 & 1  & 2  \\
				-1 & 2  & -1
			\end{matrix}\right)
	$$
	$$
		A_{3,\,3}=\left(\begin{matrix}
				1  & -2 \\
				-2 & 1
			\end{matrix}\right)
	$$
	calcolo il determinante: $det(A)=6$ e $det(A_{3,\,3})=-3$ quindi si ha un'iperbole non equilatera
\end{esercizio}
\section{Ortogonalizzazione di Gram-Schmidt}
Il metodo di ortogonalizzazione di Gram-Schmidt permette di trovare una famiglia di vettori ortogonali. Partendo da una base si trova una base ortogonale.
\begin{definizione}
	Sia $B=\{v_1,...,v_n\}$ una base di $V$, spazio vettoriale, di dimensione $n$. $B$ è \textbf{ortogonale} sse:
	$$v_i\cdot v_j=0,\, \forall i\neq j$$
	si dice invece che $B$ è \textbf{ortonormale} sse è una base ortogonale e:
	$$v_i\cdot v_i=1,\,\forall i=1,...,n$$
	Quindi una base ortonormale è anche una base ortogonale mentre non vale il viceversa
\end{definizione}
\begin{teorema}[ di Gran-Schmidt]
	Sia $B=\{v_1,...,v_n\}$ una base di $V$, posso costruire una famiglia di vettori ortogonali $\{w_1,...,w_n\}$ a due a due che formano una base di $V$ così:
	$$w_1=v_1$$
	$$w_2=v_2-\frac{v_2\cdot w_1}{w_1\cdot w_1}w_1$$
	$$w_3=v_3-\frac{v_3\cdot w_1}{w_1\cdot w_1}w_1-\frac{v_3\cdot w_2}{w_2\cdot w_2}w_2$$
	$$\vdots$$
	$$w_i=v_i-\frac{v_i\cdot w_1}{w_1\cdot w_1}w_1-\cdots-\frac{v_i\cdot w_{i-1}}{w_{i-1}\cdot w_{i-1}}w_{i-1},\,\, con\,\, i=1,...,n$$
	e i vettori ortogonali sono certamente non nulli.\\
	Si può infine costruire una base ortonormale dividendo i vari $w_i$ per la loro norma:
	$$n_i=\frac{w_i}{\norm{w_i}},\, i=1,...,n$$
\end{teorema}
\end{document}